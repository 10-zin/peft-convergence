{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c051d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cf52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hice1/pgarg76/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c6977",
   "metadata": {},
   "source": [
    "### Full Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c026c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_160825-j6wjsbx3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfull_ft_adamw_warmup_lr3e-05_wd0.01_ag_news_20250421_160825\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/j6wjsbx3\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Using full fine-tuning (no PEFT)\n",
      "Total parameters: 76961152\n",
      "Trainable parameters: 76961152 (100.00%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.001068115234375, 'lr': 0.0, 'memory_usage_mb': 1384.29296875}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 31.981639862060547, 'lr': 1.910828025477707e-06, 'memory_usage_mb': 1449.703125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 31.18136978149414, 'lr': 3.821656050955414e-06, 'memory_usage_mb': 1449.703125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 31.699541091918945, 'lr': 5.732484076433122e-06, 'memory_usage_mb': 1449.703125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 30.848588943481445, 'lr': 7.643312101910828e-06, 'memory_usage_mb': 1449.703125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 28.71842384338379, 'lr': 9.554140127388534e-06, 'memory_usage_mb': 1449.703125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 27.59979248046875, 'lr': 1.1464968152866244e-05, 'memory_usage_mb': 1449.703125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 26.66900932788849, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 10.666992664337158, 'memory_usage_mb': 1548.60546875}\n",
      "New best accuracy: 0.2500\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 26.436506271362305, 'lr': 1.337579617834395e-05, 'memory_usage_mb': 1550.16796875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 26.168081283569336, 'lr': 1.5286624203821656e-05, 'memory_usage_mb': 1550.16796875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 23.478361129760742, 'lr': 1.7197452229299362e-05, 'memory_usage_mb': 1550.16796875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 21.914411544799805, 'lr': 1.9108280254777068e-05, 'memory_usage_mb': 1550.16796875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 20.85085105895996, 'lr': 2.1019108280254778e-05, 'memory_usage_mb': 1550.16796875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 18.19219207763672, 'lr': 2.2929936305732487e-05, 'memory_usage_mb': 1550.16796875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 15.786821752786636, 'accuracy': 0.248, 'f1_score': 0.10408153831370741, 'epoch_time_seconds': 10.194221258163452, 'memory_usage_mb': 1550.16796875}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 17.674461364746094, 'lr': 2.4840764331210193e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 15.186902046203613, 'lr': 2.67515923566879e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 13.379997253417969, 'lr': 2.8662420382165606e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 11.04747486114502, 'lr': 2.9936530324400563e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 9.27135944366455, 'lr': 2.9724964739069114e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 8.663094520568848, 'lr': 2.951339915373766e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 6.733583748340607, 'accuracy': 0.278, 'f1_score': 0.16289631254104056, 'epoch_time_seconds': 10.184904336929321, 'memory_usage_mb': 1550.48046875}\n",
      "New best accuracy: 0.2780\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 7.975485324859619, 'lr': 2.9301833568406208e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 7.531722068786621, 'lr': 2.9090267983074754e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 7.273833274841309, 'lr': 2.8878702397743298e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 6.459965229034424, 'lr': 2.8667136812411848e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 6.438586711883545, 'lr': 2.8455571227080395e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 5.726416110992432, 'lr': 2.8244005641748942e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 5.396097183227539, 'lr': 2.803244005641749e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.9681216031312943, 'accuracy': 0.412, 'f1_score': 0.364914861811229, 'epoch_time_seconds': 10.18715524673462, 'memory_usage_mb': 1550.48046875}\n",
      "New best accuracy: 0.4120\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.751456260681152, 'lr': 2.7820874471086036e-05, 'memory_usage_mb': 1550.48046875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 4.725096702575684, 'lr': 2.7609308885754586e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 4.126827239990234, 'lr': 2.7397743300423133e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.8685357570648193, 'lr': 2.718617771509168e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.425658702850342, 'lr': 2.6974612129760227e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.249878406524658, 'lr': 2.676304654442877e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 2.212372846901417, 'accuracy': 0.536, 'f1_score': 0.5041721312182679, 'epoch_time_seconds': 10.015222072601318, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.5360\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.0530214309692383, 'lr': 2.655148095909732e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 2.7093210220336914, 'lr': 2.6339915373765867e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 2.539242744445801, 'lr': 2.6128349788434414e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 2.3834307193756104, 'lr': 2.591678420310296e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 2.241633176803589, 'lr': 2.5705218617771508e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 2.1856913566589355, 'lr': 2.549365303244006e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 1.4108827523887157, 'accuracy': 0.618, 'f1_score': 0.5907811404400528, 'epoch_time_seconds': 9.196699857711792, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.6180\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 2.03728985786438, 'lr': 2.5282087447108605e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 1.8938524723052979, 'lr': 2.5070521861777152e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.0415663719177246, 'lr': 2.48589562764457e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 1.7182236909866333, 'lr': 2.4647390691114246e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 1.5630820989608765, 'lr': 2.4435825105782793e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 1.4784376621246338, 'lr': 2.422425952045134e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 1.4849069118499756, 'lr': 2.4012693935119887e-05, 'memory_usage_mb': 1550.79296875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 441: {'epoch': 6, 'eval_loss': 0.8843726944178343, 'accuracy': 0.708, 'f1_score': 0.7008687771561105, 'epoch_time_seconds': 8.632069110870361, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.7080\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 1.328782558441162, 'lr': 2.3801128349788434e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 1.3605326414108276, 'lr': 2.3589562764456984e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 1.2354484796524048, 'lr': 2.337799717912553e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 1.0546306371688843, 'lr': 2.3166431593794078e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 1.0567699670791626, 'lr': 2.2954866008462625e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 0.9447345733642578, 'lr': 2.274330042313117e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 0.5317540802061558, 'accuracy': 0.772, 'f1_score': 0.7723711790377917, 'epoch_time_seconds': 8.074477434158325, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.7720\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 0.9227848649024963, 'lr': 2.2531734837799722e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 0.882978081703186, 'lr': 2.2320169252468265e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 0.8692755103111267, 'lr': 2.2108603667136812e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 0.8260706663131714, 'lr': 2.189703808180536e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 0.8342351913452148, 'lr': 2.1685472496473906e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 0.7125546932220459, 'lr': 2.1473906911142456e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 0.3370794104412198, 'accuracy': 0.802, 'f1_score': 0.8036981821063932, 'epoch_time_seconds': 8.084363460540771, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8020\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 0.6799828410148621, 'lr': 2.1262341325811003e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 0.6519795060157776, 'lr': 2.105077574047955e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 0.5984928607940674, 'lr': 2.0839210155148097e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 0.5769447684288025, 'lr': 2.0627644569816644e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 0.5864225625991821, 'lr': 2.0416078984485194e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 0.5600874423980713, 'lr': 2.0204513399153738e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 0.23367807129397988, 'accuracy': 0.804, 'f1_score': 0.8049018775567254, 'epoch_time_seconds': 8.088406801223755, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8040\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 0.4734446406364441, 'lr': 1.9992947813822284e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 0.46967723965644836, 'lr': 1.978138222849083e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 0.5166589617729187, 'lr': 1.9569816643159378e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 0.4968607723712921, 'lr': 1.935825105782793e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 0.4692363142967224, 'lr': 1.9146685472496475e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 0.4867851138114929, 'lr': 1.8935119887165022e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 0.40943509340286255, 'lr': 1.872355430183357e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 0.1787174455821514, 'accuracy': 0.804, 'f1_score': 0.8030608803232724, 'epoch_time_seconds': 8.083696842193604, 'memory_usage_mb': 1550.79296875}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 0.4707670509815216, 'lr': 1.8511988716502116e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 0.40609556436538696, 'lr': 1.8300423131170666e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 0.3704991936683655, 'lr': 1.808885754583921e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 0.4446489214897156, 'lr': 1.7877291960507757e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 0.44281333684921265, 'lr': 1.7665726375176304e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 0.32164767384529114, 'lr': 1.745416078984485e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 0.14544697245582938, 'accuracy': 0.8, 'f1_score': 0.7978737326073806, 'epoch_time_seconds': 8.066136121749878, 'memory_usage_mb': 1550.79296875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 0.3069436252117157, 'lr': 1.72425952045134e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 0.32650235295295715, 'lr': 1.7031029619181948e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 0.3765682578086853, 'lr': 1.6819464033850495e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 0.37107333540916443, 'lr': 1.660789844851904e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 0.3105769455432892, 'lr': 1.639633286318759e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 0.37302228808403015, 'lr': 1.618476727785614e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 0.1254388161469251, 'accuracy': 0.806, 'f1_score': 0.8027629534247871, 'epoch_time_seconds': 8.085289716720581, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8060\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 0.26175519824028015, 'lr': 1.5973201692524682e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 0.313740998506546, 'lr': 1.576163610719323e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 0.34850913286209106, 'lr': 1.5550070521861776e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 0.29467812180519104, 'lr': 1.5338504936530323e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 0.27509602904319763, 'lr': 1.5126939351198872e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 0.23255200684070587, 'lr': 1.491537376586742e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 0.26233845949172974, 'lr': 1.4703808180535967e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 0.11057137930765748, 'accuracy': 0.81, 'f1_score': 0.8066240211872941, 'epoch_time_seconds': 8.092112302780151, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8100\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 0.28262341022491455, 'lr': 1.4492242595204514e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 0.2909945249557495, 'lr': 1.428067700987306e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 0.303145170211792, 'lr': 1.4069111424541608e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 0.2770479917526245, 'lr': 1.3857545839210156e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 0.24767979979515076, 'lr': 1.3645980253878703e-05, 'memory_usage_mb': 1550.79296875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 0.31680527329444885, 'lr': 1.343441466854725e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 0.10117912699934095, 'accuracy': 0.814, 'f1_score': 0.810614983292461, 'epoch_time_seconds': 8.067701816558838, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8140\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 0.23678091168403625, 'lr': 1.3222849083215797e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 0.26545917987823486, 'lr': 1.3011283497884344e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 0.32027527689933777, 'lr': 1.2799717912552892e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 0.32052093744277954, 'lr': 1.258815232722144e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 0.23883436620235443, 'lr': 1.2376586741889986e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 0.20245271921157837, 'lr': 1.2165021156558533e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 0.093110779998824, 'accuracy': 0.822, 'f1_score': 0.8191423865039744, 'epoch_time_seconds': 8.089321613311768, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8220\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 0.2009371817111969, 'lr': 1.195345557122708e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 0.23245756328105927, 'lr': 1.1741889985895629e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 0.19361349940299988, 'lr': 1.1530324400564176e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 0.20859594643115997, 'lr': 1.1318758815232722e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 0.21581420302391052, 'lr': 1.110719322990127e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 0.21311092376708984, 'lr': 1.0895627644569816e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 0.21229888498783112, 'lr': 1.0684062059238365e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 0.08878630690742284, 'accuracy': 0.824, 'f1_score': 0.8209749370791555, 'epoch_time_seconds': 8.078140020370483, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8240\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 0.21354472637176514, 'lr': 1.0472496473906912e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 0.20441560447216034, 'lr': 1.0260930888575459e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 0.15557296574115753, 'lr': 1.0049365303244006e-05, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 0.20133188366889954, 'lr': 9.837799717912552e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 0.19727042317390442, 'lr': 9.626234132581101e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 0.28396815061569214, 'lr': 9.414668547249648e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 0.08557083213236183, 'accuracy': 0.828, 'f1_score': 0.8247404375799856, 'epoch_time_seconds': 8.084646940231323, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8280\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 0.23054863512516022, 'lr': 9.203102961918195e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 0.20940953493118286, 'lr': 8.991537376586742e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 0.16635176539421082, 'lr': 8.779971791255289e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 0.21799993515014648, 'lr': 8.568406205923837e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 0.20564353466033936, 'lr': 8.356840620592384e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 0.1857587993144989, 'lr': 8.145275035260931e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 0.08198035473469645, 'accuracy': 0.826, 'f1_score': 0.822515510791124, 'epoch_time_seconds': 8.077022790908813, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 0.15475207567214966, 'lr': 7.933709449929478e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 0.19893789291381836, 'lr': 7.722143864598025e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 0.2294083535671234, 'lr': 7.5105782792665725e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 0.20072193443775177, 'lr': 7.29901269393512e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 0.15656383335590363, 'lr': 7.087447108603667e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 0.2339591085910797, 'lr': 6.875881523272215e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 0.0792734146816656, 'accuracy': 0.836, 'f1_score': 0.8333664183762332, 'epoch_time_seconds': 8.089935779571533, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8360\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 0.15406109392642975, 'lr': 6.664315937940762e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 0.18207460641860962, 'lr': 6.452750352609309e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 0.18060767650604248, 'lr': 6.241184767277856e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 0.18513897061347961, 'lr': 6.029619181946403e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 0.14750663936138153, 'lr': 5.818053596614951e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 0.20058870315551758, 'lr': 5.606488011283498e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 0.18477226793766022, 'lr': 5.394922425952045e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 0.0776508238632232, 'accuracy': 0.836, 'f1_score': 0.8333664183762332, 'epoch_time_seconds': 8.08144760131836, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 0.18616029620170593, 'lr': 5.183356840620593e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 0.2199590504169464, 'lr': 4.9717912552891395e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 0.1484033763408661, 'lr': 4.760225669957687e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 0.18958334624767303, 'lr': 4.548660084626234e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 0.16182206571102142, 'lr': 4.337094499294781e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 0.22204093635082245, 'lr': 4.125528913963329e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 0.07594071922358125, 'accuracy': 0.836, 'f1_score': 0.8333664183762332, 'epoch_time_seconds': 8.078439950942993, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 0.17293508350849152, 'lr': 3.913963328631876e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 0.2256917655467987, 'lr': 3.7023977433004234e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 0.2682092487812042, 'lr': 3.4908321579689703e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 0.1956682801246643, 'lr': 3.2792665726375176e-06, 'memory_usage_mb': 1550.79296875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 0.13273803889751434, 'lr': 3.067700987306065e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 0.1651715189218521, 'lr': 2.8561354019746123e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 0.07511062861885875, 'accuracy': 0.836, 'f1_score': 0.8333664183762332, 'epoch_time_seconds': 8.071419715881348, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 0.18084022402763367, 'lr': 2.6445698166431596e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 0.13850797712802887, 'lr': 2.4330042313117065e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 0.16831453144550323, 'lr': 2.2214386459802538e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 0.17418822646141052, 'lr': 2.0098730606488015e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 0.14469203352928162, 'lr': 1.7983074753173484e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 0.14513182640075684, 'lr': 1.5867418899858957e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 0.20199592411518097, 'lr': 1.3751763046544429e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 0.07452548667788506, 'accuracy': 0.838, 'f1_score': 0.8353003623135333, 'epoch_time_seconds': 8.091424942016602, 'memory_usage_mb': 1550.79296875}\n",
      "New best accuracy: 0.8380\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 0.1719827651977539, 'lr': 1.1636107193229902e-06, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 0.15719549357891083, 'lr': 9.520451339915374e-07, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 0.16270871460437775, 'lr': 7.404795486600846e-07, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 0.2432665079832077, 'lr': 5.289139633286319e-07, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 0.14741621911525726, 'lr': 3.1734837799717913e-07, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 0.16032622754573822, 'lr': 1.0578279266572638e-07, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 0.07435812579933554, 'accuracy': 0.838, 'f1_score': 0.8353003623135333, 'epoch_time_seconds': 8.085947513580322, 'memory_usage_mb': 1550.79296875}\n",
      "Step 1575: {'total_training_time_seconds': 214.59520816802979, 'avg_epoch_time_seconds': 8.581887836456298, 'best_accuracy': 0.838, 'convergence_step': 'Not converged', 'memory_usage_mb': 1550.79296875}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 8.58189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.08595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 0.07436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.8353\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1550.79297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 214.59521\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 0.16033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mfull_ft_adamw_warmup_lr3e-05_wd0.01_ag_news_20250421_160825\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/j6wjsbx3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_160825-j6wjsbx3/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --full_ft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8999c",
   "metadata": {},
   "source": [
    "### Default LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687a148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_161925-gc8ggl5h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.01_ag_news_20250421_161925\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/gc8ggl5h\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0, 'memory_usage_mb': 1309.953125}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 34.35995864868164, 'lr': 1.9108280254777068e-05, 'memory_usage_mb': 1375.046875}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 32.95455551147461, 'lr': 3.8216560509554137e-05, 'memory_usage_mb': 1375.046875}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 32.37012481689453, 'lr': 5.732484076433121e-05, 'memory_usage_mb': 1375.046875}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 30.977373123168945, 'lr': 7.643312101910827e-05, 'memory_usage_mb': 1375.046875}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 31.219219207763672, 'lr': 9.554140127388533e-05, 'memory_usage_mb': 1375.046875}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 29.5000057220459, 'lr': 0.00011464968152866242, 'memory_usage_mb': 1375.046875}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 27.663359463214874, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 11.76837706565857, 'memory_usage_mb': 1478.01171875}\n",
      "New best accuracy: 0.2500\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 26.28169822692871, 'lr': 0.00013375796178343948, 'memory_usage_mb': 1479.88671875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 24.65950584411621, 'lr': 0.00015286624203821655, 'memory_usage_mb': 1479.88671875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 20.966201782226562, 'lr': 0.0001719745222929936, 'memory_usage_mb': 1479.88671875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 18.1718692779541, 'lr': 0.00019108280254777067, 'memory_usage_mb': 1479.88671875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 14.971231460571289, 'lr': 0.00021019108280254773, 'memory_usage_mb': 1480.19921875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 11.278234481811523, 'lr': 0.00022929936305732485, 'memory_usage_mb': 1480.19921875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 7.716626092791557, 'accuracy': 0.274, 'f1_score': 0.1549491789219367, 'epoch_time_seconds': 10.974283456802368, 'memory_usage_mb': 1480.19921875}\n",
      "New best accuracy: 0.2740\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 8.963021278381348, 'lr': 0.0002484076433121019, 'memory_usage_mb': 1480.19921875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 8.0890531539917, 'lr': 0.00026751592356687897, 'memory_usage_mb': 1480.19921875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 6.773282051086426, 'lr': 0.00028662420382165603, 'memory_usage_mb': 1480.19921875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 6.106011867523193, 'lr': 0.0002993653032440056, 'memory_usage_mb': 1480.19921875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 5.090319633483887, 'lr': 0.0002972496473906911, 'memory_usage_mb': 1480.19921875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.744742393493652, 'lr': 0.00029513399153737656, 'memory_usage_mb': 1480.19921875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.8900668919086456, 'accuracy': 0.662, 'f1_score': 0.6575254157766892, 'epoch_time_seconds': 8.409146308898926, 'memory_usage_mb': 1480.19921875}\n",
      "New best accuracy: 0.6620\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.221572399139404, 'lr': 0.00029301833568406207, 'memory_usage_mb': 1480.19921875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.183194637298584, 'lr': 0.0002909026798307475, 'memory_usage_mb': 1480.19921875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.976060152053833, 'lr': 0.000288787023977433, 'memory_usage_mb': 1480.19921875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.918962001800537, 'lr': 0.00028667136812411843, 'memory_usage_mb': 1480.19921875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.7271738052368164, 'lr': 0.0002845557122708039, 'memory_usage_mb': 1480.19921875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.685589551925659, 'lr': 0.0002824400564174894, 'memory_usage_mb': 1480.19921875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.671074151992798, 'lr': 0.00028032440056417486, 'memory_usage_mb': 1480.19921875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.5383882597088814, 'accuracy': 0.776, 'f1_score': 0.7761762845651718, 'epoch_time_seconds': 8.40566897392273, 'memory_usage_mb': 1480.19921875}\n",
      "New best accuracy: 0.7760\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.658399820327759, 'lr': 0.00027820874471086036, 'memory_usage_mb': 1480.19921875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.62530517578125, 'lr': 0.0002760930888575458, 'memory_usage_mb': 1480.19921875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.5053250789642334, 'lr': 0.0002739774330042313, 'memory_usage_mb': 1480.19921875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.4432272911071777, 'lr': 0.0002718617771509168, 'memory_usage_mb': 1480.19921875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.4186904430389404, 'lr': 0.00026974612129760224, 'memory_usage_mb': 1480.51171875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.3662965297698975, 'lr': 0.0002676304654442877, 'memory_usage_mb': 1480.51171875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.256181575357914, 'accuracy': 0.768, 'f1_score': 0.7635544965565014, 'epoch_time_seconds': 8.357581615447998, 'memory_usage_mb': 1480.51171875}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.365149736404419, 'lr': 0.00026551480959097315, 'memory_usage_mb': 1480.51171875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.3850903511047363, 'lr': 0.00026339915373765866, 'memory_usage_mb': 1480.51171875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.2169976234436035, 'lr': 0.0002612834978843441, 'memory_usage_mb': 1480.51171875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.163402557373047, 'lr': 0.00025916784203102957, 'memory_usage_mb': 1480.51171875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.1416523456573486, 'lr': 0.0002570521861777151, 'memory_usage_mb': 1480.51171875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.1838219165802, 'lr': 0.00025493653032440054, 'memory_usage_mb': 1480.51171875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 2.926258273422718, 'accuracy': 0.8, 'f1_score': 0.7968912781457128, 'epoch_time_seconds': 8.366405725479126, 'memory_usage_mb': 1480.51171875}\n",
      "New best accuracy: 0.8000\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.0922603607177734, 'lr': 0.00025282087447108605, 'memory_usage_mb': 1480.51171875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.0669028759002686, 'lr': 0.0002507052186177715, 'memory_usage_mb': 1480.51171875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.9680936336517334, 'lr': 0.00024858956276445696, 'memory_usage_mb': 1480.51171875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.9415194988250732, 'lr': 0.00024647390691114247, 'memory_usage_mb': 1480.51171875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.9012465476989746, 'lr': 0.00024435825105782787, 'memory_usage_mb': 1480.51171875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.9741697311401367, 'lr': 0.00024224259520451338, 'memory_usage_mb': 1480.51171875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 2.9298675060272217, 'lr': 0.00024012693935119883, 'memory_usage_mb': 1480.51171875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 441: {'epoch': 6, 'eval_loss': 2.682950370013714, 'accuracy': 0.772, 'f1_score': 0.7677930398510022, 'epoch_time_seconds': 8.368855714797974, 'memory_usage_mb': 1480.51171875}\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 2.957512617111206, 'lr': 0.00023801128349788432, 'memory_usage_mb': 1480.51171875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.8560893535614014, 'lr': 0.0002358956276445698, 'memory_usage_mb': 1480.51171875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.843632936477661, 'lr': 0.00023377997179125528, 'memory_usage_mb': 1480.51171875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.698059558868408, 'lr': 0.00023166431593794074, 'memory_usage_mb': 1480.51171875}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.7124545574188232, 'lr': 0.00022954866008462622, 'memory_usage_mb': 1480.51171875}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.6970114707946777, 'lr': 0.0002274330042313117, 'memory_usage_mb': 1481.76171875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.4506560042500496, 'accuracy': 0.816, 'f1_score': 0.8153317006293871, 'epoch_time_seconds': 8.343753099441528, 'memory_usage_mb': 1481.76171875}\n",
      "New best accuracy: 0.8160\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.664064884185791, 'lr': 0.00022531734837799718, 'memory_usage_mb': 1481.76171875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.6952052116394043, 'lr': 0.0002232016925246826, 'memory_usage_mb': 1481.76171875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.616560935974121, 'lr': 0.0002210860366713681, 'memory_usage_mb': 1481.76171875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.6149749755859375, 'lr': 0.00021897038081805358, 'memory_usage_mb': 1481.76171875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.5861854553222656, 'lr': 0.00021685472496473906, 'memory_usage_mb': 1481.76171875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.552180528640747, 'lr': 0.00021473906911142451, 'memory_usage_mb': 1481.76171875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.3291685432195663, 'accuracy': 0.816, 'f1_score': 0.8157657039500896, 'epoch_time_seconds': 8.319509267807007, 'memory_usage_mb': 1482.38671875}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.569028615951538, 'lr': 0.00021262341325811, 'memory_usage_mb': 1482.38671875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.572502851486206, 'lr': 0.00021050775740479548, 'memory_usage_mb': 1482.38671875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.546933174133301, 'lr': 0.00020839210155148096, 'memory_usage_mb': 1482.38671875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.5803022384643555, 'lr': 0.00020627644569816642, 'memory_usage_mb': 1482.38671875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.432624340057373, 'lr': 0.0002041607898448519, 'memory_usage_mb': 1482.38671875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.6544129848480225, 'lr': 0.00020204513399153736, 'memory_usage_mb': 1482.38671875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.2188377901911736, 'accuracy': 0.822, 'f1_score': 0.8186104751134433, 'epoch_time_seconds': 8.3062903881073, 'memory_usage_mb': 1482.38671875}\n",
      "New best accuracy: 0.8220\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.545261859893799, 'lr': 0.0001999294781382228, 'memory_usage_mb': 1482.38671875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.400876522064209, 'lr': 0.0001978138222849083, 'memory_usage_mb': 1482.38671875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.535189628601074, 'lr': 0.00019569816643159378, 'memory_usage_mb': 1482.38671875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.4481630325317383, 'lr': 0.00019358251057827926, 'memory_usage_mb': 1482.38671875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.4243671894073486, 'lr': 0.00019146685472496471, 'memory_usage_mb': 1482.38671875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.3497610092163086, 'lr': 0.0001893511988716502, 'memory_usage_mb': 1482.38671875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.3370094299316406, 'lr': 0.00018723554301833568, 'memory_usage_mb': 1482.38671875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.1256914734840393, 'accuracy': 0.844, 'f1_score': 0.8434449139715647, 'epoch_time_seconds': 8.313044548034668, 'memory_usage_mb': 1482.38671875}\n",
      "New best accuracy: 0.8440\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.390596866607666, 'lr': 0.00018511988716502116, 'memory_usage_mb': 1482.38671875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.3314735889434814, 'lr': 0.00018300423131170662, 'memory_usage_mb': 1482.38671875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.357469320297241, 'lr': 0.00018088857545839207, 'memory_usage_mb': 1485.19921875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.327448844909668, 'lr': 0.00017877291960507755, 'memory_usage_mb': 1485.19921875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.40926456451416, 'lr': 0.000176657263751763, 'memory_usage_mb': 1485.19921875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.3905107975006104, 'lr': 0.0001745416078984485, 'memory_usage_mb': 1485.19921875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.072029046714306, 'accuracy': 0.812, 'f1_score': 0.8098881019307489, 'epoch_time_seconds': 8.294989585876465, 'memory_usage_mb': 1485.19921875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.327730178833008, 'lr': 0.00017242595204513398, 'memory_usage_mb': 1485.19921875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.318941831588745, 'lr': 0.00017031029619181946, 'memory_usage_mb': 1485.19921875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.346386671066284, 'lr': 0.00016819464033850494, 'memory_usage_mb': 1485.19921875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.2553629875183105, 'lr': 0.0001660789844851904, 'memory_usage_mb': 1485.19921875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.270026206970215, 'lr': 0.00016396332863187588, 'memory_usage_mb': 1485.19921875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.2355785369873047, 'lr': 0.00016184767277856136, 'memory_usage_mb': 1485.19921875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.020196981728077, 'accuracy': 0.834, 'f1_score': 0.8329132286525132, 'epoch_time_seconds': 8.304968118667603, 'memory_usage_mb': 1485.19921875}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.2844765186309814, 'lr': 0.0001597320169252468, 'memory_usage_mb': 1485.19921875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.309903383255005, 'lr': 0.00015761636107193227, 'memory_usage_mb': 1485.19921875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.2698254585266113, 'lr': 0.00015550070521861775, 'memory_usage_mb': 1485.19921875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.19219970703125, 'lr': 0.00015338504936530324, 'memory_usage_mb': 1485.19921875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.2875759601593018, 'lr': 0.0001512693935119887, 'memory_usage_mb': 1485.19921875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.184007167816162, 'lr': 0.00014915373765867417, 'memory_usage_mb': 1485.19921875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.1562204360961914, 'lr': 0.00014703808180535966, 'memory_usage_mb': 1485.19921875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.960793774574995, 'accuracy': 0.844, 'f1_score': 0.8431140629655014, 'epoch_time_seconds': 8.296716213226318, 'memory_usage_mb': 1492.07421875}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.2058253288269043, 'lr': 0.00014492242595204514, 'memory_usage_mb': 1492.07421875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.3140954971313477, 'lr': 0.0001428067700987306, 'memory_usage_mb': 1492.07421875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.167815685272217, 'lr': 0.00014069111424541608, 'memory_usage_mb': 1492.07421875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.1770894527435303, 'lr': 0.00013857545839210153, 'memory_usage_mb': 1492.07421875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.172449827194214, 'lr': 0.00013645980253878702, 'memory_usage_mb': 1492.07421875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.1671366691589355, 'lr': 0.0001343441466854725, 'memory_usage_mb': 1492.07421875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 945: {'epoch': 14, 'eval_loss': 1.9282378405332565, 'accuracy': 0.836, 'f1_score': 0.8354477663192225, 'epoch_time_seconds': 8.289541482925415, 'memory_usage_mb': 1504.88671875}\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 2.1583023071289062, 'lr': 0.00013222849083215795, 'memory_usage_mb': 1504.88671875}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 2.091834545135498, 'lr': 0.00013011283497884344, 'memory_usage_mb': 1504.88671875}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 2.129479169845581, 'lr': 0.0001279971791255289, 'memory_usage_mb': 1504.88671875}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.229170322418213, 'lr': 0.00012588152327221437, 'memory_usage_mb': 1504.88671875}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 2.167292833328247, 'lr': 0.00012376586741889986, 'memory_usage_mb': 1504.88671875}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 2.155362367630005, 'lr': 0.00012165021156558531, 'memory_usage_mb': 1504.88671875}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.8848514966666698, 'accuracy': 0.844, 'f1_score': 0.8436362611371522, 'epoch_time_seconds': 8.425671577453613, 'memory_usage_mb': 1504.88671875}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 2.217862844467163, 'lr': 0.0001195345557122708, 'memory_usage_mb': 1504.88671875}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 2.160851001739502, 'lr': 0.00011741889985895626, 'memory_usage_mb': 1504.88671875}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 2.0688793659210205, 'lr': 0.00011530324400564175, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 2.1448795795440674, 'lr': 0.00011318758815232721, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 2.1209359169006348, 'lr': 0.00011107193229901268, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 2.1205379962921143, 'lr': 0.00010895627644569815, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 2.025204658508301, 'lr': 0.00010684062059238363, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.8525605499744415, 'accuracy': 0.86, 'f1_score': 0.859605861683054, 'epoch_time_seconds': 8.283432483673096, 'memory_usage_mb': 1517.38671875}\n",
      "New best accuracy: 0.8600\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 2.1179966926574707, 'lr': 0.0001047249647390691, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 2.1926019191741943, 'lr': 0.00010260930888575459, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 2.24772310256958, 'lr': 0.00010049365303244004, 'memory_usage_mb': 1517.38671875}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 2.033931016921997, 'lr': 9.837799717912551e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 2.095865249633789, 'lr': 9.626234132581099e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 2.124854564666748, 'lr': 9.414668547249648e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.832818053662777, 'accuracy': 0.852, 'f1_score': 0.8534457826992189, 'epoch_time_seconds': 8.289097547531128, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 2.0597422122955322, 'lr': 9.203102961918194e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 2.040086269378662, 'lr': 8.99153737658674e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 2.3295679092407227, 'lr': 8.779971791255288e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 2.1519250869750977, 'lr': 8.568406205923835e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 2.051187515258789, 'lr': 8.356840620592383e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 2.0753870010375977, 'lr': 8.14527503526093e-05, 'memory_usage_mb': 1528.32421875}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.813090741634369, 'accuracy': 0.842, 'f1_score': 0.8405278322661959, 'epoch_time_seconds': 8.2640540599823, 'memory_usage_mb': 1532.38671875}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 2.025308132171631, 'lr': 7.933709449929477e-05, 'memory_usage_mb': 1532.38671875}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.954224705696106, 'lr': 7.722143864598024e-05, 'memory_usage_mb': 1532.38671875}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 2.070199728012085, 'lr': 7.510578279266572e-05, 'memory_usage_mb': 1532.38671875}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 2.0704987049102783, 'lr': 7.299012693935119e-05, 'memory_usage_mb': 1532.38671875}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 2.071406364440918, 'lr': 7.087447108603666e-05, 'memory_usage_mb': 1532.38671875}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 2.06219744682312, 'lr': 6.875881523272214e-05, 'memory_usage_mb': 1532.38671875}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.807406160980463, 'accuracy': 0.854, 'f1_score': 0.8532099479467901, 'epoch_time_seconds': 8.278603315353394, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.9875391721725464, 'lr': 6.664315937940761e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 2.0250778198242188, 'lr': 6.452750352609308e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 2.0142886638641357, 'lr': 6.241184767277856e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.984904408454895, 'lr': 6.0296191819464026e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 2.0599350929260254, 'lr': 5.81805359661495e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 2.0579352378845215, 'lr': 5.606488011283497e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 2.0701231956481934, 'lr': 5.394922425952045e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.7822649478912354, 'accuracy': 0.848, 'f1_score': 0.8486333859214185, 'epoch_time_seconds': 8.271929025650024, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 2.036663770675659, 'lr': 5.183356840620592e-05, 'memory_usage_mb': 1542.69921875}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.9552290439605713, 'lr': 4.971791255289139e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.9910002946853638, 'lr': 4.760225669957687e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 2.0558395385742188, 'lr': 4.5486600846262336e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.9531450271606445, 'lr': 4.337094499294781e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 2.0107882022857666, 'lr': 4.125528913963329e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.7803924605250359, 'accuracy': 0.848, 'f1_score': 0.8473062413516859, 'epoch_time_seconds': 8.283512115478516, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 2.0066051483154297, 'lr': 3.913963328631875e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 2.022697925567627, 'lr': 3.7023977433004226e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 2.0039591789245605, 'lr': 3.49083215796897e-05, 'memory_usage_mb': 1546.76171875}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 2.05114483833313, 'lr': 3.279266572637517e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 1.9929821491241455, 'lr': 3.0677009873060646e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 2.155865430831909, 'lr': 2.856135401974612e-05, 'memory_usage_mb': 1551.13671875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1449: {'epoch': 22, 'eval_loss': 1.7728780768811703, 'accuracy': 0.846, 'f1_score': 0.8453733326964349, 'epoch_time_seconds': 8.26002049446106, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.962445616722107, 'lr': 2.644569816643159e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 2.1182174682617188, 'lr': 2.4330042313117063e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.9827817678451538, 'lr': 2.2214386459802535e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 2.07003116607666, 'lr': 2.009873060648801e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 1.9966206550598145, 'lr': 1.798307475317348e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.948713779449463, 'lr': 1.5867418899858956e-05, 'memory_usage_mb': 1551.13671875}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.9533644914627075, 'lr': 1.3751763046544426e-05, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.7668118998408318, 'accuracy': 0.852, 'f1_score': 0.8520142094480854, 'epoch_time_seconds': 8.296844959259033, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 2.0650904178619385, 'lr': 1.16361071932299e-05, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.9246811866760254, 'lr': 9.520451339915373e-06, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 2.022752285003662, 'lr': 7.404795486600846e-06, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 2.0035953521728516, 'lr': 5.289139633286318e-06, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.9548944234848022, 'lr': 3.1734837799717906e-06, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.9575532674789429, 'lr': 1.0578279266572636e-06, 'memory_usage_mb': 1560.19921875}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.7668670639395714, 'accuracy': 0.854, 'f1_score': 0.8539263612012301, 'epoch_time_seconds': 8.271525621414185, 'memory_usage_mb': 1565.19921875}\n",
      "Step 1575: {'total_training_time_seconds': 214.08846426010132, 'avg_epoch_time_seconds': 8.561752910614013, 'best_accuracy': 0.86, 'convergence_step': 'Not converged', 'memory_usage_mb': 1565.19921875}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 8.56175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.27153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.76687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.85393\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1565.19922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 214.08846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.95755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.01_ag_news_20250421_161925\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/gc8ggl5h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_161925-gc8ggl5h/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4f173",
   "metadata": {},
   "source": [
    "### AdaFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6c21533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_162406-qzqxijny\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adafactor_warmup_lr0.0003_wd0.01_ag_news_20250421_162406\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/qzqxijny\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0, 'memory_usage_mb': 1343.33984375}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 34.36098098754883, 'lr': 1.9108280254777068e-05, 'memory_usage_mb': 1379.8203125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 32.9541015625, 'lr': 3.8216560509554137e-05, 'memory_usage_mb': 1380.1328125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 32.36995315551758, 'lr': 5.732484076433121e-05, 'memory_usage_mb': 1380.1328125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 31.07559585571289, 'lr': 7.643312101910827e-05, 'memory_usage_mb': 1380.1328125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 31.528671264648438, 'lr': 9.554140127388533e-05, 'memory_usage_mb': 1380.1328125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 30.088783264160156, 'lr': 0.00011464968152866242, 'memory_usage_mb': 1380.1328125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 28.925231635570526, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 13.427789449691772, 'memory_usage_mb': 1492.0390625}\n",
      "New best accuracy: 0.2500\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 27.87065315246582, 'lr': 0.00013375796178343948, 'memory_usage_mb': 1494.2265625}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 26.692523956298828, 'lr': 0.00015286624203821655, 'memory_usage_mb': 1494.2265625}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 23.180644989013672, 'lr': 0.0001719745222929936, 'memory_usage_mb': 1494.2265625}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 20.824020385742188, 'lr': 0.00019108280254777067, 'memory_usage_mb': 1494.2265625}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 17.950658798217773, 'lr': 0.00021019108280254773, 'memory_usage_mb': 1494.2265625}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 14.470695495605469, 'lr': 0.00022929936305732485, 'memory_usage_mb': 1494.2265625}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 8.922459810972214, 'accuracy': 0.26, 'f1_score': 0.12823527217383587, 'epoch_time_seconds': 12.977197170257568, 'memory_usage_mb': 1494.2265625}\n",
      "New best accuracy: 0.2600\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 11.213851928710938, 'lr': 0.0002484076433121019, 'memory_usage_mb': 1494.2265625}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 9.502738952636719, 'lr': 0.00026751592356687897, 'memory_usage_mb': 1494.2265625}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 7.924943923950195, 'lr': 0.00028662420382165603, 'memory_usage_mb': 1494.2265625}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 7.36043643951416, 'lr': 0.0002993653032440056, 'memory_usage_mb': 1494.2265625}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 5.913516998291016, 'lr': 0.0002972496473906911, 'memory_usage_mb': 1494.2265625}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 5.2873334884643555, 'lr': 0.00029513399153737656, 'memory_usage_mb': 1494.2265625}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.036609269678593, 'accuracy': 0.604, 'f1_score': 0.574288698743963, 'epoch_time_seconds': 10.877187013626099, 'memory_usage_mb': 1494.2265625}\n",
      "New best accuracy: 0.6040\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.57325553894043, 'lr': 0.00029301833568406207, 'memory_usage_mb': 1494.2265625}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.475767612457275, 'lr': 0.0002909026798307475, 'memory_usage_mb': 1494.2265625}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.9967122077941895, 'lr': 0.000288787023977433, 'memory_usage_mb': 1494.2265625}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.9175286293029785, 'lr': 0.00028667136812411843, 'memory_usage_mb': 1494.2265625}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.764636993408203, 'lr': 0.0002845557122708039, 'memory_usage_mb': 1494.2265625}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.684487819671631, 'lr': 0.0002824400564174894, 'memory_usage_mb': 1494.2265625}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.6406500339508057, 'lr': 0.00028032440056417486, 'memory_usage_mb': 1494.2265625}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.5106920301914215, 'accuracy': 0.774, 'f1_score': 0.773896464659758, 'epoch_time_seconds': 10.38873553276062, 'memory_usage_mb': 1494.5390625}\n",
      "New best accuracy: 0.7740\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.6203622817993164, 'lr': 0.00027820874471086036, 'memory_usage_mb': 1494.5390625}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.606372117996216, 'lr': 0.0002760930888575458, 'memory_usage_mb': 1494.5390625}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.6695125102996826, 'lr': 0.0002739774330042313, 'memory_usage_mb': 1494.5390625}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.4510841369628906, 'lr': 0.0002718617771509168, 'memory_usage_mb': 1494.5390625}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.4344441890716553, 'lr': 0.00026974612129760224, 'memory_usage_mb': 1494.5390625}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.6369807720184326, 'lr': 0.0002676304654442877, 'memory_usage_mb': 1494.5390625}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.3116405457258224, 'accuracy': 0.72, 'f1_score': 0.7119453055010172, 'epoch_time_seconds': 10.379107475280762, 'memory_usage_mb': 1494.5390625}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.439720630645752, 'lr': 0.00026551480959097315, 'memory_usage_mb': 1494.5390625}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.4075493812561035, 'lr': 0.00026339915373765866, 'memory_usage_mb': 1494.5390625}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.3000833988189697, 'lr': 0.0002612834978843441, 'memory_usage_mb': 1494.5390625}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.224780559539795, 'lr': 0.00025916784203102957, 'memory_usage_mb': 1494.5390625}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.216132879257202, 'lr': 0.0002570521861777151, 'memory_usage_mb': 1494.5390625}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.253157615661621, 'lr': 0.00025493653032440054, 'memory_usage_mb': 1494.5390625}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.0395811274647713, 'accuracy': 0.802, 'f1_score': 0.79471297770342, 'epoch_time_seconds': 10.3784761428833, 'memory_usage_mb': 1494.5390625}\n",
      "New best accuracy: 0.8020\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.2315635681152344, 'lr': 0.00025282087447108605, 'memory_usage_mb': 1494.5390625}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.3250608444213867, 'lr': 0.0002507052186177715, 'memory_usage_mb': 1494.5390625}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.0986180305480957, 'lr': 0.00024858956276445696, 'memory_usage_mb': 1494.5390625}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.074129104614258, 'lr': 0.00024647390691114247, 'memory_usage_mb': 1494.5390625}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.017298936843872, 'lr': 0.00024435825105782787, 'memory_usage_mb': 1494.5390625}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.003291368484497, 'lr': 0.00024224259520451338, 'memory_usage_mb': 1494.5390625}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.0403223037719727, 'lr': 0.00024012693935119883, 'memory_usage_mb': 1494.5390625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 441: {'epoch': 6, 'eval_loss': 2.7287978753447533, 'accuracy': 0.812, 'f1_score': 0.8088936858231126, 'epoch_time_seconds': 10.402098178863525, 'memory_usage_mb': 1494.5390625}\n",
      "New best accuracy: 0.8120\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.0163559913635254, 'lr': 0.00023801128349788432, 'memory_usage_mb': 1494.8515625}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.8961446285247803, 'lr': 0.0002358956276445698, 'memory_usage_mb': 1494.8515625}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.9659616947174072, 'lr': 0.00023377997179125528, 'memory_usage_mb': 1494.8515625}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.8304026126861572, 'lr': 0.00023166431593794074, 'memory_usage_mb': 1494.8515625}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.7830729484558105, 'lr': 0.00022954866008462622, 'memory_usage_mb': 1494.8515625}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.7103419303894043, 'lr': 0.0002274330042313117, 'memory_usage_mb': 1494.8515625}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.473983481526375, 'accuracy': 0.81, 'f1_score': 0.8056209695989597, 'epoch_time_seconds': 10.385480165481567, 'memory_usage_mb': 1494.8515625}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.6733500957489014, 'lr': 0.00022531734837799718, 'memory_usage_mb': 1494.8515625}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.744882345199585, 'lr': 0.0002232016925246826, 'memory_usage_mb': 1494.8515625}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.673474073410034, 'lr': 0.0002210860366713681, 'memory_usage_mb': 1494.8515625}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.692366600036621, 'lr': 0.00021897038081805358, 'memory_usage_mb': 1494.8515625}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.5639777183532715, 'lr': 0.00021685472496473906, 'memory_usage_mb': 1494.8515625}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.642482280731201, 'lr': 0.00021473906911142451, 'memory_usage_mb': 1494.8515625}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.305013529956341, 'accuracy': 0.822, 'f1_score': 0.8173811957024887, 'epoch_time_seconds': 10.36776351928711, 'memory_usage_mb': 1494.8515625}\n",
      "New best accuracy: 0.8220\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.5770421028137207, 'lr': 0.00021262341325811, 'memory_usage_mb': 1494.8515625}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.596571922302246, 'lr': 0.00021050775740479548, 'memory_usage_mb': 1494.8515625}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.5213911533355713, 'lr': 0.00020839210155148096, 'memory_usage_mb': 1494.8515625}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.5916833877563477, 'lr': 0.00020627644569816642, 'memory_usage_mb': 1494.8515625}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.4211299419403076, 'lr': 0.0002041607898448519, 'memory_usage_mb': 1494.8515625}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.533320188522339, 'lr': 0.00020204513399153736, 'memory_usage_mb': 1494.8515625}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.1694473773241043, 'accuracy': 0.776, 'f1_score': 0.7630826726542077, 'epoch_time_seconds': 10.395166158676147, 'memory_usage_mb': 1494.8515625}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.570129156112671, 'lr': 0.0001999294781382228, 'memory_usage_mb': 1494.8515625}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.3677799701690674, 'lr': 0.0001978138222849083, 'memory_usage_mb': 1494.8515625}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.558884620666504, 'lr': 0.00019569816643159378, 'memory_usage_mb': 1494.8515625}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.384619951248169, 'lr': 0.00019358251057827926, 'memory_usage_mb': 1494.8515625}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.3688466548919678, 'lr': 0.00019146685472496471, 'memory_usage_mb': 1494.8515625}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.329655885696411, 'lr': 0.0001893511988716502, 'memory_usage_mb': 1494.8515625}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.321889877319336, 'lr': 0.00018723554301833568, 'memory_usage_mb': 1494.8515625}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.0576588809490204, 'accuracy': 0.85, 'f1_score': 0.8490012625342835, 'epoch_time_seconds': 10.383090496063232, 'memory_usage_mb': 1494.8515625}\n",
      "New best accuracy: 0.8500\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.316795825958252, 'lr': 0.00018511988716502116, 'memory_usage_mb': 1494.8515625}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.26619553565979, 'lr': 0.00018300423131170662, 'memory_usage_mb': 1494.8515625}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.2767364978790283, 'lr': 0.00018088857545839207, 'memory_usage_mb': 1494.8515625}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.2570390701293945, 'lr': 0.00017877291960507755, 'memory_usage_mb': 1494.8515625}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.3407678604125977, 'lr': 0.000176657263751763, 'memory_usage_mb': 1494.8515625}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.33162260055542, 'lr': 0.0001745416078984485, 'memory_usage_mb': 1494.8515625}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 1.9857475720345974, 'accuracy': 0.832, 'f1_score': 0.8298823844280514, 'epoch_time_seconds': 10.525896310806274, 'memory_usage_mb': 1494.8515625}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.2155847549438477, 'lr': 0.00017242595204513398, 'memory_usage_mb': 1494.8515625}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.209362506866455, 'lr': 0.00017031029619181946, 'memory_usage_mb': 1494.8515625}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.289139747619629, 'lr': 0.00016819464033850494, 'memory_usage_mb': 1494.8515625}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.1820411682128906, 'lr': 0.0001660789844851904, 'memory_usage_mb': 1494.8515625}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.2368690967559814, 'lr': 0.00016396332863187588, 'memory_usage_mb': 1494.8515625}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.1899497509002686, 'lr': 0.00016184767277856136, 'memory_usage_mb': 1494.8515625}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 1.9295915737748146, 'accuracy': 0.828, 'f1_score': 0.8216821713507664, 'epoch_time_seconds': 10.400307893753052, 'memory_usage_mb': 1494.8515625}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.20947265625, 'lr': 0.0001597320169252468, 'memory_usage_mb': 1494.8515625}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.2378878593444824, 'lr': 0.00015761636107193227, 'memory_usage_mb': 1494.8515625}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.167081356048584, 'lr': 0.00015550070521861775, 'memory_usage_mb': 1494.8515625}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.1270689964294434, 'lr': 0.00015338504936530324, 'memory_usage_mb': 1494.8515625}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.263850212097168, 'lr': 0.0001512693935119887, 'memory_usage_mb': 1494.8515625}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.0753095149993896, 'lr': 0.00014915373765867417, 'memory_usage_mb': 1494.8515625}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.0493297576904297, 'lr': 0.00014703808180535966, 'memory_usage_mb': 1494.8515625}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.863151989877224, 'accuracy': 0.828, 'f1_score': 0.8244132550750976, 'epoch_time_seconds': 10.398513317108154, 'memory_usage_mb': 1494.8515625}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.081557035446167, 'lr': 0.00014492242595204514, 'memory_usage_mb': 1494.8515625}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.1685848236083984, 'lr': 0.0001428067700987306, 'memory_usage_mb': 1494.8515625}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.063330888748169, 'lr': 0.00014069111424541608, 'memory_usage_mb': 1494.8515625}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.0997707843780518, 'lr': 0.00013857545839210153, 'memory_usage_mb': 1494.8515625}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.030336380004883, 'lr': 0.00013645980253878702, 'memory_usage_mb': 1494.8515625}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.055290699005127, 'lr': 0.0001343441466854725, 'memory_usage_mb': 1494.8515625}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 1.8133409284055233, 'accuracy': 0.858, 'f1_score': 0.8569676586893971, 'epoch_time_seconds': 10.360479593276978, 'memory_usage_mb': 1494.8515625}\n",
      "New best accuracy: 0.8580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 2.010601282119751, 'lr': 0.00013222849083215795, 'memory_usage_mb': 1494.8515625}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 1.984780192375183, 'lr': 0.00013011283497884344, 'memory_usage_mb': 1494.8515625}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 2.0198869705200195, 'lr': 0.0001279971791255289, 'memory_usage_mb': 1494.8515625}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.1307857036590576, 'lr': 0.00012588152327221437, 'memory_usage_mb': 1494.8515625}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 2.041376829147339, 'lr': 0.00012376586741889986, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 2.057826042175293, 'lr': 0.00012165021156558531, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.7981105335056782, 'accuracy': 0.83, 'f1_score': 0.8285270039918006, 'epoch_time_seconds': 10.372056007385254, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 2.1077864170074463, 'lr': 0.0001195345557122708, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 2.0439326763153076, 'lr': 0.00011741889985895626, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 1.9611594676971436, 'lr': 0.00011530324400564175, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 2.046433210372925, 'lr': 0.00011318758815232721, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 1.982957124710083, 'lr': 0.00011107193229901268, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 1.9685150384902954, 'lr': 0.00010895627644569815, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 1.947218656539917, 'lr': 0.00010684062059238363, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.7383807562291622, 'accuracy': 0.864, 'f1_score': 0.8637133741943183, 'epoch_time_seconds': 10.393128633499146, 'memory_usage_mb': 1494.8515625}\n",
      "New best accuracy: 0.8640\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 1.9877878427505493, 'lr': 0.0001047249647390691, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 2.0846927165985107, 'lr': 0.00010260930888575459, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 2.048534393310547, 'lr': 0.00010049365303244004, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 1.9123024940490723, 'lr': 9.837799717912551e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 1.9934407472610474, 'lr': 9.626234132581099e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 1.995422601699829, 'lr': 9.414668547249648e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.7090014070272446, 'accuracy': 0.858, 'f1_score': 0.8579119290451704, 'epoch_time_seconds': 10.396969556808472, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 1.9359303712844849, 'lr': 9.203102961918194e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 1.8932666778564453, 'lr': 8.99153737658674e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 2.0975089073181152, 'lr': 8.779971791255288e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 1.9917688369750977, 'lr': 8.568406205923835e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 1.933172583580017, 'lr': 8.356840620592383e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 1.9480398893356323, 'lr': 8.14527503526093e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.7095992602407932, 'accuracy': 0.842, 'f1_score': 0.8403100696594034, 'epoch_time_seconds': 10.393044471740723, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 1.9318422079086304, 'lr': 7.933709449929477e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.841848373413086, 'lr': 7.722143864598024e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 1.9142721891403198, 'lr': 7.510578279266572e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 1.9334486722946167, 'lr': 7.299012693935119e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 1.9262293577194214, 'lr': 7.087447108603666e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 1.9420946836471558, 'lr': 6.875881523272214e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.6969766914844513, 'accuracy': 0.866, 'f1_score': 0.8651551981017931, 'epoch_time_seconds': 10.38186264038086, 'memory_usage_mb': 1494.8515625}\n",
      "New best accuracy: 0.8660\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.8769785165786743, 'lr': 6.664315937940761e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 1.8769358396530151, 'lr': 6.452750352609308e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 1.9080129861831665, 'lr': 6.241184767277856e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.881408452987671, 'lr': 6.0296191819464026e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 1.9379878044128418, 'lr': 5.81805359661495e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 1.8868443965911865, 'lr': 5.606488011283497e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 1.9132035970687866, 'lr': 5.394922425952045e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.671545870602131, 'accuracy': 0.862, 'f1_score': 0.8616243243075625, 'epoch_time_seconds': 10.386195421218872, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 1.9592441320419312, 'lr': 5.183356840620592e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.8295352458953857, 'lr': 4.971791255289139e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.890568733215332, 'lr': 4.760225669957687e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 1.9402172565460205, 'lr': 4.5486600846262336e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.8615498542785645, 'lr': 4.337094499294781e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 1.8753045797348022, 'lr': 4.125528913963329e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.6652839183807373, 'accuracy': 0.866, 'f1_score': 0.8655232490512917, 'epoch_time_seconds': 10.381391286849976, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 1.9036537408828735, 'lr': 3.913963328631875e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 1.8607239723205566, 'lr': 3.7023977433004226e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 1.839838981628418, 'lr': 3.49083215796897e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 1.9164083003997803, 'lr': 3.279266572637517e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 1.8630434274673462, 'lr': 3.0677009873060646e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 2.051466703414917, 'lr': 2.856135401974612e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 1.6641982421278954, 'accuracy': 0.854, 'f1_score': 0.8526558771077429, 'epoch_time_seconds': 10.43841028213501, 'memory_usage_mb': 1494.8515625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.847814917564392, 'lr': 2.644569816643159e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 1.9505014419555664, 'lr': 2.4330042313117063e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.8773308992385864, 'lr': 2.2214386459802535e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 1.9938267469406128, 'lr': 2.009873060648801e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 1.873185396194458, 'lr': 1.798307475317348e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.8221237659454346, 'lr': 1.5867418899858956e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.8434897661209106, 'lr': 1.3751763046544426e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.6554801538586617, 'accuracy': 0.866, 'f1_score': 0.865315526940461, 'epoch_time_seconds': 10.388096332550049, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 1.9592013359069824, 'lr': 1.16361071932299e-05, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.880821704864502, 'lr': 9.520451339915373e-06, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 1.9111868143081665, 'lr': 7.404795486600846e-06, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 1.8601579666137695, 'lr': 5.289139633286318e-06, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.8635666370391846, 'lr': 3.1734837799717906e-06, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.869523048400879, 'lr': 1.0578279266572636e-06, 'memory_usage_mb': 1494.8515625}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.6564725637435913, 'accuracy': 0.868, 'f1_score': 0.8672764488035151, 'epoch_time_seconds': 10.39651894569397, 'memory_usage_mb': 1494.8515625}\n",
      "New best accuracy: 0.8680\n",
      "Step 1575: {'total_training_time_seconds': 266.02227449417114, 'avg_epoch_time_seconds': 10.63899847984314, 'best_accuracy': 0.868, 'convergence_step': 'Not converged', 'memory_usage_mb': 1494.8515625}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 10.639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 10.39652\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.65647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.86728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1494.85156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 266.02227\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.86952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adafactor_warmup_lr0.0003_wd0.01_ag_news_20250421_162406\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/qzqxijny\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_162406-qzqxijny/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adafactor \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b956f6",
   "metadata": {},
   "source": [
    "### Lion Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe68e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_162857-xjf3lfr0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_lion_warmup_lr0.0003_wd0.01_ag_news_20250421_162857\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/xjf3lfr0\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0, 'memory_usage_mb': 1311.4921875}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 34.35332489013672, 'lr': 1.9108280254777068e-05, 'memory_usage_mb': 1313.9921875}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 32.90130615234375, 'lr': 3.8216560509554137e-05, 'memory_usage_mb': 1314.3046875}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 32.276920318603516, 'lr': 5.732484076433121e-05, 'memory_usage_mb': 1314.3046875}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 30.761343002319336, 'lr': 7.643312101910827e-05, 'memory_usage_mb': 1314.3046875}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 30.83897590637207, 'lr': 9.554140127388533e-05, 'memory_usage_mb': 1314.3046875}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 29.422706604003906, 'lr': 0.00011464968152866242, 'memory_usage_mb': 1314.3046875}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 27.619964361190796, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 11.808895111083984, 'memory_usage_mb': 1416.984375}\n",
      "New best accuracy: 0.2500\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 26.308319091796875, 'lr': 0.00013375796178343948, 'memory_usage_mb': 1418.859375}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 25.4589786529541, 'lr': 0.00015286624203821655, 'memory_usage_mb': 1418.859375}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 22.206090927124023, 'lr': 0.0001719745222929936, 'memory_usage_mb': 1418.859375}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 20.3458194732666, 'lr': 0.00019108280254777067, 'memory_usage_mb': 1418.859375}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 18.703229904174805, 'lr': 0.00021019108280254773, 'memory_usage_mb': 1418.859375}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 16.8834171295166, 'lr': 0.00022929936305732485, 'memory_usage_mb': 1418.859375}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 13.400740683078766, 'accuracy': 0.244, 'f1_score': 0.09571704180064307, 'epoch_time_seconds': 11.44419527053833, 'memory_usage_mb': 1419.171875}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 14.447463035583496, 'lr': 0.0002484076433121019, 'memory_usage_mb': 1419.171875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 11.777130126953125, 'lr': 0.00026751592356687897, 'memory_usage_mb': 1419.171875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 9.683854103088379, 'lr': 0.00028662420382165603, 'memory_usage_mb': 1419.171875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 9.005757331848145, 'lr': 0.0002993653032440056, 'memory_usage_mb': 1419.171875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 7.755100727081299, 'lr': 0.0002972496473906911, 'memory_usage_mb': 1419.171875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 7.063311576843262, 'lr': 0.00029513399153737656, 'memory_usage_mb': 1419.171875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 5.26101054251194, 'accuracy': 0.316, 'f1_score': 0.22256876920862048, 'epoch_time_seconds': 9.356215238571167, 'memory_usage_mb': 1419.171875}\n",
      "New best accuracy: 0.3160\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 6.721736431121826, 'lr': 0.00029301833568406207, 'memory_usage_mb': 1419.171875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 5.808251857757568, 'lr': 0.0002909026798307475, 'memory_usage_mb': 1419.171875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 5.651321887969971, 'lr': 0.000288787023977433, 'memory_usage_mb': 1419.171875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 5.141834735870361, 'lr': 0.00028667136812411843, 'memory_usage_mb': 1419.171875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 4.7039875984191895, 'lr': 0.0002845557122708039, 'memory_usage_mb': 1419.171875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.500013828277588, 'lr': 0.0002824400564174894, 'memory_usage_mb': 1419.171875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 4.156192779541016, 'lr': 0.00028032440056417486, 'memory_usage_mb': 1419.171875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.918861098587513, 'accuracy': 0.404, 'f1_score': 0.2850937206634574, 'epoch_time_seconds': 8.843152284622192, 'memory_usage_mb': 1419.171875}\n",
      "New best accuracy: 0.4040\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.1781005859375, 'lr': 0.00027820874471086036, 'memory_usage_mb': 1419.171875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 4.0625319480896, 'lr': 0.0002760930888575458, 'memory_usage_mb': 1419.171875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.910625696182251, 'lr': 0.0002739774330042313, 'memory_usage_mb': 1419.171875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.835541009902954, 'lr': 0.0002718617771509168, 'memory_usage_mb': 1419.171875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.802002429962158, 'lr': 0.00026974612129760224, 'memory_usage_mb': 1419.171875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.734096050262451, 'lr': 0.0002676304654442877, 'memory_usage_mb': 1419.171875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.7676273584365845, 'accuracy': 0.254, 'f1_score': 0.10306070287539935, 'epoch_time_seconds': 8.825748205184937, 'memory_usage_mb': 1419.484375}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.7860324382781982, 'lr': 0.00026551480959097315, 'memory_usage_mb': 1419.484375}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.6952574253082275, 'lr': 0.00026339915373765866, 'memory_usage_mb': 1419.484375}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.6462230682373047, 'lr': 0.0002612834978843441, 'memory_usage_mb': 1419.484375}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.620943069458008, 'lr': 0.00025916784203102957, 'memory_usage_mb': 1419.484375}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.5273447036743164, 'lr': 0.0002570521861777151, 'memory_usage_mb': 1419.484375}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.4646477699279785, 'lr': 0.00025493653032440054, 'memory_usage_mb': 1419.484375}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.423342280089855, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.84971284866333, 'memory_usage_mb': 1419.484375}\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.5931308269500732, 'lr': 0.00025282087447108605, 'memory_usage_mb': 1419.484375}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.5285565853118896, 'lr': 0.0002507052186177715, 'memory_usage_mb': 1419.484375}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.6897757053375244, 'lr': 0.00024858956276445696, 'memory_usage_mb': 1419.484375}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.3632314205169678, 'lr': 0.00024647390691114247, 'memory_usage_mb': 1419.484375}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.4170053005218506, 'lr': 0.00024435825105782787, 'memory_usage_mb': 1419.484375}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.3202285766601562, 'lr': 0.00024224259520451338, 'memory_usage_mb': 1419.484375}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.199615001678467, 'lr': 0.00024012693935119883, 'memory_usage_mb': 1419.484375}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 3.2073702961206436, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.83643627166748, 'memory_usage_mb': 1419.484375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.5087859630584717, 'lr': 0.00023801128349788432, 'memory_usage_mb': 1419.484375}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.206023931503296, 'lr': 0.0002358956276445698, 'memory_usage_mb': 1419.484375}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.1423275470733643, 'lr': 0.00023377997179125528, 'memory_usage_mb': 1419.484375}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.1038124561309814, 'lr': 0.00023166431593794074, 'memory_usage_mb': 1419.484375}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.1180667877197266, 'lr': 0.00022954866008462622, 'memory_usage_mb': 1419.484375}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.0906875133514404, 'lr': 0.0002274330042313117, 'memory_usage_mb': 1419.484375}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.907024823129177, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.847262382507324, 'memory_usage_mb': 1419.484375}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.000460386276245, 'lr': 0.00022531734837799718, 'memory_usage_mb': 1419.484375}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.4544880390167236, 'lr': 0.0002232016925246826, 'memory_usage_mb': 1419.484375}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.350116014480591, 'lr': 0.0002210860366713681, 'memory_usage_mb': 1419.484375}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 3.0318899154663086, 'lr': 0.00021897038081805358, 'memory_usage_mb': 1419.484375}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 3.202030897140503, 'lr': 0.00021685472496473906, 'memory_usage_mb': 1419.484375}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 3.115230083465576, 'lr': 0.00021473906911142451, 'memory_usage_mb': 1419.484375}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.74125012755394, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.821113348007202, 'memory_usage_mb': 1419.484375}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 3.0516576766967773, 'lr': 0.00021262341325811, 'memory_usage_mb': 1419.484375}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.8087379932403564, 'lr': 0.00021050775740479548, 'memory_usage_mb': 1419.484375}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.7610692977905273, 'lr': 0.00020839210155148096, 'memory_usage_mb': 1419.484375}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.776587724685669, 'lr': 0.00020627644569816642, 'memory_usage_mb': 1419.484375}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.7200748920440674, 'lr': 0.0002041607898448519, 'memory_usage_mb': 1419.484375}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.7240915298461914, 'lr': 0.00020204513399153736, 'memory_usage_mb': 1419.484375}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.541605718433857, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.871950149536133, 'memory_usage_mb': 1419.484375}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.6925928592681885, 'lr': 0.0001999294781382228, 'memory_usage_mb': 1419.484375}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.584336042404175, 'lr': 0.0001978138222849083, 'memory_usage_mb': 1419.484375}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.6572773456573486, 'lr': 0.00019569816643159378, 'memory_usage_mb': 1419.484375}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.5987355709075928, 'lr': 0.00019358251057827926, 'memory_usage_mb': 1419.484375}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.5513341426849365, 'lr': 0.00019146685472496471, 'memory_usage_mb': 1419.484375}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.5095582008361816, 'lr': 0.0001893511988716502, 'memory_usage_mb': 1419.484375}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.4742238521575928, 'lr': 0.00018723554301833568, 'memory_usage_mb': 1419.484375}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.402871273458004, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.858172178268433, 'memory_usage_mb': 1419.484375}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.4781408309936523, 'lr': 0.00018511988716502116, 'memory_usage_mb': 1419.484375}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.432873249053955, 'lr': 0.00018300423131170662, 'memory_usage_mb': 1419.484375}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.6944706439971924, 'lr': 0.00018088857545839207, 'memory_usage_mb': 1419.484375}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.3523924350738525, 'lr': 0.00017877291960507755, 'memory_usage_mb': 1419.484375}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.397341012954712, 'lr': 0.000176657263751763, 'memory_usage_mb': 1419.484375}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.349060535430908, 'lr': 0.0001745416078984485, 'memory_usage_mb': 1419.484375}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.2430426999926567, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.83008861541748, 'memory_usage_mb': 1419.484375}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.319948434829712, 'lr': 0.00017242595204513398, 'memory_usage_mb': 1419.484375}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.333157539367676, 'lr': 0.00017031029619181946, 'memory_usage_mb': 1419.484375}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.425600528717041, 'lr': 0.00016819464033850494, 'memory_usage_mb': 1419.484375}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.374908685684204, 'lr': 0.0001660789844851904, 'memory_usage_mb': 1419.484375}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.277615547180176, 'lr': 0.00016396332863187588, 'memory_usage_mb': 1419.484375}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.266913890838623, 'lr': 0.00016184767277856136, 'memory_usage_mb': 1419.484375}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.121505506336689, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.848451137542725, 'memory_usage_mb': 1419.484375}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.2684414386749268, 'lr': 0.0001597320169252468, 'memory_usage_mb': 1419.484375}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.227996349334717, 'lr': 0.00015761636107193227, 'memory_usage_mb': 1419.484375}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.273942708969116, 'lr': 0.00015550070521861775, 'memory_usage_mb': 1419.484375}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.298678159713745, 'lr': 0.00015338504936530324, 'memory_usage_mb': 1419.484375}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.372101068496704, 'lr': 0.0001512693935119887, 'memory_usage_mb': 1419.484375}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.204192638397217, 'lr': 0.00014915373765867417, 'memory_usage_mb': 1419.484375}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.355741262435913, 'lr': 0.00014703808180535966, 'memory_usage_mb': 1419.484375}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.034306611865759, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.844523668289185, 'memory_usage_mb': 1419.484375}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.4245190620422363, 'lr': 0.00014492242595204514, 'memory_usage_mb': 1419.484375}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.2671196460723877, 'lr': 0.0001428067700987306, 'memory_usage_mb': 1419.484375}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.226337194442749, 'lr': 0.00014069111424541608, 'memory_usage_mb': 1419.484375}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.138596773147583, 'lr': 0.00013857545839210153, 'memory_usage_mb': 1419.484375}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.3687527179718018, 'lr': 0.00013645980253878702, 'memory_usage_mb': 1419.484375}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.0892975330352783, 'lr': 0.0001343441466854725, 'memory_usage_mb': 1419.484375}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 1.99079829454422, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.90281867980957, 'memory_usage_mb': 1419.484375}\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 2.434154748916626, 'lr': 0.00013222849083215795, 'memory_usage_mb': 1419.484375}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 2.065312385559082, 'lr': 0.00013011283497884344, 'memory_usage_mb': 1419.484375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 2.0900518894195557, 'lr': 0.0001279971791255289, 'memory_usage_mb': 1419.484375}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.079824209213257, 'lr': 0.00012588152327221437, 'memory_usage_mb': 1419.484375}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 2.0870585441589355, 'lr': 0.00012376586741889986, 'memory_usage_mb': 1419.484375}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 2.0950605869293213, 'lr': 0.00012165021156558531, 'memory_usage_mb': 1419.484375}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.9519553743302822, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 9.017604351043701, 'memory_usage_mb': 1419.484375}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 2.106989860534668, 'lr': 0.0001195345557122708, 'memory_usage_mb': 1419.796875}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 2.086254596710205, 'lr': 0.00011741889985895626, 'memory_usage_mb': 1419.796875}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 1.9831174612045288, 'lr': 0.00011530324400564175, 'memory_usage_mb': 1419.796875}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 2.0737574100494385, 'lr': 0.00011318758815232721, 'memory_usage_mb': 1419.796875}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 1.9807246923446655, 'lr': 0.00011107193229901268, 'memory_usage_mb': 1419.796875}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 1.9891022443771362, 'lr': 0.00010895627644569815, 'memory_usage_mb': 1419.796875}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 2.0237936973571777, 'lr': 0.00010684062059238363, 'memory_usage_mb': 1419.796875}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.856217909604311, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.84806203842163, 'memory_usage_mb': 1419.796875}\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 2.0482800006866455, 'lr': 0.0001047249647390691, 'memory_usage_mb': 1419.796875}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 2.039360284805298, 'lr': 0.00010260930888575459, 'memory_usage_mb': 1419.796875}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 1.9743292331695557, 'lr': 0.00010049365303244004, 'memory_usage_mb': 1419.796875}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 1.9374582767486572, 'lr': 9.837799717912551e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 1.9605909585952759, 'lr': 9.626234132581099e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 1.9658538103103638, 'lr': 9.414668547249648e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.8089607059955597, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.84826135635376, 'memory_usage_mb': 1419.796875}\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 1.9399369955062866, 'lr': 9.203102961918194e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 1.9347658157348633, 'lr': 8.99153737658674e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 2.066908597946167, 'lr': 8.779971791255288e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 1.9585455656051636, 'lr': 8.568406205923835e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 2.3432154655456543, 'lr': 8.356840620592383e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 1.9563618898391724, 'lr': 8.14527503526093e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.8022627048194408, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.831807136535645, 'memory_usage_mb': 1419.796875}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 1.9558155536651611, 'lr': 7.933709449929477e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.9144635200500488, 'lr': 7.722143864598024e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 1.9635237455368042, 'lr': 7.510578279266572e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 1.9147558212280273, 'lr': 7.299012693935119e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 1.916745901107788, 'lr': 7.087447108603666e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 1.894616723060608, 'lr': 6.875881523272214e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.7611074149608612, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.939039468765259, 'memory_usage_mb': 1419.796875}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.9272606372833252, 'lr': 6.664315937940761e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 1.8261032104492188, 'lr': 6.452750352609308e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 1.8706785440444946, 'lr': 6.241184767277856e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.9494596719741821, 'lr': 6.0296191819464026e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 1.8825726509094238, 'lr': 5.81805359661495e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 1.9012689590454102, 'lr': 5.606488011283497e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 1.8771063089370728, 'lr': 5.394922425952045e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.76029646769166, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.862777709960938, 'memory_usage_mb': 1419.796875}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 1.8457913398742676, 'lr': 5.183356840620592e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.910521388053894, 'lr': 4.971791255289139e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.91426420211792, 'lr': 4.760225669957687e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 1.8754035234451294, 'lr': 4.5486600846262336e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.901047945022583, 'lr': 4.337094499294781e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 1.8940068483352661, 'lr': 4.125528913963329e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.7292711772024632, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.83487844467163, 'memory_usage_mb': 1419.796875}\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 1.926147222518921, 'lr': 3.913963328631875e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 1.8754264116287231, 'lr': 3.7023977433004226e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 1.8671859502792358, 'lr': 3.49083215796897e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 1.8954157829284668, 'lr': 3.279266572637517e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 1.8322502374649048, 'lr': 3.0677009873060646e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 1.9071859121322632, 'lr': 2.856135401974612e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 1.7071005254983902, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.860249042510986, 'memory_usage_mb': 1419.796875}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.8397046327590942, 'lr': 2.644569816643159e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 1.930797815322876, 'lr': 2.4330042313117063e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.8327512741088867, 'lr': 2.2214386459802535e-05, 'memory_usage_mb': 1419.796875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 1.8533530235290527, 'lr': 2.009873060648801e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 1.8120380640029907, 'lr': 1.798307475317348e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.8609899282455444, 'lr': 1.5867418899858956e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.860266089439392, 'lr': 1.3751763046544426e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.706971250474453, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.850960969924927, 'memory_usage_mb': 1419.796875}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 1.8727635145187378, 'lr': 1.16361071932299e-05, 'memory_usage_mb': 1419.796875}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.8591437339782715, 'lr': 9.520451339915373e-06, 'memory_usage_mb': 1419.796875}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 1.882835030555725, 'lr': 7.404795486600846e-06, 'memory_usage_mb': 1419.796875}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 1.8033044338226318, 'lr': 5.289139633286318e-06, 'memory_usage_mb': 1419.796875}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.830869436264038, 'lr': 3.1734837799717906e-06, 'memory_usage_mb': 1419.796875}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.8345566987991333, 'lr': 1.0578279266572636e-06, 'memory_usage_mb': 1419.796875}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.6975929848849773, 'accuracy': 0.254, 'f1_score': 0.10289633173843701, 'epoch_time_seconds': 8.855010271072388, 'memory_usage_mb': 1419.796875}\n",
      "Step 1575: {'total_training_time_seconds': 227.5850043296814, 'avg_epoch_time_seconds': 9.101495447158813, 'best_accuracy': 0.404, 'convergence_step': 'Not converged', 'memory_usage_mb': 1419.796875}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÅ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 9.1015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.404\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.85501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.69759\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.1029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1419.79688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 227.585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.83456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_lion_warmup_lr0.0003_wd0.01_ag_news_20250421_162857\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/xjf3lfr0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_162857-xjf3lfr0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer lion \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c329f83",
   "metadata": {},
   "source": [
    "### CosineAnnealing LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91db027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_163406-rskyv0rd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_cosine_lr0.0003_wd0.01_ag_news_20250421_163406\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/rskyv0rd\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0003, 'memory_usage_mb': 1313.89453125}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 32.3741340637207, 'lr': 0.0002999701609155785, 'memory_usage_mb': 1379.0}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 27.26553726196289, 'lr': 0.0002998806555339269, 'memory_usage_mb': 1379.3125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 23.406156539916992, 'lr': 0.00029973151946516025, 'memory_usage_mb': 1379.3125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 19.619308471679688, 'lr': 0.00029952281204372863, 'memory_usage_mb': 1379.3125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 16.927202224731445, 'lr': 0.0002992546163048102, 'memory_usage_mb': 1379.3125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 11.535784721374512, 'lr': 0.0002989270389512756, 'memory_usage_mb': 1379.3125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 8.29259878396988, 'accuracy': 0.272, 'f1_score': 0.15405068163592622, 'epoch_time_seconds': 11.541016817092896, 'memory_usage_mb': 1481.96875}\n",
      "New best accuracy: 0.2720\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 9.231133460998535, 'lr': 0.0002985402103112355, 'memory_usage_mb': 1484.15625}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 7.947559356689453, 'lr': 0.0002980942842861893, 'memory_usage_mb': 1484.15625}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 7.345113277435303, 'lr': 0.00029758943828979444, 'memory_usage_mb': 1484.15625}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 6.360744953155518, 'lr': 0.00029702587317728153, 'memory_usage_mb': 1484.15625}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 5.281554222106934, 'lr': 0.0002964038131655436, 'memory_usage_mb': 1484.15625}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 4.821225643157959, 'lr': 0.0002957235057439301, 'memory_usage_mb': 1484.15625}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 3.9626448825001717, 'accuracy': 0.666, 'f1_score': 0.6470055520521019, 'epoch_time_seconds': 8.52165675163269, 'memory_usage_mb': 1484.15625}\n",
      "New best accuracy: 0.6660\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 4.446669578552246, 'lr': 0.000294985221575782, 'memory_usage_mb': 1484.15625}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.172039985656738, 'lr': 0.0002941892543907478, 'memory_usage_mb': 1484.15625}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.04648494720459, 'lr': 0.00029333592086792107, 'memory_usage_mb': 1484.15625}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 3.9503984451293945, 'lr': 0.0002924255605098489, 'memory_usage_mb': 1484.15625}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 3.8783395290374756, 'lr': 0.00029145853550745904, 'memory_usage_mb': 1484.15625}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 3.834874391555786, 'lr': 0.00029043523059596053, 'memory_usage_mb': 1484.15625}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.58700180798769, 'accuracy': 0.764, 'f1_score': 0.7616220135909461, 'epoch_time_seconds': 8.561073303222656, 'memory_usage_mb': 1484.15625}\n",
      "New best accuracy: 0.7640\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 3.697645425796509, 'lr': 0.00028935605290177535, 'memory_usage_mb': 1484.15625}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 3.710867404937744, 'lr': 0.00028822143178056114, 'memory_usage_mb': 1484.15625}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.600229501724243, 'lr': 0.0002870318186463901, 'memory_usage_mb': 1484.15625}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.6028404235839844, 'lr': 0.0002857876867921522, 'memory_usage_mb': 1484.15625}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.499927520751953, 'lr': 0.0002844895312012531, 'memory_usage_mb': 1484.15625}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.467383861541748, 'lr': 0.0002831378683506831, 'memory_usage_mb': 1484.15625}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.433725357055664, 'lr': 0.0002817332360055343, 'memory_usage_mb': 1484.15625}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.335764415562153, 'accuracy': 0.792, 'f1_score': 0.7918799210893059, 'epoch_time_seconds': 8.50251030921936, 'memory_usage_mb': 1484.15625}\n",
      "New best accuracy: 0.7920\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.4559531211853027, 'lr': 0.00028027619300504834, 'memory_usage_mb': 1484.15625}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.422909736633301, 'lr': 0.0002787673190402799, 'memory_usage_mb': 1484.15625}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.2872049808502197, 'lr': 0.00027720721442346387, 'memory_usage_mb': 1484.15625}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.206533432006836, 'lr': 0.0002755964998491785, 'memory_usage_mb': 1484.15625}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.196784496307373, 'lr': 0.00027393581614739923, 'memory_usage_mb': 1484.15625}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.136995792388916, 'lr': 0.00027222582402854176, 'memory_usage_mb': 1484.15625}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 2.97068290412426, 'accuracy': 0.796, 'f1_score': 0.7933398643823316, 'epoch_time_seconds': 8.509503364562988, 'memory_usage_mb': 1484.15625}\n",
      "New best accuracy: 0.7960\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.16371488571167, 'lr': 0.00027046720382059526, 'memory_usage_mb': 1484.46875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.1348536014556885, 'lr': 0.0002686606551984512, 'memory_usage_mb': 1484.46875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.039599657058716, 'lr': 0.0002668068969055341, 'memory_usage_mb': 1484.46875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 2.967677354812622, 'lr': 0.00026490666646784665, 'memory_usage_mb': 1484.46875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 2.9140560626983643, 'lr': 0.00026296071990054165, 'memory_usage_mb': 1484.46875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 2.917767286300659, 'lr': 0.0002609698314071376, 'memory_usage_mb': 1484.46875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 2.6487277671694756, 'accuracy': 0.81, 'f1_score': 0.8040505368814193, 'epoch_time_seconds': 8.52681851387024, 'memory_usage_mb': 1484.46875}\n",
      "New best accuracy: 0.8100\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 2.8980870246887207, 'lr': 0.00025893479307149893, 'memory_usage_mb': 1484.46875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 2.949800729751587, 'lr': 0.00025685641454270173, 'memory_usage_mb': 1484.46875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.758523941040039, 'lr': 0.0002547355227129109, 'memory_usage_mb': 1484.46875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.727987051010132, 'lr': 0.000252572961388398, 'memory_usage_mb': 1484.46875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.716320514678955, 'lr': 0.0002503695909538287, 'memory_usage_mb': 1484.46875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.7403597831726074, 'lr': 0.0002481262880299552, 'memory_usage_mb': 1484.46875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 2.7487545013427734, 'lr': 0.0002458439451248484, 'memory_usage_mb': 1484.46875}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 2.497007705271244, 'accuracy': 0.716, 'f1_score': 0.7108078645028031, 'epoch_time_seconds': 8.65112590789795, 'memory_usage_mb': 1484.46875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 2.7118358612060547, 'lr': 0.00024352347027881003, 'memory_usage_mb': 1484.46875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.6174046993255615, 'lr': 0.0002411657867031045, 'memory_usage_mb': 1484.46875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.606999635696411, 'lr': 0.00023877183241265514, 'memory_usage_mb': 1484.46875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.5091912746429443, 'lr': 0.00023634255985285102, 'memory_usage_mb': 1484.46875}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.4946463108062744, 'lr': 0.00023387893552061199, 'memory_usage_mb': 1484.46875}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.492065191268921, 'lr': 0.0002313819395798639, 'memory_usage_mb': 1484.46875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.269767075777054, 'accuracy': 0.814, 'f1_score': 0.812789955155448, 'epoch_time_seconds': 8.622080326080322, 'memory_usage_mb': 1484.46875}\n",
      "New best accuracy: 0.8140\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.4850518703460693, 'lr': 0.00022885256547157566, 'memory_usage_mb': 1484.46875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.484898567199707, 'lr': 0.00022629181951851473, 'memory_usage_mb': 1484.46875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.44104266166687, 'lr': 0.00022370072052487668, 'memory_usage_mb': 1484.46875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.4587717056274414, 'lr': 0.0002210802993709498, 'memory_usage_mb': 1484.46875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.406069040298462, 'lr': 0.00021843159860297442, 'memory_usage_mb': 1484.46875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.3991355895996094, 'lr': 0.0002157556720183616, 'memory_usage_mb': 1484.46875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.1562713235616684, 'accuracy': 0.806, 'f1_score': 0.8055259863251777, 'epoch_time_seconds': 8.629014492034912, 'memory_usage_mb': 1484.46875}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.381202459335327, 'lr': 0.0002130535842464348, 'memory_usage_mb': 1484.46875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.4124441146850586, 'lr': 0.0002103264103248626, 'memory_usage_mb': 1484.46875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.377896785736084, 'lr': 0.00020757523527195005, 'memory_usage_mb': 1484.46875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.4300472736358643, 'lr': 0.00020480115365495926, 'memory_usage_mb': 1484.46875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.28330659866333, 'lr': 0.00020200526915463107, 'memory_usage_mb': 1484.46875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.474177122116089, 'lr': 0.00019918869412608066, 'memory_usage_mb': 1484.46875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.054370664060116, 'accuracy': 0.832, 'f1_score': 0.8296801674641148, 'epoch_time_seconds': 8.694641828536987, 'memory_usage_mb': 1484.46875}\n",
      "New best accuracy: 0.8320\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.4264163970947266, 'lr': 0.0001963525491562421, 'memory_usage_mb': 1484.46875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.2635984420776367, 'lr': 0.00019349796261803793, 'memory_usage_mb': 1484.46875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.3285350799560547, 'lr': 0.00019062607022145078, 'memory_usage_mb': 1484.46875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.320621967315674, 'lr': 0.00018773801456167628, 'memory_usage_mb': 1484.46875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.2997474670410156, 'lr': 0.00018483494466453636, 'memory_usage_mb': 1484.46875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.234410524368286, 'lr': 0.00018191801552933432, 'memory_usage_mb': 1484.46875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.2318506240844727, 'lr': 0.00017898838766933298, 'memory_usage_mb': 1484.46875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 1.9965021163225174, 'accuracy': 0.844, 'f1_score': 0.843764469643279, 'epoch_time_seconds': 8.687564373016357, 'memory_usage_mb': 1484.46875}\n",
      "New best accuracy: 0.8440\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.2466025352478027, 'lr': 0.00017604722665003956, 'memory_usage_mb': 1484.46875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.2147979736328125, 'lr': 0.00017309570262548, 'memory_usage_mb': 1484.46875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.2439558506011963, 'lr': 0.00017013498987264832, 'memory_usage_mb': 1484.46875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.199242353439331, 'lr': 0.00016716626632431477, 'memory_usage_mb': 1484.46875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.268948554992676, 'lr': 0.00016419071310038057, 'memory_usage_mb': 1484.46875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.3648791313171387, 'lr': 0.00016120951403796364, 'memory_usage_mb': 1484.46875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 1.937113169580698, 'accuracy': 0.826, 'f1_score': 0.8242004281949934, 'epoch_time_seconds': 8.707317352294922, 'memory_usage_mb': 1484.46875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.286675214767456, 'lr': 0.000158223855220404, 'memory_usage_mb': 1484.46875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.2022171020507812, 'lr': 0.00015523492450537517, 'memory_usage_mb': 1484.46875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.2570600509643555, 'lr': 0.00015224391105228953, 'memory_usage_mb': 1484.46875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.152207136154175, 'lr': 0.0001492520048491863, 'memory_usage_mb': 1484.46875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.1804003715515137, 'lr': 0.00014626039623928907, 'memory_usage_mb': 1484.46875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.1424686908721924, 'lr': 0.0001432702754474228, 'memory_usage_mb': 1484.46875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 1.8868178389966488, 'accuracy': 0.84, 'f1_score': 0.839355383873597, 'epoch_time_seconds': 8.711693048477173, 'memory_usage_mb': 1484.46875}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.1477737426757812, 'lr': 0.00014028283210647718, 'memory_usage_mb': 1484.46875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.1934430599212646, 'lr': 0.0001372992547841065, 'memory_usage_mb': 1484.46875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.1484158039093018, 'lr': 0.000134320730509852, 'memory_usage_mb': 1484.46875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.095956325531006, 'lr': 0.00013134844430287725, 'memory_usage_mb': 1484.46875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.163606643676758, 'lr': 0.000128383578700503, 'memory_usage_mb': 1484.46875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.0762083530426025, 'lr': 0.00012542731328772934, 'memory_usage_mb': 1484.46875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.0447990894317627, 'lr': 0.00012248082422793266, 'memory_usage_mb': 1484.46875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.849599178880453, 'accuracy': 0.836, 'f1_score': 0.8345843347577938, 'epoch_time_seconds': 8.753793001174927, 'memory_usage_mb': 1484.46875}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.1016314029693604, 'lr': 0.00011954528379492359, 'memory_usage_mb': 1484.46875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.207270860671997, 'lr': 0.00011662185990655284, 'memory_usage_mb': 1484.46875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.063997268676758, 'lr': 0.00011371171566004985, 'memory_usage_mb': 1484.46875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.0994269847869873, 'lr': 0.00011081600886927924, 'memory_usage_mb': 1484.46875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.072434425354004, 'lr': 0.0001079358916040996, 'memory_usage_mb': 1484.46875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.068091869354248, 'lr': 0.0001050725097320071, 'memory_usage_mb': 1484.46875}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 1.836283016949892, 'accuracy': 0.83, 'f1_score': 0.8284146299163149, 'epoch_time_seconds': 8.709378480911255, 'memory_usage_mb': 1484.46875}\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 2.064237594604492, 'lr': 0.00010222700246224735, 'memory_usage_mb': 1484.46875}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 2.008026361465454, 'lr': 9.94005018925755e-05, 'memory_usage_mb': 1484.46875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 2.0716872215270996, 'lr': 9.659413255884647e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.1889450550079346, 'lr': 9.380901098761319e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 2.0622847080230713, 'lr': 9.104624525191145e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 2.074561834335327, 'lr': 8.830693453040829e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.8081525042653084, 'accuracy': 0.844, 'f1_score': 0.8425534559027895, 'epoch_time_seconds': 8.86791706085205, 'memory_usage_mb': 1484.46875}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 2.1362338066101074, 'lr': 8.55921686700886e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 2.101228713989258, 'lr': 8.290302775265509e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 2.0048563480377197, 'lr': 8.024058166481243e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 2.0688564777374268, 'lr': 7.760588967260838e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 2.0582971572875977, 'lr': 7.500000000000002e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 2.0569205284118652, 'lr': 7.242394941181308e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 1.9601563215255737, 'lr': 6.987876280126068e-05, 'memory_usage_mb': 1484.46875}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.7892962731420994, 'accuracy': 0.852, 'f1_score': 0.8517526709807792, 'epoch_time_seconds': 8.714930057525635, 'memory_usage_mb': 1484.46875}\n",
      "New best accuracy: 0.8520\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 2.030944585800171, 'lr': 6.736545278218463e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 2.1375014781951904, 'lr': 6.488501928618274e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 2.158303737640381, 'lr': 6.243844916478155e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 1.9704115390777588, 'lr': 6.002671579681294e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 2.0442183017730713, 'lr': 5.765077870115125e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 2.066612720489502, 'lr': 5.531158315496417e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.7745384648442268, 'accuracy': 0.858, 'f1_score': 0.8585015620285913, 'epoch_time_seconds': 8.69646668434143, 'memory_usage_mb': 1484.78125}\n",
      "New best accuracy: 0.8580\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 2.0015504360198975, 'lr': 5.3010059817630066e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 1.9522658586502075, 'lr': 5.074712436047112e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 2.2018773555755615, 'lr': 4.852367710244921e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 2.075394630432129, 'lr': 4.63406026519703e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 2.018148899078369, 'lr': 4.419876955493869e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 2.0427615642547607, 'lr': 4.209902994920235e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.777025230228901, 'accuracy': 0.84, 'f1_score': 0.8378677645583027, 'epoch_time_seconds': 8.71409273147583, 'memory_usage_mb': 1484.78125}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 1.9950045347213745, 'lr': 4.004221922552608e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.9145067930221558, 'lr': 3.8029155695227474e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 2.046926259994507, 'lr': 3.60606402646083e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 2.0043623447418213, 'lr': 3.413745611631009e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 2.014636516571045, 'lr': 3.226036839772165e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 2.0238301753997803, 'lr': 3.0430123916561672e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.7743688374757767, 'accuracy': 0.84, 'f1_score': 0.8378743407252023, 'epoch_time_seconds': 8.706734657287598, 'memory_usage_mb': 1484.78125}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.9997038841247559, 'lr': 2.8647450843757897e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 1.979238748550415, 'lr': 2.691305842374128e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 2.0101165771484375, 'lr': 2.5227636692269688e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.952231526374817, 'lr': 2.359185620189412e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 2.023913860321045, 'lr': 2.2006367755176655e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 2.0124058723449707, 'lr': 2.0471802145765376e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 2.045527935028076, 'lr': 1.8988769907430552e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.7600407861173153, 'accuracy': 0.848, 'f1_score': 0.8466777709052172, 'epoch_time_seconds': 8.709554195404053, 'memory_usage_mb': 1484.78125}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 2.0276827812194824, 'lr': 1.755786107116095e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.908522605895996, 'lr': 1.6179644930417497e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.965229868888855, 'lr': 1.4854669814637143e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 2.0643205642700195, 'lr': 1.3583462871077672e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.928326964378357, 'lr': 1.2366529855089913e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 1.9884722232818604, 'lr': 1.1204354928900494e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.7598980590701103, 'accuracy': 0.846, 'f1_score': 0.8447478621120129, 'epoch_time_seconds': 8.738599061965942, 'memory_usage_mb': 1484.78125}\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 1.9802476167678833, 'lr': 1.0097400468985855e-05, 'memory_usage_mb': 1484.78125}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 2.012826681137085, 'lr': 9.046106882113751e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 1.9871519804000854, 'lr': 8.050892430125361e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 2.011960744857788, 'lr': 7.112153063528064e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 1.9845887422561646, 'lr': 6.230262263964641e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 2.110780954360962, 'lr': 5.405570895622013e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 1.7589411921799183, 'accuracy': 0.852, 'f1_score': 0.8510885111581781, 'epoch_time_seconds': 8.702975034713745, 'memory_usage_mb': 1484.78125}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.9528976678848267, 'lr': 4.638407065638322e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 2.1138648986816406, 'lr': 3.929075993564052e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.9756931066513062, 'lr': 3.2778598899291465e-06, 'memory_usage_mb': 1484.78125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 2.0521414279937744, 'lr': 2.685017843964177e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 1.9906022548675537, 'lr': 2.150785720520559e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.9433493614196777, 'lr': 1.6753760662307215e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.9720170497894287, 'lr': 1.2589780249454618e-06, 'memory_usage_mb': 1484.78125}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.7580483555793762, 'accuracy': 0.846, 'f1_score': 0.8449829673951984, 'epoch_time_seconds': 8.75754427909851, 'memory_usage_mb': 1484.78125}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 2.059286594390869, 'lr': 9.017572624822112e-07, 'memory_usage_mb': 1484.78125}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.9424479007720947, 'lr': 6.038559007141397e-07, 'memory_usage_mb': 1484.78125}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 2.0506179332733154, 'lr': 3.653924610263703e-07, 'memory_usage_mb': 1484.78125}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 2.019019365310669, 'lr': 1.8646181716164831e-07, 'memory_usage_mb': 1484.78125}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.9709041118621826, 'lr': 6.71351574745016e-08, 'memory_usage_mb': 1484.78125}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.9547724723815918, 'lr': 7.45995660854648e-09, 'memory_usage_mb': 1484.78125}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.7579495087265968, 'accuracy': 0.846, 'f1_score': 0.8449829673951984, 'epoch_time_seconds': 8.677802562713623, 'memory_usage_mb': 1484.78125}\n",
      "Step 1575: {'total_training_time_seconds': 219.67592692375183, 'avg_epoch_time_seconds': 8.784632167816163, 'best_accuracy': 0.858, 'convergence_step': 'Not converged', 'memory_usage_mb': 1484.78125}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 8.78463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.6778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.75795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.84498\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1484.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 219.67593\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.95477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_cosine_lr0.0003_wd0.01_ag_news_20250421_163406\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/rskyv0rd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_163406-rskyv0rd/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule cosine \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5ffce",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd4d9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_164046-kruq8j9j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_ag_news_20250421_164046\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/kruq8j9j\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0003, 'memory_usage_mb': 1343.80078125}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 32.374046325683594, 'lr': 0.0003, 'memory_usage_mb': 1408.89453125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 27.264545440673828, 'lr': 0.0003, 'memory_usage_mb': 1409.20703125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 23.402685165405273, 'lr': 0.0003, 'memory_usage_mb': 1409.20703125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 19.61326789855957, 'lr': 0.0003, 'memory_usage_mb': 1409.20703125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 16.914587020874023, 'lr': 0.0003, 'memory_usage_mb': 1409.20703125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 11.505192756652832, 'lr': 0.0003, 'memory_usage_mb': 1409.20703125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 8.270396739244461, 'accuracy': 0.274, 'f1_score': 0.15758627634493952, 'epoch_time_seconds': 11.664311408996582, 'memory_usage_mb': 1511.85546875}\n",
      "New best accuracy: 0.2740\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 9.201558113098145, 'lr': 0.0003, 'memory_usage_mb': 1513.73046875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 7.916651248931885, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 7.297325134277344, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 6.299482345581055, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 5.2289605140686035, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 4.7654218673706055, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 3.9304012283682823, 'accuracy': 0.678, 'f1_score': 0.663251515410228, 'epoch_time_seconds': 8.659484148025513, 'memory_usage_mb': 1514.04296875}\n",
      "New best accuracy: 0.6780\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 4.394257545471191, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.143557548522949, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.030944347381592, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 3.919788360595703, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 3.841381311416626, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 3.836195468902588, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.5917198583483696, 'accuracy': 0.752, 'f1_score': 0.7427694440095739, 'epoch_time_seconds': 8.63503384590149, 'memory_usage_mb': 1514.04296875}\n",
      "New best accuracy: 0.7520\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 3.694894552230835, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 3.7083663940429688, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.5852370262145996, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.606839895248413, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.4811999797821045, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.4577903747558594, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.419304609298706, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.310423344373703, 'accuracy': 0.788, 'f1_score': 0.7866016615431211, 'epoch_time_seconds': 8.668664693832397, 'memory_usage_mb': 1514.04296875}\n",
      "New best accuracy: 0.7880\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.440838575363159, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.39803409576416, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.266427755355835, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.164292335510254, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.1569056510925293, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.065777540206909, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 2.930202327668667, 'accuracy': 0.728, 'f1_score': 0.7216832850753775, 'epoch_time_seconds': 8.660078048706055, 'memory_usage_mb': 1514.04296875}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.1035866737365723, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.0767011642456055, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.0078330039978027, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 2.897343158721924, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 2.883071184158325, 'lr': 0.0003, 'memory_usage_mb': 1514.04296875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 2.8677000999450684, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 2.6256391927599907, 'accuracy': 0.778, 'f1_score': 0.7674454374404653, 'epoch_time_seconds': 8.670989513397217, 'memory_usage_mb': 1514.35546875}\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 2.8615963459014893, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 2.892716884613037, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.719242811203003, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.6684505939483643, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.6563284397125244, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.712092876434326, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 2.6852588653564453, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 2.4226046055555344, 'accuracy': 0.71, 'f1_score': 0.7067627141422346, 'epoch_time_seconds': 8.67958116531372, 'memory_usage_mb': 1514.35546875}\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 2.646329402923584, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.5878379344940186, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.5558693408966064, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.4567158222198486, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.427929162979126, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.436307668685913, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.2285792902112007, 'accuracy': 0.78, 'f1_score': 0.7784314379830989, 'epoch_time_seconds': 8.666211605072021, 'memory_usage_mb': 1514.35546875}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.433681011199951, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.425759792327881, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.362025737762451, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.400813579559326, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.3336641788482666, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.3398776054382324, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.0778718739748, 'accuracy': 0.804, 'f1_score': 0.8020893731942528, 'epoch_time_seconds': 8.667683124542236, 'memory_usage_mb': 1514.35546875}\n",
      "New best accuracy: 0.8040\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.305908679962158, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.3418941497802734, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.2833757400512695, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.307349920272827, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.216146230697632, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.374312162399292, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 1.9592251032590866, 'accuracy': 0.812, 'f1_score': 0.8085937972107015, 'epoch_time_seconds': 8.636681079864502, 'memory_usage_mb': 1514.35546875}\n",
      "New best accuracy: 0.8120\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.318708658218384, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.1746249198913574, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.259342908859253, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.2139058113098145, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.187296152114868, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.1578898429870605, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.122509241104126, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 1.8952007293701172, 'accuracy': 0.854, 'f1_score': 0.8533394235300539, 'epoch_time_seconds': 8.663682222366333, 'memory_usage_mb': 1514.35546875}\n",
      "New best accuracy: 0.8540\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.119215726852417, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.0803983211517334, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.1352896690368652, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.085705518722534, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.165972948074341, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.213473081588745, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 1.816403467208147, 'accuracy': 0.834, 'f1_score': 0.830963820241814, 'epoch_time_seconds': 8.655115127563477, 'memory_usage_mb': 1514.35546875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.0403854846954346, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.0431861877441406, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.1018056869506836, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.032442092895508, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.0341598987579346, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.047206163406372, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 1.759761843830347, 'accuracy': 0.848, 'f1_score': 0.8468012443083687, 'epoch_time_seconds': 8.6302649974823, 'memory_usage_mb': 1514.35546875}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.006685972213745, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.0214080810546875, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 1.9972702264785767, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 1.97792649269104, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.079158067703247, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 1.9013186693191528, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 1.8987834453582764, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.7212288416922092, 'accuracy': 0.808, 'f1_score': 0.8029637523272893, 'epoch_time_seconds': 8.654903888702393, 'memory_usage_mb': 1514.35546875}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 1.94415283203125, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.032931327819824, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 1.915563702583313, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 1.9349277019500732, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 1.918576717376709, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 1.9720942974090576, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 1.6935112290084362, 'accuracy': 0.812, 'f1_score': 0.8087956548368327, 'epoch_time_seconds': 8.660093307495117, 'memory_usage_mb': 1514.35546875}\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 1.8822780847549438, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 1.8242398500442505, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 1.9107714891433716, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.0298333168029785, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 1.8618693351745605, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 1.8598121404647827, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.6601590178906918, 'accuracy': 0.822, 'f1_score': 0.817978304149819, 'epoch_time_seconds': 8.769594430923462, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 1.8965567350387573, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 1.882120132446289, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 1.8162904977798462, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 1.8740406036376953, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 1.8345694541931152, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 1.875516414642334, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 1.7890167236328125, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.6197289265692234, 'accuracy': 0.854, 'f1_score': 0.8527574647508359, 'epoch_time_seconds': 8.614487409591675, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 1.8408212661743164, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 1.9479568004608154, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 1.9846433401107788, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 1.747328758239746, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 1.8557417392730713, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 1.8469030857086182, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.5920432023704052, 'accuracy': 0.858, 'f1_score': 0.8579686460494252, 'epoch_time_seconds': 8.624521732330322, 'memory_usage_mb': 1514.35546875}\n",
      "New best accuracy: 0.8580\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 1.8325746059417725, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 1.7336833477020264, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 1.9203300476074219, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 1.822117805480957, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 1.8079389333724976, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 1.8042495250701904, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.5842003636062145, 'accuracy': 0.83, 'f1_score': 0.8271326820057993, 'epoch_time_seconds': 8.631119012832642, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 1.7445363998413086, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.7280166149139404, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 1.7862741947174072, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 1.7155141830444336, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 1.773156762123108, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 1.7842108011245728, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.5594648383557796, 'accuracy': 0.854, 'f1_score': 0.8541554503998555, 'epoch_time_seconds': 8.598790168762207, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.7231659889221191, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 1.720412015914917, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 1.7721678018569946, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.7286591529846191, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 1.8109283447265625, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 1.7549800872802734, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 1.7508587837219238, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.5534140206873417, 'accuracy': 0.828, 'f1_score': 0.8282130491094765, 'epoch_time_seconds': 8.617091417312622, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 1.7653709650039673, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.6907610893249512, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.7237341403961182, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 1.7763314247131348, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.7151901721954346, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 1.694117784500122, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.5157535672187805, 'accuracy': 0.874, 'f1_score': 0.8743414230235204, 'epoch_time_seconds': 8.63746452331543, 'memory_usage_mb': 1514.35546875}\n",
      "New best accuracy: 0.8740\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 1.7327951192855835, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 1.6943010091781616, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 1.6733622550964355, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 1.7501020431518555, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 1.6797620058059692, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 1.828584909439087, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 1.5200646258890629, 'accuracy': 0.836, 'f1_score': 0.8348661360837741, 'epoch_time_seconds': 8.667280673980713, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.6608530282974243, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 1.7250806093215942, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.68110191822052, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 1.8232834339141846, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 1.7549993991851807, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.6834135055541992, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.6587939262390137, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.4960892610251904, 'accuracy': 0.858, 'f1_score': 0.8581346289541477, 'epoch_time_seconds': 8.6034095287323, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 1.7353976964950562, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.6083142757415771, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 1.6723213195800781, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 1.6368484497070312, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.6399621963500977, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.6962722539901733, 'lr': 0.0003, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.4755594283342361, 'accuracy': 0.862, 'f1_score': 0.8616925526205443, 'epoch_time_seconds': 8.617897510528564, 'memory_usage_mb': 1514.35546875}\n",
      "Step 1575: {'total_training_time_seconds': 219.31915140151978, 'avg_epoch_time_seconds': 8.770177383422851, 'best_accuracy': 0.874, 'convergence_step': 'Not converged', 'memory_usage_mb': 1514.35546875}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.862\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 8.77018\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.874\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.6179\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.47556\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.86169\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0003\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1514.35547\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 219.31915\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.69627\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_ag_news_20250421_164046\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/kruq8j9j\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_164046-kruq8j9j/logs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule plateau \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06f50d",
   "metadata": {},
   "source": [
    "### Lower LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be6ee87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_165007-tkb49r2u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0001_wd0.01_ag_news_20250421_165007\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/tkb49r2u\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0, 'memory_usage_mb': 1312.640625}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 34.38414001464844, 'lr': 6.369426751592357e-06, 'memory_usage_mb': 1377.75390625}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 33.07734680175781, 'lr': 1.2738853503184714e-05, 'memory_usage_mb': 1378.06640625}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 32.622806549072266, 'lr': 1.910828025477707e-05, 'memory_usage_mb': 1378.06640625}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 31.677284240722656, 'lr': 2.5477707006369428e-05, 'memory_usage_mb': 1378.06640625}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 32.89472198486328, 'lr': 3.184713375796178e-05, 'memory_usage_mb': 1378.06640625}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 32.181602478027344, 'lr': 3.821656050955414e-05, 'memory_usage_mb': 1378.06640625}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 32.33864349126816, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 11.747039556503296, 'memory_usage_mb': 1480.6796875}\n",
      "New best accuracy: 0.2500\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 31.420869827270508, 'lr': 4.45859872611465e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 31.620840072631836, 'lr': 5.0955414012738855e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 29.48360824584961, 'lr': 5.732484076433121e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 29.05438995361328, 'lr': 6.369426751592356e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 27.060483932495117, 'lr': 7.006369426751592e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 26.1599063873291, 'lr': 7.643312101910829e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 23.494177997112274, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 11.373803853988647, 'memory_usage_mb': 1482.8671875}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 24.025461196899414, 'lr': 8.280254777070065e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 22.46007537841797, 'lr': 8.9171974522293e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 20.757862091064453, 'lr': 9.554140127388536e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 18.680593490600586, 'lr': 9.978843441466856e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 16.717674255371094, 'lr': 9.908321579689705e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 15.481292724609375, 'lr': 9.837799717912554e-05, 'memory_usage_mb': 1482.8671875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 9.591948449611664, 'accuracy': 0.254, 'f1_score': 0.1160055169636565, 'epoch_time_seconds': 11.369856357574463, 'memory_usage_mb': 1482.8671875}\n",
      "New best accuracy: 0.2540\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 12.846671104431152, 'lr': 9.767277856135403e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 10.48254108428955, 'lr': 9.696755994358252e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 9.633712768554688, 'lr': 9.6262341325811e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 9.059290885925293, 'lr': 9.55571227080395e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 8.535893440246582, 'lr': 9.485190409026799e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 7.829732418060303, 'lr': 9.414668547249648e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 7.629542350769043, 'lr': 9.344146685472497e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 6.387418881058693, 'accuracy': 0.394, 'f1_score': 0.3389019316017732, 'epoch_time_seconds': 11.207242250442505, 'memory_usage_mb': 1483.1796875}\n",
      "New best accuracy: 0.3940\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 7.360933303833008, 'lr': 9.273624823695345e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 7.089491367340088, 'lr': 9.203102961918196e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 6.567254543304443, 'lr': 9.132581100141045e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 6.32560920715332, 'lr': 9.062059238363894e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 6.04249382019043, 'lr': 8.991537376586743e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 5.714991092681885, 'lr': 8.92101551480959e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 4.713338866829872, 'accuracy': 0.566, 'f1_score': 0.5294977879552956, 'epoch_time_seconds': 9.810951471328735, 'memory_usage_mb': 1483.1796875}\n",
      "New best accuracy: 0.5660\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 5.38925313949585, 'lr': 8.85049365303244e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 5.1603593826293945, 'lr': 8.77997179125529e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 4.941141605377197, 'lr': 8.709449929478139e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 4.7398457527160645, 'lr': 8.638928067700988e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 4.635510444641113, 'lr': 8.568406205923837e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 4.480752468109131, 'lr': 8.497884344146685e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.8530458733439445, 'accuracy': 0.678, 'f1_score': 0.6709397694715725, 'epoch_time_seconds': 8.687642335891724, 'memory_usage_mb': 1483.1796875}\n",
      "New best accuracy: 0.6780\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 4.272380828857422, 'lr': 8.427362482369536e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 4.333490371704102, 'lr': 8.356840620592385e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 4.452032089233398, 'lr': 8.286318758815234e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 4.195225715637207, 'lr': 8.215796897038083e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 4.110706329345703, 'lr': 8.14527503526093e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.960245370864868, 'lr': 8.07475317348378e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 4.026957035064697, 'lr': 8.00423131170663e-05, 'memory_usage_mb': 1483.1796875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 441: {'epoch': 6, 'eval_loss': 3.7045627161860466, 'accuracy': 0.698, 'f1_score': 0.6998597285067871, 'epoch_time_seconds': 8.7111337184906, 'memory_usage_mb': 1483.1796875}\n",
      "New best accuracy: 0.6980\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.8935413360595703, 'lr': 7.933709449929479e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.8920376300811768, 'lr': 7.863187588152328e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.958024024963379, 'lr': 7.792665726375176e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.7610418796539307, 'lr': 7.722143864598025e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.7504942417144775, 'lr': 7.651622002820876e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.8033459186553955, 'lr': 7.581100141043725e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 3.625752992928028, 'accuracy': 0.746, 'f1_score': 0.7453173402855798, 'epoch_time_seconds': 8.692320823669434, 'memory_usage_mb': 1483.1796875}\n",
      "New best accuracy: 0.7460\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.795900583267212, 'lr': 7.510578279266574e-05, 'memory_usage_mb': 1483.1796875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.7838916778564453, 'lr': 7.440056417489421e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.723266363143921, 'lr': 7.36953455571227e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 3.7243664264678955, 'lr': 7.29901269393512e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 3.6881635189056396, 'lr': 7.22849083215797e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 3.685404062271118, 'lr': 7.157968970380819e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 3.5467293933033943, 'accuracy': 0.754, 'f1_score': 0.7541215636401501, 'epoch_time_seconds': 8.703543901443481, 'memory_usage_mb': 1483.4921875}\n",
      "New best accuracy: 0.7540\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 3.6163249015808105, 'lr': 7.087447108603667e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 3.6323156356811523, 'lr': 7.016925246826516e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 3.627779483795166, 'lr': 6.946403385049365e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 3.7026822566986084, 'lr': 6.875881523272216e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 3.5505220890045166, 'lr': 6.805359661495065e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 3.6634879112243652, 'lr': 6.734837799717912e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 3.4693786799907684, 'accuracy': 0.74, 'f1_score': 0.7339653543661302, 'epoch_time_seconds': 8.681480169296265, 'memory_usage_mb': 1483.4921875}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 3.6856701374053955, 'lr': 6.664315937940761e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 3.514922857284546, 'lr': 6.59379407616361e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 3.5793681144714355, 'lr': 6.52327221438646e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 3.5641372203826904, 'lr': 6.45275035260931e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 3.5549442768096924, 'lr': 6.382228490832158e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 3.4877917766571045, 'lr': 6.311706629055007e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 3.554387092590332, 'lr': 6.241184767277856e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 3.3999258056282997, 'accuracy': 0.772, 'f1_score': 0.7706056847422591, 'epoch_time_seconds': 8.884050130844116, 'memory_usage_mb': 1483.4921875}\n",
      "New best accuracy: 0.7720\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 3.4531171321868896, 'lr': 6.170662905500705e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 3.4846863746643066, 'lr': 6.100141043723555e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 3.4919817447662354, 'lr': 6.029619181946403e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 3.4417195320129395, 'lr': 5.959097320169252e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 3.484165906906128, 'lr': 5.888575458392101e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 3.5356991291046143, 'lr': 5.818053596614951e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 3.3234103620052338, 'accuracy': 0.768, 'f1_score': 0.7622304687915021, 'epoch_time_seconds': 8.675902366638184, 'memory_usage_mb': 1483.4921875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 3.366062879562378, 'lr': 5.7475317348378e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 3.41074275970459, 'lr': 5.677009873060649e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 3.4580440521240234, 'lr': 5.6064880112834985e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 3.3986072540283203, 'lr': 5.5359661495063474e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 3.392259359359741, 'lr': 5.4654442877291964e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 3.3931264877319336, 'lr': 5.394922425952046e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 3.2620272636413574, 'accuracy': 0.8, 'f1_score': 0.7983735992151129, 'epoch_time_seconds': 8.697304010391235, 'memory_usage_mb': 1483.4921875}\n",
      "New best accuracy: 0.8000\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 3.3741865158081055, 'lr': 5.3244005641748936e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 3.4025306701660156, 'lr': 5.253878702397743e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 3.2982239723205566, 'lr': 5.183356840620592e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 3.3155081272125244, 'lr': 5.112834978843441e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 3.4159460067749023, 'lr': 5.042313117066291e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 3.281982660293579, 'lr': 4.97179125528914e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 3.2499301433563232, 'lr': 4.901269393511989e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 3.1806297078728676, 'accuracy': 0.808, 'f1_score': 0.8038052753898617, 'epoch_time_seconds': 8.68773627281189, 'memory_usage_mb': 1483.4921875}\n",
      "New best accuracy: 0.8080\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 3.267756700515747, 'lr': 4.8307475317348384e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 3.328329563140869, 'lr': 4.760225669957687e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 3.2946507930755615, 'lr': 4.6897038081805364e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 3.28159236907959, 'lr': 4.619181946403385e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 3.268779754638672, 'lr': 4.548660084626234e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 3.2230820655822754, 'lr': 4.478138222849084e-05, 'memory_usage_mb': 1483.4921875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 945: {'epoch': 14, 'eval_loss': 3.116063341498375, 'accuracy': 0.814, 'f1_score': 0.8093348476315755, 'epoch_time_seconds': 8.682355880737305, 'memory_usage_mb': 1483.4921875}\n",
      "New best accuracy: 0.8140\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 3.2278101444244385, 'lr': 4.407616361071932e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 3.266437292098999, 'lr': 4.337094499294781e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 3.2357170581817627, 'lr': 4.266572637517631e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 3.2465403079986572, 'lr': 4.19605077574048e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 3.365604877471924, 'lr': 4.125528913963329e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 3.229320764541626, 'lr': 4.055007052186178e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 3.045913651585579, 'accuracy': 0.79, 'f1_score': 0.7861848657543873, 'epoch_time_seconds': 8.72050166130066, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 3.228506565093994, 'lr': 3.984485190409027e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 3.3527698516845703, 'lr': 3.913963328631876e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 3.2128913402557373, 'lr': 3.843441466854725e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 3.2053349018096924, 'lr': 3.772919605077574e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 3.1327052116394043, 'lr': 3.702397743300423e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 3.145228624343872, 'lr': 3.631875881523272e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 3.1271605491638184, 'lr': 3.561354019746121e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 2.9873539358377457, 'accuracy': 0.808, 'f1_score': 0.8047835212499035, 'epoch_time_seconds': 8.667508125305176, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 3.142864465713501, 'lr': 3.490832157968971e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 3.2797558307647705, 'lr': 3.42031029619182e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 3.1402182579040527, 'lr': 3.349788434414669e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 3.132966995239258, 'lr': 3.279266572637518e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 3.121248960494995, 'lr': 3.208744710860367e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 3.237048625946045, 'lr': 3.138222849083216e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 2.941450484097004, 'accuracy': 0.812, 'f1_score': 0.8084451120674533, 'epoch_time_seconds': 8.696049928665161, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 3.0767323970794678, 'lr': 3.067700987306065e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 3.0649302005767822, 'lr': 2.997179125528914e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 3.1726272106170654, 'lr': 2.9266572637517632e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 3.115973949432373, 'lr': 2.856135401974612e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 3.07804274559021, 'lr': 2.7856135401974615e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 3.0860726833343506, 'lr': 2.7150916784203108e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 2.8982366994023323, 'accuracy': 0.802, 'f1_score': 0.7975812201768385, 'epoch_time_seconds': 8.693111181259155, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 3.0917673110961914, 'lr': 2.6445698166431594e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 2.9827487468719482, 'lr': 2.5740479548660084e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 3.1403706073760986, 'lr': 2.5035260930888577e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 3.034510612487793, 'lr': 2.433004231311707e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 3.05997633934021, 'lr': 2.3624823695345556e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 3.1462149620056152, 'lr': 2.291960507757405e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 2.8652098327875137, 'accuracy': 0.796, 'f1_score': 0.7910649146451034, 'epoch_time_seconds': 8.679677248001099, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 3.006593942642212, 'lr': 2.221438645980254e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 2.936095714569092, 'lr': 2.1509167842031032e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 3.1874947547912598, 'lr': 2.080394922425952e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 2.979914426803589, 'lr': 2.009873060648801e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 3.05484938621521, 'lr': 1.9393511988716504e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 3.06465220451355, 'lr': 1.8688293370944994e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 3.0347237586975098, 'lr': 1.7983074753173483e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 2.8368158638477325, 'accuracy': 0.808, 'f1_score': 0.803766433252667, 'epoch_time_seconds': 8.708460569381714, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 3.0387864112854004, 'lr': 1.7277856135401976e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 2.9904754161834717, 'lr': 1.6572637517630466e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 3.0765740871429443, 'lr': 1.5867418899858956e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 3.038843870162964, 'lr': 1.5162200282087447e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 2.966555595397949, 'lr': 1.4456981664315938e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 3.0378875732421875, 'lr': 1.375176304654443e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 2.8084968477487564, 'accuracy': 0.816, 'f1_score': 0.8121075267455887, 'epoch_time_seconds': 8.695346355438232, 'memory_usage_mb': 1483.4921875}\n",
      "New best accuracy: 0.8160\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 2.9696273803710938, 'lr': 1.304654442877292e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 3.007042407989502, 'lr': 1.234132581100141e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 3.007265329360962, 'lr': 1.1636107193229902e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 3.0181667804718018, 'lr': 1.0930888575458392e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 2.9811551570892334, 'lr': 1.0225669957686883e-05, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 3.0346968173980713, 'lr': 9.520451339915374e-06, 'memory_usage_mb': 1483.4921875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1449: {'epoch': 22, 'eval_loss': 2.793957270681858, 'accuracy': 0.816, 'f1_score': 0.8121222625496421, 'epoch_time_seconds': 8.708997011184692, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 3.0129406452178955, 'lr': 8.815232722143866e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 2.9749960899353027, 'lr': 8.110014104372355e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 2.953660488128662, 'lr': 7.404795486600846e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 3.08308482170105, 'lr': 6.699576868829338e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 2.9356143474578857, 'lr': 5.994358251057828e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 2.927826166152954, 'lr': 5.289139633286319e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 3.0028743743896484, 'lr': 4.58392101551481e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 2.7869586050510406, 'accuracy': 0.816, 'f1_score': 0.8121075267455887, 'epoch_time_seconds': 8.668251752853394, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 2.9849131107330322, 'lr': 3.878702397743301e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 3.073946952819824, 'lr': 3.1734837799717915e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 3.0357351303100586, 'lr': 2.468265162200282e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 3.0120763778686523, 'lr': 1.7630465444287731e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 2.9798405170440674, 'lr': 1.0578279266572636e-06, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 2.994253635406494, 'lr': 3.526093088857546e-07, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 2.7860543578863144, 'accuracy': 0.816, 'f1_score': 0.8121075267455887, 'epoch_time_seconds': 8.700033187866211, 'memory_usage_mb': 1483.4921875}\n",
      "Step 1575: {'total_training_time_seconds': 229.60231804847717, 'avg_epoch_time_seconds': 9.182012004852295, 'best_accuracy': 0.816, 'convergence_step': 'Not converged', 'memory_usage_mb': 1483.4921875}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñá‚ñá‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 9.18201\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.70003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.78605\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.81211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1483.49219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 229.60232\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 2.99425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.0001_wd0.01_ag_news_20250421_165007\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/tkb49r2u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_165007-tkb49r2u/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8906f8",
   "metadata": {},
   "source": [
    "### Higher LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "312ae686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_165439-w0ip99gh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.001_wd0.01_ag_news_20250421_165438\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/w0ip99gh\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0, 'memory_usage_mb': 1312.4765625}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 34.27025604248047, 'lr': 6.369426751592357e-05, 'memory_usage_mb': 1377.609375}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 32.386898040771484, 'lr': 0.00012738853503184715, 'memory_usage_mb': 1377.921875}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 30.812036514282227, 'lr': 0.00019108280254777072, 'memory_usage_mb': 1377.921875}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 26.46196174621582, 'lr': 0.0002547770700636943, 'memory_usage_mb': 1377.921875}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 23.121826171875, 'lr': 0.0003184713375796178, 'memory_usage_mb': 1377.921875}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 17.883779525756836, 'lr': 0.00038216560509554145, 'memory_usage_mb': 1377.921875}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 14.9937444627285, 'accuracy': 0.244, 'f1_score': 0.09571704180064307, 'epoch_time_seconds': 11.731300115585327, 'memory_usage_mb': 1480.515625}\n",
      "New best accuracy: 0.2440\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 12.000202178955078, 'lr': 0.00044585987261146497, 'memory_usage_mb': 1482.703125}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 8.228310585021973, 'lr': 0.0005095541401273886, 'memory_usage_mb': 1482.703125}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 6.724298000335693, 'lr': 0.0005732484076433121, 'memory_usage_mb': 1482.703125}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 5.072380065917969, 'lr': 0.0006369426751592356, 'memory_usage_mb': 1482.703125}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 4.3553643226623535, 'lr': 0.0007006369426751592, 'memory_usage_mb': 1482.703125}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 3.8840794563293457, 'lr': 0.0007643312101910829, 'memory_usage_mb': 1482.703125}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 3.67176965624094, 'accuracy': 0.65, 'f1_score': 0.6330044870082273, 'epoch_time_seconds': 8.713456392288208, 'memory_usage_mb': 1482.703125}\n",
      "New best accuracy: 0.6500\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 3.7381978034973145, 'lr': 0.0008280254777070065, 'memory_usage_mb': 1482.703125}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 3.659773826599121, 'lr': 0.0008917197452229299, 'memory_usage_mb': 1482.703125}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 3.552372694015503, 'lr': 0.0009554140127388535, 'memory_usage_mb': 1482.703125}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 3.3569023609161377, 'lr': 0.0009978843441466854, 'memory_usage_mb': 1482.703125}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 3.2434327602386475, 'lr': 0.0009908321579689705, 'memory_usage_mb': 1482.703125}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 3.2065155506134033, 'lr': 0.0009837799717912553, 'memory_usage_mb': 1482.703125}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 2.817175582051277, 'accuracy': 0.704, 'f1_score': 0.6986960785186247, 'epoch_time_seconds': 8.708857297897339, 'memory_usage_mb': 1482.703125}\n",
      "New best accuracy: 0.7040\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 2.953993797302246, 'lr': 0.0009767277856135403, 'memory_usage_mb': 1482.703125}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 2.9957640171051025, 'lr': 0.0009696755994358252, 'memory_usage_mb': 1482.703125}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 2.8823063373565674, 'lr': 0.0009626234132581099, 'memory_usage_mb': 1482.703125}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 2.6845834255218506, 'lr': 0.0009555712270803949, 'memory_usage_mb': 1482.703125}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 2.5442538261413574, 'lr': 0.0009485190409026798, 'memory_usage_mb': 1482.703125}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 2.5152883529663086, 'lr': 0.0009414668547249647, 'memory_usage_mb': 1482.703125}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 2.432219982147217, 'lr': 0.0009344146685472496, 'memory_usage_mb': 1482.703125}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 2.2040707394480705, 'accuracy': 0.844, 'f1_score': 0.8440610549433045, 'epoch_time_seconds': 8.687721490859985, 'memory_usage_mb': 1482.703125}\n",
      "New best accuracy: 0.8440\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 2.5425498485565186, 'lr': 0.0009273624823695345, 'memory_usage_mb': 1482.703125}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 2.3600096702575684, 'lr': 0.0009203102961918195, 'memory_usage_mb': 1482.703125}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 2.290905475616455, 'lr': 0.0009132581100141044, 'memory_usage_mb': 1482.703125}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 2.2347238063812256, 'lr': 0.0009062059238363893, 'memory_usage_mb': 1482.703125}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 2.230060577392578, 'lr': 0.0008991537376586742, 'memory_usage_mb': 1482.703125}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 2.160219192504883, 'lr': 0.0008921015514809591, 'memory_usage_mb': 1482.703125}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 1.935663916170597, 'accuracy': 0.8, 'f1_score': 0.7992064761425027, 'epoch_time_seconds': 8.73416805267334, 'memory_usage_mb': 1482.703125}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 2.1813197135925293, 'lr': 0.000885049365303244, 'memory_usage_mb': 1483.015625}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 2.203897714614868, 'lr': 0.0008779971791255289, 'memory_usage_mb': 1483.015625}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 2.133362293243408, 'lr': 0.0008709449929478138, 'memory_usage_mb': 1483.015625}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 2.0358285903930664, 'lr': 0.0008638928067700988, 'memory_usage_mb': 1483.015625}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 2.0894665718078613, 'lr': 0.0008568406205923837, 'memory_usage_mb': 1483.015625}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 2.0021119117736816, 'lr': 0.0008497884344146686, 'memory_usage_mb': 1483.015625}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 1.795410018414259, 'accuracy': 0.848, 'f1_score': 0.8474725702974549, 'epoch_time_seconds': 8.700186729431152, 'memory_usage_mb': 1483.015625}\n",
      "New best accuracy: 0.8480\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 1.9837244749069214, 'lr': 0.0008427362482369535, 'memory_usage_mb': 1483.015625}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 2.026486396789551, 'lr': 0.0008356840620592384, 'memory_usage_mb': 1483.015625}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.0098392963409424, 'lr': 0.0008286318758815234, 'memory_usage_mb': 1483.015625}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 1.9255092144012451, 'lr': 0.0008215796897038083, 'memory_usage_mb': 1483.015625}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 1.9955801963806152, 'lr': 0.0008145275035260931, 'memory_usage_mb': 1483.015625}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 1.9112299680709839, 'lr': 0.0008074753173483779, 'memory_usage_mb': 1483.015625}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 1.9798448085784912, 'lr': 0.0008004231311706628, 'memory_usage_mb': 1483.015625}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 1.7840954288840294, 'accuracy': 0.812, 'f1_score': 0.8088683517055053, 'epoch_time_seconds': 8.712806701660156, 'memory_usage_mb': 1483.015625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 1.9112679958343506, 'lr': 0.0007933709449929478, 'memory_usage_mb': 1483.015625}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 1.9572699069976807, 'lr': 0.0007863187588152327, 'memory_usage_mb': 1483.015625}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 1.9731470346450806, 'lr': 0.0007792665726375176, 'memory_usage_mb': 1483.015625}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 1.849137783050537, 'lr': 0.0007722143864598025, 'memory_usage_mb': 1483.015625}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 1.835005283355713, 'lr': 0.0007651622002820875, 'memory_usage_mb': 1483.015625}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 1.9234894514083862, 'lr': 0.0007581100141043724, 'memory_usage_mb': 1483.015625}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 1.6642524041235447, 'accuracy': 0.814, 'f1_score': 0.811853008319246, 'epoch_time_seconds': 8.684736967086792, 'memory_usage_mb': 1483.015625}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 1.778519630432129, 'lr': 0.0007510578279266573, 'memory_usage_mb': 1483.015625}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 1.8439620733261108, 'lr': 0.0007440056417489421, 'memory_usage_mb': 1483.015625}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 1.807173252105713, 'lr': 0.000736953455571227, 'memory_usage_mb': 1483.015625}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.4240124225616455, 'lr': 0.000729901269393512, 'memory_usage_mb': 1483.015625}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 1.858847975730896, 'lr': 0.0007228490832157969, 'memory_usage_mb': 1483.015625}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 1.8949623107910156, 'lr': 0.0007157968970380818, 'memory_usage_mb': 1483.015625}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 1.6214867047965527, 'accuracy': 0.838, 'f1_score': 0.8370693921493779, 'epoch_time_seconds': 8.673340320587158, 'memory_usage_mb': 1483.015625}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 1.8535828590393066, 'lr': 0.0007087447108603667, 'memory_usage_mb': 1483.015625}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 1.9002143144607544, 'lr': 0.0007016925246826517, 'memory_usage_mb': 1483.015625}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 1.8572368621826172, 'lr': 0.0006946403385049366, 'memory_usage_mb': 1483.015625}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 1.8547028303146362, 'lr': 0.0006875881523272215, 'memory_usage_mb': 1483.015625}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 1.751638650894165, 'lr': 0.0006805359661495064, 'memory_usage_mb': 1483.015625}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 1.9562796354293823, 'lr': 0.0006734837799717913, 'memory_usage_mb': 1483.015625}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 1.5855268351733685, 'accuracy': 0.856, 'f1_score': 0.8562520056087747, 'epoch_time_seconds': 8.736021518707275, 'memory_usage_mb': 1483.015625}\n",
      "New best accuracy: 0.8560\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 1.891310214996338, 'lr': 0.0006664315937940762, 'memory_usage_mb': 1483.015625}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 1.7485337257385254, 'lr': 0.0006593794076163611, 'memory_usage_mb': 1483.015625}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 1.7991060018539429, 'lr': 0.000652327221438646, 'memory_usage_mb': 1483.015625}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 1.777698040008545, 'lr': 0.0006452750352609308, 'memory_usage_mb': 1483.015625}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 1.8481782674789429, 'lr': 0.0006382228490832158, 'memory_usage_mb': 1483.015625}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 1.7842891216278076, 'lr': 0.0006311706629055007, 'memory_usage_mb': 1483.015625}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 1.7476744651794434, 'lr': 0.0006241184767277856, 'memory_usage_mb': 1483.015625}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 1.5618332661688328, 'accuracy': 0.864, 'f1_score': 0.862960421221061, 'epoch_time_seconds': 8.73530125617981, 'memory_usage_mb': 1483.015625}\n",
      "New best accuracy: 0.8640\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 1.801987648010254, 'lr': 0.0006170662905500705, 'memory_usage_mb': 1483.015625}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 1.7646594047546387, 'lr': 0.0006100141043723555, 'memory_usage_mb': 1483.015625}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 1.8660800457000732, 'lr': 0.0006029619181946403, 'memory_usage_mb': 1483.015625}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 1.7258586883544922, 'lr': 0.0005959097320169252, 'memory_usage_mb': 1483.015625}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 1.771493673324585, 'lr': 0.0005888575458392101, 'memory_usage_mb': 1483.015625}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 1.726798176765442, 'lr': 0.000581805359661495, 'memory_usage_mb': 1483.328125}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 1.555298138409853, 'accuracy': 0.85, 'f1_score': 0.8474413739093622, 'epoch_time_seconds': 8.692229986190796, 'memory_usage_mb': 1483.328125}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 1.683333396911621, 'lr': 0.00057475317348378, 'memory_usage_mb': 1483.328125}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 1.7475038766860962, 'lr': 0.0005677009873060649, 'memory_usage_mb': 1483.328125}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 1.6864811182022095, 'lr': 0.0005606488011283498, 'memory_usage_mb': 1483.328125}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 1.6582399606704712, 'lr': 0.0005535966149506347, 'memory_usage_mb': 1483.328125}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 1.6899594068527222, 'lr': 0.0005465444287729197, 'memory_usage_mb': 1483.328125}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 1.6964383125305176, 'lr': 0.0005394922425952046, 'memory_usage_mb': 1483.328125}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 1.515994731336832, 'accuracy': 0.866, 'f1_score': 0.8641046997113052, 'epoch_time_seconds': 8.69539737701416, 'memory_usage_mb': 1483.328125}\n",
      "New best accuracy: 0.8660\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 1.7319386005401611, 'lr': 0.0005324400564174894, 'memory_usage_mb': 1483.328125}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 1.810223937034607, 'lr': 0.0005253878702397743, 'memory_usage_mb': 1483.328125}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 1.8240365982055664, 'lr': 0.0005183356840620593, 'memory_usage_mb': 1483.328125}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 1.6743491888046265, 'lr': 0.0005112834978843442, 'memory_usage_mb': 1483.328125}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 1.8003530502319336, 'lr': 0.0005042313117066291, 'memory_usage_mb': 1483.328125}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 1.628877878189087, 'lr': 0.000497179125528914, 'memory_usage_mb': 1483.328125}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 1.6371071338653564, 'lr': 0.000490126939351199, 'memory_usage_mb': 1483.328125}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.5085130408406258, 'accuracy': 0.866, 'f1_score': 0.8648089389652144, 'epoch_time_seconds': 8.713014602661133, 'memory_usage_mb': 1483.328125}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 1.6545618772506714, 'lr': 0.0004830747531734838, 'memory_usage_mb': 1483.328125}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 1.7534258365631104, 'lr': 0.0004760225669957687, 'memory_usage_mb': 1483.328125}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 1.7065743207931519, 'lr': 0.0004689703808180536, 'memory_usage_mb': 1483.328125}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 1.668556571006775, 'lr': 0.00046191819464033853, 'memory_usage_mb': 1483.328125}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 1.6675854921340942, 'lr': 0.00045486600846262346, 'memory_usage_mb': 1483.328125}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 1.6917524337768555, 'lr': 0.0004478138222849084, 'memory_usage_mb': 1483.328125}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 1.5007595717906952, 'accuracy': 0.86, 'f1_score': 0.8595024890718074, 'epoch_time_seconds': 8.706892967224121, 'memory_usage_mb': 1483.328125}\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 1.6506816148757935, 'lr': 0.0004407616361071932, 'memory_usage_mb': 1483.328125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 1.6096230745315552, 'lr': 0.0004337094499294781, 'memory_usage_mb': 1483.328125}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 1.622977375984192, 'lr': 0.00042665726375176304, 'memory_usage_mb': 1483.328125}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 1.6964750289916992, 'lr': 0.00041960507757404797, 'memory_usage_mb': 1483.328125}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 1.6063456535339355, 'lr': 0.0004125528913963329, 'memory_usage_mb': 1483.328125}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 1.6507115364074707, 'lr': 0.00040550070521861776, 'memory_usage_mb': 1483.328125}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.4922991506755352, 'accuracy': 0.858, 'f1_score': 0.8571260531731218, 'epoch_time_seconds': 8.868985652923584, 'memory_usage_mb': 1483.328125}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 1.654908537864685, 'lr': 0.0003984485190409027, 'memory_usage_mb': 1483.328125}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 1.637316346168518, 'lr': 0.0003913963328631876, 'memory_usage_mb': 1483.328125}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 1.6014461517333984, 'lr': 0.00038434414668547253, 'memory_usage_mb': 1483.328125}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 1.6792595386505127, 'lr': 0.00037729196050775745, 'memory_usage_mb': 1483.328125}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 1.6325368881225586, 'lr': 0.0003702397743300423, 'memory_usage_mb': 1483.328125}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 1.6543192863464355, 'lr': 0.0003631875881523272, 'memory_usage_mb': 1483.328125}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 1.647911548614502, 'lr': 0.0003561354019746121, 'memory_usage_mb': 1483.328125}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.4831089563667774, 'accuracy': 0.862, 'f1_score': 0.8631650097109783, 'epoch_time_seconds': 8.677727937698364, 'memory_usage_mb': 1483.328125}\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 1.6162359714508057, 'lr': 0.00034908321579689704, 'memory_usage_mb': 1483.328125}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 1.7806501388549805, 'lr': 0.00034203102961918196, 'memory_usage_mb': 1483.328125}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 1.7406564950942993, 'lr': 0.00033497884344146683, 'memory_usage_mb': 1483.328125}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 1.5801292657852173, 'lr': 0.00032792665726375176, 'memory_usage_mb': 1483.328125}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 1.6539616584777832, 'lr': 0.0003208744710860367, 'memory_usage_mb': 1483.328125}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 1.6276676654815674, 'lr': 0.0003138222849083216, 'memory_usage_mb': 1483.328125}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.4679550901055336, 'accuracy': 0.864, 'f1_score': 0.8639271136738073, 'epoch_time_seconds': 8.5829336643219, 'memory_usage_mb': 1483.328125}\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 1.613502025604248, 'lr': 0.00030677009873060653, 'memory_usage_mb': 1483.328125}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 1.5569123029708862, 'lr': 0.0002997179125528914, 'memory_usage_mb': 1483.328125}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 1.7354867458343506, 'lr': 0.0002926657263751763, 'memory_usage_mb': 1483.328125}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 1.6441606283187866, 'lr': 0.00028561354019746124, 'memory_usage_mb': 1483.328125}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 1.6155940294265747, 'lr': 0.0002785613540197461, 'memory_usage_mb': 1483.328125}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 1.6843509674072266, 'lr': 0.00027150916784203104, 'memory_usage_mb': 1483.328125}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.4665707051753998, 'accuracy': 0.86, 'f1_score': 0.8589639388080835, 'epoch_time_seconds': 8.665122270584106, 'memory_usage_mb': 1483.328125}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 1.6036473512649536, 'lr': 0.0002644569816643159, 'memory_usage_mb': 1483.328125}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.5581834316253662, 'lr': 0.00025740479548660083, 'memory_usage_mb': 1483.328125}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 1.6651618480682373, 'lr': 0.00025035260930888575, 'memory_usage_mb': 1483.328125}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 1.5678497552871704, 'lr': 0.00024330042313117068, 'memory_usage_mb': 1483.328125}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 1.6083563566207886, 'lr': 0.00023624823695345557, 'memory_usage_mb': 1483.328125}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 1.5981799364089966, 'lr': 0.0002291960507757405, 'memory_usage_mb': 1483.328125}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.4645218774676323, 'accuracy': 0.848, 'f1_score': 0.8463600685365716, 'epoch_time_seconds': 8.611413478851318, 'memory_usage_mb': 1483.328125}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.5809303522109985, 'lr': 0.00022214386459802537, 'memory_usage_mb': 1483.328125}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 1.5833979845046997, 'lr': 0.0002150916784203103, 'memory_usage_mb': 1483.328125}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 1.5987522602081299, 'lr': 0.00020803949224259521, 'memory_usage_mb': 1483.328125}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.5957930088043213, 'lr': 0.0002009873060648801, 'memory_usage_mb': 1483.328125}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 1.6335381269454956, 'lr': 0.00019393511988716503, 'memory_usage_mb': 1483.328125}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 1.649022102355957, 'lr': 0.00018688293370944993, 'memory_usage_mb': 1483.328125}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 1.627479076385498, 'lr': 0.00017983074753173483, 'memory_usage_mb': 1483.328125}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.451768234372139, 'accuracy': 0.868, 'f1_score': 0.8675837018147775, 'epoch_time_seconds': 8.65110468864441, 'memory_usage_mb': 1483.328125}\n",
      "New best accuracy: 0.8680\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 1.5959317684173584, 'lr': 0.00017277856135401975, 'memory_usage_mb': 1483.328125}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.6038364171981812, 'lr': 0.00016572637517630465, 'memory_usage_mb': 1483.328125}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.5978295803070068, 'lr': 0.00015867418899858957, 'memory_usage_mb': 1483.328125}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 1.6497409343719482, 'lr': 0.00015162200282087447, 'memory_usage_mb': 1483.328125}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.6318985223770142, 'lr': 0.0001445698166431594, 'memory_usage_mb': 1483.328125}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 1.6015455722808838, 'lr': 0.0001375176304654443, 'memory_usage_mb': 1483.328125}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.4431912787258625, 'accuracy': 0.874, 'f1_score': 0.8731048860138115, 'epoch_time_seconds': 8.702195405960083, 'memory_usage_mb': 1483.328125}\n",
      "New best accuracy: 0.8740\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 1.6552544832229614, 'lr': 0.00013046544428772918, 'memory_usage_mb': 1483.328125}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 1.5516090393066406, 'lr': 0.0001234132581100141, 'memory_usage_mb': 1483.328125}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 1.5336428880691528, 'lr': 0.00011636107193229902, 'memory_usage_mb': 1483.328125}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 1.631082534790039, 'lr': 0.00010930888575458391, 'memory_usage_mb': 1483.328125}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 1.5447137355804443, 'lr': 0.00010225669957686882, 'memory_usage_mb': 1483.328125}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 1.65199875831604, 'lr': 9.520451339915375e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 1.4407651089131832, 'accuracy': 0.852, 'f1_score': 0.8506348246993509, 'epoch_time_seconds': 8.728153467178345, 'memory_usage_mb': 1483.328125}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.569058895111084, 'lr': 8.815232722143864e-05, 'memory_usage_mb': 1483.328125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 1.6039618253707886, 'lr': 8.110014104372355e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.5700697898864746, 'lr': 7.404795486600846e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 1.6521272659301758, 'lr': 6.699576868829337e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 1.5907361507415771, 'lr': 5.994358251057828e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.5783857107162476, 'lr': 5.289139633286319e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.54702627658844, 'lr': 4.58392101551481e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.438472781330347, 'accuracy': 0.874, 'f1_score': 0.8733072458795822, 'epoch_time_seconds': 8.625268459320068, 'memory_usage_mb': 1483.328125}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 1.600664734840393, 'lr': 3.878702397743301e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.5552756786346436, 'lr': 3.173483779971791e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 1.6009280681610107, 'lr': 2.468265162200282e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 1.538732647895813, 'lr': 1.763046544428773e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.5646295547485352, 'lr': 1.0578279266572637e-05, 'memory_usage_mb': 1483.328125}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.6169309616088867, 'lr': 3.5260930888575462e-06, 'memory_usage_mb': 1483.328125}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.439385011792183, 'accuracy': 0.868, 'f1_score': 0.8668284708859326, 'epoch_time_seconds': 8.599869728088379, 'memory_usage_mb': 1483.328125}\n",
      "Step 1575: {'total_training_time_seconds': 220.38834047317505, 'avg_epoch_time_seconds': 8.813528261184693, 'best_accuracy': 0.874, 'convergence_step': 'Not converged', 'memory_usage_mb': 1483.328125}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 8.81353\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.874\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.59987\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.43939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.86683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1483.32812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 220.38834\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.61693\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.001_wd0.01_ag_news_20250421_165438\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/w0ip99gh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_165439-w0ip99gh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a90b2a",
   "metadata": {},
   "source": [
    "### Higher Weight Decay (0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786923e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_165840-loysvnry\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.1_ag_news_20250421_165840\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/loysvnry\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0, 'memory_usage_mb': 1313.66015625}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 34.35995864868164, 'lr': 1.9108280254777068e-05, 'memory_usage_mb': 1378.7734375}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 32.954566955566406, 'lr': 3.8216560509554137e-05, 'memory_usage_mb': 1379.0859375}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 32.37015914916992, 'lr': 5.732484076433121e-05, 'memory_usage_mb': 1379.0859375}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 30.977529525756836, 'lr': 7.643312101910827e-05, 'memory_usage_mb': 1379.0859375}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 31.219736099243164, 'lr': 9.554140127388533e-05, 'memory_usage_mb': 1379.0859375}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 29.500967025756836, 'lr': 0.00011464968152866242, 'memory_usage_mb': 1379.0859375}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 27.665142595767975, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 11.601835012435913, 'memory_usage_mb': 1481.7109375}\n",
      "New best accuracy: 0.2500\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 26.2840576171875, 'lr': 0.00013375796178343948, 'memory_usage_mb': 1483.8984375}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 24.662424087524414, 'lr': 0.00015286624203821655, 'memory_usage_mb': 1483.8984375}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 20.970003128051758, 'lr': 0.0001719745222929936, 'memory_usage_mb': 1483.8984375}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 18.177486419677734, 'lr': 0.00019108280254777067, 'memory_usage_mb': 1483.8984375}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 14.979850769042969, 'lr': 0.00021019108280254773, 'memory_usage_mb': 1483.8984375}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 11.288056373596191, 'lr': 0.00022929936305732485, 'memory_usage_mb': 1483.8984375}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 7.718694806098938, 'accuracy': 0.274, 'f1_score': 0.1549491789219367, 'epoch_time_seconds': 11.223500967025757, 'memory_usage_mb': 1483.8984375}\n",
      "New best accuracy: 0.2740\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 8.96762466430664, 'lr': 0.0002484076433121019, 'memory_usage_mb': 1483.8984375}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 8.09268856048584, 'lr': 0.00026751592356687897, 'memory_usage_mb': 1483.8984375}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 6.777428150177002, 'lr': 0.00028662420382165603, 'memory_usage_mb': 1483.8984375}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 6.111300945281982, 'lr': 0.0002993653032440056, 'memory_usage_mb': 1483.8984375}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 5.100846767425537, 'lr': 0.0002972496473906911, 'memory_usage_mb': 1483.8984375}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.748329162597656, 'lr': 0.00029513399153737656, 'memory_usage_mb': 1483.8984375}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.9005387276411057, 'accuracy': 0.658, 'f1_score': 0.6530801214519417, 'epoch_time_seconds': 8.611518144607544, 'memory_usage_mb': 1483.8984375}\n",
      "New best accuracy: 0.6580\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.220909118652344, 'lr': 0.00029301833568406207, 'memory_usage_mb': 1483.8984375}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.1839423179626465, 'lr': 0.0002909026798307475, 'memory_usage_mb': 1483.8984375}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.9742491245269775, 'lr': 0.000288787023977433, 'memory_usage_mb': 1483.8984375}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.914605140686035, 'lr': 0.00028667136812411843, 'memory_usage_mb': 1483.8984375}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.7292749881744385, 'lr': 0.0002845557122708039, 'memory_usage_mb': 1483.8984375}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.68465256690979, 'lr': 0.0002824400564174894, 'memory_usage_mb': 1483.8984375}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.6730589866638184, 'lr': 0.00028032440056417486, 'memory_usage_mb': 1483.8984375}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.5418814569711685, 'accuracy': 0.778, 'f1_score': 0.7785966504473967, 'epoch_time_seconds': 8.584592819213867, 'memory_usage_mb': 1483.8984375}\n",
      "New best accuracy: 0.7780\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.6618218421936035, 'lr': 0.00027820874471086036, 'memory_usage_mb': 1484.2109375}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.625767230987549, 'lr': 0.0002760930888575458, 'memory_usage_mb': 1484.2109375}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.505819797515869, 'lr': 0.0002739774330042313, 'memory_usage_mb': 1484.2109375}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.44830060005188, 'lr': 0.0002718617771509168, 'memory_usage_mb': 1484.2109375}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.4323930740356445, 'lr': 0.00026974612129760224, 'memory_usage_mb': 1484.2109375}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.3920438289642334, 'lr': 0.0002676304654442877, 'memory_usage_mb': 1484.2109375}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.2822631672024727, 'accuracy': 0.772, 'f1_score': 0.7698270547672575, 'epoch_time_seconds': 8.630283117294312, 'memory_usage_mb': 1484.2109375}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.371123790740967, 'lr': 0.00026551480959097315, 'memory_usage_mb': 1484.2109375}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.379978895187378, 'lr': 0.00026339915373765866, 'memory_usage_mb': 1484.2109375}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.2476139068603516, 'lr': 0.0002612834978843441, 'memory_usage_mb': 1484.2109375}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.1880290508270264, 'lr': 0.00025916784203102957, 'memory_usage_mb': 1484.2109375}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.1565604209899902, 'lr': 0.0002570521861777151, 'memory_usage_mb': 1484.2109375}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.1521458625793457, 'lr': 0.00025493653032440054, 'memory_usage_mb': 1484.2109375}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 2.9283244535326958, 'accuracy': 0.798, 'f1_score': 0.7923794302958718, 'epoch_time_seconds': 8.584775686264038, 'memory_usage_mb': 1484.2109375}\n",
      "New best accuracy: 0.7980\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.0940449237823486, 'lr': 0.00025282087447108605, 'memory_usage_mb': 1484.2109375}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.0931427478790283, 'lr': 0.0002507052186177715, 'memory_usage_mb': 1484.2109375}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.9821698665618896, 'lr': 0.00024858956276445696, 'memory_usage_mb': 1484.2109375}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.9361424446105957, 'lr': 0.00024647390691114247, 'memory_usage_mb': 1484.2109375}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.9144837856292725, 'lr': 0.00024435825105782787, 'memory_usage_mb': 1484.2109375}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.9749391078948975, 'lr': 0.00024224259520451338, 'memory_usage_mb': 1484.2109375}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 2.9472804069519043, 'lr': 0.00024012693935119883, 'memory_usage_mb': 1484.2109375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 441: {'epoch': 6, 'eval_loss': 2.6933252215385437, 'accuracy': 0.752, 'f1_score': 0.7485375280373061, 'epoch_time_seconds': 8.58955454826355, 'memory_usage_mb': 1484.2109375}\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 2.942636728286743, 'lr': 0.00023801128349788432, 'memory_usage_mb': 1484.2109375}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.834367036819458, 'lr': 0.0002358956276445698, 'memory_usage_mb': 1484.2109375}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.821096897125244, 'lr': 0.00023377997179125528, 'memory_usage_mb': 1484.2109375}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.7310805320739746, 'lr': 0.00023166431593794074, 'memory_usage_mb': 1484.2109375}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.714613914489746, 'lr': 0.00022954866008462622, 'memory_usage_mb': 1484.2109375}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.706024646759033, 'lr': 0.0002274330042313117, 'memory_usage_mb': 1484.2109375}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.4549196362495422, 'accuracy': 0.81, 'f1_score': 0.808157528204652, 'epoch_time_seconds': 8.596609354019165, 'memory_usage_mb': 1484.2109375}\n",
      "New best accuracy: 0.8100\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.6849911212921143, 'lr': 0.00022531734837799718, 'memory_usage_mb': 1484.2109375}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.7020530700683594, 'lr': 0.0002232016925246826, 'memory_usage_mb': 1484.2109375}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.6133413314819336, 'lr': 0.0002210860366713681, 'memory_usage_mb': 1484.2109375}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.6383752822875977, 'lr': 0.00021897038081805358, 'memory_usage_mb': 1484.2109375}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.58522367477417, 'lr': 0.00021685472496473906, 'memory_usage_mb': 1484.2109375}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.572018623352051, 'lr': 0.00021473906911142451, 'memory_usage_mb': 1484.2109375}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.3401271253824234, 'accuracy': 0.804, 'f1_score': 0.8028777743388888, 'epoch_time_seconds': 8.609660625457764, 'memory_usage_mb': 1484.2109375}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.575458526611328, 'lr': 0.00021262341325811, 'memory_usage_mb': 1484.2109375}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.580227851867676, 'lr': 0.00021050775740479548, 'memory_usage_mb': 1484.2109375}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.559924602508545, 'lr': 0.00020839210155148096, 'memory_usage_mb': 1484.2109375}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.601062774658203, 'lr': 0.00020627644569816642, 'memory_usage_mb': 1484.2109375}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.4513344764709473, 'lr': 0.0002041607898448519, 'memory_usage_mb': 1484.2109375}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.6413044929504395, 'lr': 0.00020204513399153736, 'memory_usage_mb': 1484.2109375}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.220417208969593, 'accuracy': 0.81, 'f1_score': 0.8063032322708539, 'epoch_time_seconds': 8.608682870864868, 'memory_usage_mb': 1484.2109375}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.55722713470459, 'lr': 0.0001999294781382228, 'memory_usage_mb': 1484.2109375}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.4145779609680176, 'lr': 0.0001978138222849083, 'memory_usage_mb': 1484.2109375}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.523299217224121, 'lr': 0.00019569816643159378, 'memory_usage_mb': 1484.2109375}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.457970380783081, 'lr': 0.00019358251057827926, 'memory_usage_mb': 1484.2109375}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.4419353008270264, 'lr': 0.00019146685472496471, 'memory_usage_mb': 1484.2109375}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.355229139328003, 'lr': 0.0001893511988716502, 'memory_usage_mb': 1484.5234375}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.337758779525757, 'lr': 0.00018723554301833568, 'memory_usage_mb': 1484.5234375}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.1389603689312935, 'accuracy': 0.842, 'f1_score': 0.8414274125331265, 'epoch_time_seconds': 8.589374780654907, 'memory_usage_mb': 1484.5234375}\n",
      "New best accuracy: 0.8420\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.420968532562256, 'lr': 0.00018511988716502116, 'memory_usage_mb': 1484.5234375}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.359907865524292, 'lr': 0.00018300423131170662, 'memory_usage_mb': 1484.5234375}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.374295234680176, 'lr': 0.00018088857545839207, 'memory_usage_mb': 1484.5234375}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.3565311431884766, 'lr': 0.00017877291960507755, 'memory_usage_mb': 1484.5234375}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.413116455078125, 'lr': 0.000176657263751763, 'memory_usage_mb': 1484.5234375}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.408140182495117, 'lr': 0.0001745416078984485, 'memory_usage_mb': 1484.5234375}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.0725499615073204, 'accuracy': 0.818, 'f1_score': 0.8162245057947517, 'epoch_time_seconds': 8.621735572814941, 'memory_usage_mb': 1484.5234375}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.3690788745880127, 'lr': 0.00017242595204513398, 'memory_usage_mb': 1484.5234375}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.3237361907958984, 'lr': 0.00017031029619181946, 'memory_usage_mb': 1484.5234375}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.381178379058838, 'lr': 0.00016819464033850494, 'memory_usage_mb': 1484.5234375}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.2695648670196533, 'lr': 0.0001660789844851904, 'memory_usage_mb': 1484.5234375}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.2863831520080566, 'lr': 0.00016396332863187588, 'memory_usage_mb': 1484.5234375}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.259045362472534, 'lr': 0.00016184767277856136, 'memory_usage_mb': 1484.5234375}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.021266769617796, 'accuracy': 0.83, 'f1_score': 0.82898404367939, 'epoch_time_seconds': 8.63762903213501, 'memory_usage_mb': 1484.5234375}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.2765069007873535, 'lr': 0.0001597320169252468, 'memory_usage_mb': 1484.5234375}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.332961082458496, 'lr': 0.00015761636107193227, 'memory_usage_mb': 1484.5234375}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.2680227756500244, 'lr': 0.00015550070521861775, 'memory_usage_mb': 1484.5234375}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.2063496112823486, 'lr': 0.00015338504936530324, 'memory_usage_mb': 1484.5234375}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.314784288406372, 'lr': 0.0001512693935119887, 'memory_usage_mb': 1484.5234375}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.2082958221435547, 'lr': 0.00014915373765867417, 'memory_usage_mb': 1484.5234375}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.1781396865844727, 'lr': 0.00014703808180535966, 'memory_usage_mb': 1484.5234375}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.969593808054924, 'accuracy': 0.844, 'f1_score': 0.8434513415357768, 'epoch_time_seconds': 8.600467681884766, 'memory_usage_mb': 1484.5234375}\n",
      "New best accuracy: 0.8440\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.213709592819214, 'lr': 0.00014492242595204514, 'memory_usage_mb': 1484.5234375}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.3223719596862793, 'lr': 0.0001428067700987306, 'memory_usage_mb': 1484.5234375}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.1841800212860107, 'lr': 0.00014069111424541608, 'memory_usage_mb': 1484.5234375}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.1936638355255127, 'lr': 0.00013857545839210153, 'memory_usage_mb': 1484.5234375}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.2122249603271484, 'lr': 0.00013645980253878702, 'memory_usage_mb': 1484.5234375}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.181656837463379, 'lr': 0.0001343441466854725, 'memory_usage_mb': 1484.5234375}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 1.9328093230724335, 'accuracy': 0.834, 'f1_score': 0.8330560374458346, 'epoch_time_seconds': 8.648295402526855, 'memory_usage_mb': 1484.5234375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 2.1804747581481934, 'lr': 0.00013222849083215795, 'memory_usage_mb': 1484.5234375}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 2.093764066696167, 'lr': 0.00013011283497884344, 'memory_usage_mb': 1484.5234375}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 2.1367080211639404, 'lr': 0.0001279971791255289, 'memory_usage_mb': 1484.5234375}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.2262351512908936, 'lr': 0.00012588152327221437, 'memory_usage_mb': 1484.5234375}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 2.170027732849121, 'lr': 0.00012376586741889986, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 2.168955087661743, 'lr': 0.00012165021156558531, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.8912047632038593, 'accuracy': 0.85, 'f1_score': 0.8504541341274271, 'epoch_time_seconds': 8.770827531814575, 'memory_usage_mb': 1484.5234375}\n",
      "New best accuracy: 0.8500\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 2.224895477294922, 'lr': 0.0001195345557122708, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 2.167426586151123, 'lr': 0.00011741889985895626, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 2.071768045425415, 'lr': 0.00011530324400564175, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 2.1476917266845703, 'lr': 0.00011318758815232721, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 2.123798131942749, 'lr': 0.00011107193229901268, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 2.1405813694000244, 'lr': 0.00010895627644569815, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 2.034595251083374, 'lr': 0.00010684062059238363, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.855193067342043, 'accuracy': 0.856, 'f1_score': 0.8552807334433967, 'epoch_time_seconds': 8.59815526008606, 'memory_usage_mb': 1484.5234375}\n",
      "New best accuracy: 0.8560\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 2.1231071949005127, 'lr': 0.0001047249647390691, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 2.214273452758789, 'lr': 0.00010260930888575459, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 2.243391990661621, 'lr': 0.00010049365303244004, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 2.0305655002593994, 'lr': 9.837799717912551e-05, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 2.1013307571411133, 'lr': 9.626234132581099e-05, 'memory_usage_mb': 1484.5234375}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 2.130199432373047, 'lr': 9.414668547249648e-05, 'memory_usage_mb': 1484.5234375}\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bf468",
   "metadata": {},
   "source": [
    "### Lower Weight Decay (0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4192c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_172958-5829naf9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.001_ag_news_20250421_172958\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/5829naf9\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0, 'memory_usage_mb': 1312.7578125}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 34.359954833984375, 'lr': 1.9108280254777068e-05, 'memory_usage_mb': 1377.89453125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 32.95455551147461, 'lr': 3.8216560509554137e-05, 'memory_usage_mb': 1378.20703125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 32.370121002197266, 'lr': 5.732484076433121e-05, 'memory_usage_mb': 1378.20703125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 30.977359771728516, 'lr': 7.643312101910827e-05, 'memory_usage_mb': 1378.20703125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 31.21917152404785, 'lr': 9.554140127388533e-05, 'memory_usage_mb': 1378.20703125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 29.499906539916992, 'lr': 0.00011464968152866242, 'memory_usage_mb': 1378.20703125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 27.66318267583847, 'accuracy': 0.25, 'f1_score': 0.10811457101710255, 'epoch_time_seconds': 11.977818250656128, 'memory_usage_mb': 1480.83984375}\n",
      "New best accuracy: 0.2500\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 26.281465530395508, 'lr': 0.00013375796178343948, 'memory_usage_mb': 1483.02734375}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 24.659204483032227, 'lr': 0.00015286624203821655, 'memory_usage_mb': 1483.02734375}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 20.965797424316406, 'lr': 0.0001719745222929936, 'memory_usage_mb': 1483.02734375}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 18.17127799987793, 'lr': 0.00019108280254777067, 'memory_usage_mb': 1483.02734375}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 14.970305442810059, 'lr': 0.00021019108280254773, 'memory_usage_mb': 1483.02734375}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 11.277169227600098, 'lr': 0.00022929936305732485, 'memory_usage_mb': 1483.02734375}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 7.7163916528224945, 'accuracy': 0.274, 'f1_score': 0.1549491789219367, 'epoch_time_seconds': 11.569084405899048, 'memory_usage_mb': 1483.02734375}\n",
      "New best accuracy: 0.2740\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 8.962526321411133, 'lr': 0.0002484076433121019, 'memory_usage_mb': 1483.02734375}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 8.088687896728516, 'lr': 0.00026751592356687897, 'memory_usage_mb': 1483.02734375}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 6.773158073425293, 'lr': 0.00028662420382165603, 'memory_usage_mb': 1483.02734375}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 6.105734825134277, 'lr': 0.0002993653032440056, 'memory_usage_mb': 1483.02734375}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 5.090592861175537, 'lr': 0.0002972496473906911, 'memory_usage_mb': 1483.02734375}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.745333194732666, 'lr': 0.00029513399153737656, 'memory_usage_mb': 1483.02734375}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.8963782787323, 'accuracy': 0.658, 'f1_score': 0.653378309052965, 'epoch_time_seconds': 8.876898050308228, 'memory_usage_mb': 1483.02734375}\n",
      "New best accuracy: 0.6580\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.228826522827148, 'lr': 0.00029301833568406207, 'memory_usage_mb': 1483.02734375}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.196692943572998, 'lr': 0.0002909026798307475, 'memory_usage_mb': 1483.02734375}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.9789538383483887, 'lr': 0.000288787023977433, 'memory_usage_mb': 1483.02734375}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.934753656387329, 'lr': 0.00028667136812411843, 'memory_usage_mb': 1483.02734375}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.7278215885162354, 'lr': 0.0002845557122708039, 'memory_usage_mb': 1483.02734375}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.6860733032226562, 'lr': 0.0002824400564174894, 'memory_usage_mb': 1483.02734375}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.672865152359009, 'lr': 0.00028032440056417486, 'memory_usage_mb': 1483.02734375}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.5383086279034615, 'accuracy': 0.782, 'f1_score': 0.7822328539709215, 'epoch_time_seconds': 8.840429782867432, 'memory_usage_mb': 1483.02734375}\n",
      "New best accuracy: 0.7820\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.6608057022094727, 'lr': 0.00027820874471086036, 'memory_usage_mb': 1483.33984375}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.6296610832214355, 'lr': 0.0002760930888575458, 'memory_usage_mb': 1483.33984375}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.506420135498047, 'lr': 0.0002739774330042313, 'memory_usage_mb': 1483.33984375}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.4576199054718018, 'lr': 0.0002718617771509168, 'memory_usage_mb': 1483.33984375}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.4331095218658447, 'lr': 0.00026974612129760224, 'memory_usage_mb': 1483.33984375}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.38704514503479, 'lr': 0.0002676304654442877, 'memory_usage_mb': 1483.33984375}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.2892544716596603, 'accuracy': 0.772, 'f1_score': 0.7719509726554182, 'epoch_time_seconds': 8.871683120727539, 'memory_usage_mb': 1483.33984375}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.395057439804077, 'lr': 0.00026551480959097315, 'memory_usage_mb': 1483.33984375}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.397505760192871, 'lr': 0.00026339915373765866, 'memory_usage_mb': 1483.33984375}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.2388384342193604, 'lr': 0.0002612834978843441, 'memory_usage_mb': 1483.33984375}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.1687300205230713, 'lr': 0.00025916784203102957, 'memory_usage_mb': 1483.33984375}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.14302396774292, 'lr': 0.0002570521861777151, 'memory_usage_mb': 1483.33984375}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.164731979370117, 'lr': 0.00025493653032440054, 'memory_usage_mb': 1483.33984375}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 2.93791700899601, 'accuracy': 0.798, 'f1_score': 0.7915737476220672, 'epoch_time_seconds': 8.865905284881592, 'memory_usage_mb': 1483.33984375}\n",
      "New best accuracy: 0.7980\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.0937161445617676, 'lr': 0.00025282087447108605, 'memory_usage_mb': 1483.33984375}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.0792245864868164, 'lr': 0.0002507052186177715, 'memory_usage_mb': 1483.33984375}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.9703845977783203, 'lr': 0.00024858956276445696, 'memory_usage_mb': 1483.33984375}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.938633918762207, 'lr': 0.00024647390691114247, 'memory_usage_mb': 1483.33984375}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.9018735885620117, 'lr': 0.00024435825105782787, 'memory_usage_mb': 1483.33984375}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.9748032093048096, 'lr': 0.00024224259520451338, 'memory_usage_mb': 1483.33984375}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 2.9358203411102295, 'lr': 0.00024012693935119883, 'memory_usage_mb': 1483.33984375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 441: {'epoch': 6, 'eval_loss': 2.6749904081225395, 'accuracy': 0.764, 'f1_score': 0.7601472359998677, 'epoch_time_seconds': 8.845281839370728, 'memory_usage_mb': 1483.33984375}\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 2.944171667098999, 'lr': 0.00023801128349788432, 'memory_usage_mb': 1483.33984375}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.820309638977051, 'lr': 0.0002358956276445698, 'memory_usage_mb': 1483.33984375}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.807262420654297, 'lr': 0.00023377997179125528, 'memory_usage_mb': 1483.33984375}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.7000787258148193, 'lr': 0.00023166431593794074, 'memory_usage_mb': 1483.33984375}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.6937427520751953, 'lr': 0.00022954866008462622, 'memory_usage_mb': 1483.33984375}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.6800765991210938, 'lr': 0.0002274330042313117, 'memory_usage_mb': 1483.33984375}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.432934284210205, 'accuracy': 0.8, 'f1_score': 0.7973763217209893, 'epoch_time_seconds': 8.854475736618042, 'memory_usage_mb': 1483.33984375}\n",
      "New best accuracy: 0.8000\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.6521682739257812, 'lr': 0.00022531734837799718, 'memory_usage_mb': 1483.33984375}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.683943748474121, 'lr': 0.0002232016925246826, 'memory_usage_mb': 1483.33984375}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.5974576473236084, 'lr': 0.0002210860366713681, 'memory_usage_mb': 1483.33984375}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.6249656677246094, 'lr': 0.00021897038081805358, 'memory_usage_mb': 1483.33984375}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.5628111362457275, 'lr': 0.00021685472496473906, 'memory_usage_mb': 1483.33984375}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.542311191558838, 'lr': 0.00021473906911142451, 'memory_usage_mb': 1483.33984375}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.317439004778862, 'accuracy': 0.796, 'f1_score': 0.7943682688738067, 'epoch_time_seconds': 8.900922536849976, 'memory_usage_mb': 1483.33984375}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.5574872493743896, 'lr': 0.00021262341325811, 'memory_usage_mb': 1483.33984375}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.5641300678253174, 'lr': 0.00021050775740479548, 'memory_usage_mb': 1483.33984375}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.537135601043701, 'lr': 0.00020839210155148096, 'memory_usage_mb': 1483.33984375}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.583500623703003, 'lr': 0.00020627644569816642, 'memory_usage_mb': 1483.33984375}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.430966377258301, 'lr': 0.0002041607898448519, 'memory_usage_mb': 1483.33984375}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.6539132595062256, 'lr': 0.00020204513399153736, 'memory_usage_mb': 1483.33984375}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.210000179708004, 'accuracy': 0.814, 'f1_score': 0.8102697930724249, 'epoch_time_seconds': 8.865799903869629, 'memory_usage_mb': 1483.33984375}\n",
      "New best accuracy: 0.8140\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.540073871612549, 'lr': 0.0001999294781382228, 'memory_usage_mb': 1483.33984375}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.3900551795959473, 'lr': 0.0001978138222849083, 'memory_usage_mb': 1483.33984375}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.4920494556427, 'lr': 0.00019569816643159378, 'memory_usage_mb': 1483.33984375}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.445079803466797, 'lr': 0.00019358251057827926, 'memory_usage_mb': 1483.33984375}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.4183826446533203, 'lr': 0.00019146685472496471, 'memory_usage_mb': 1483.33984375}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.3443286418914795, 'lr': 0.0001893511988716502, 'memory_usage_mb': 1483.33984375}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.331758499145508, 'lr': 0.00018723554301833568, 'memory_usage_mb': 1483.33984375}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.133316569030285, 'accuracy': 0.828, 'f1_score': 0.8259879268551568, 'epoch_time_seconds': 8.871150732040405, 'memory_usage_mb': 1483.33984375}\n",
      "New best accuracy: 0.8280\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.3955583572387695, 'lr': 0.00018511988716502116, 'memory_usage_mb': 1483.33984375}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.335881471633911, 'lr': 0.00018300423131170662, 'memory_usage_mb': 1483.33984375}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.3609211444854736, 'lr': 0.00018088857545839207, 'memory_usage_mb': 1483.33984375}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.335200786590576, 'lr': 0.00017877291960507755, 'memory_usage_mb': 1483.33984375}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.405867338180542, 'lr': 0.000176657263751763, 'memory_usage_mb': 1483.33984375}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.3866641521453857, 'lr': 0.0001745416078984485, 'memory_usage_mb': 1483.33984375}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.0605810284614563, 'accuracy': 0.818, 'f1_score': 0.8161648298950415, 'epoch_time_seconds': 8.85852837562561, 'memory_usage_mb': 1483.33984375}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.350205183029175, 'lr': 0.00017242595204513398, 'memory_usage_mb': 1483.33984375}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.315889596939087, 'lr': 0.00017031029619181946, 'memory_usage_mb': 1483.33984375}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.353160858154297, 'lr': 0.00016819464033850494, 'memory_usage_mb': 1483.33984375}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.25848650932312, 'lr': 0.0001660789844851904, 'memory_usage_mb': 1483.33984375}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.2687880992889404, 'lr': 0.00016396332863187588, 'memory_usage_mb': 1483.33984375}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.2483816146850586, 'lr': 0.00016184767277856136, 'memory_usage_mb': 1483.33984375}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.016035061329603, 'accuracy': 0.822, 'f1_score': 0.8202116586590986, 'epoch_time_seconds': 8.851123809814453, 'memory_usage_mb': 1483.33984375}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.271514654159546, 'lr': 0.0001597320169252468, 'memory_usage_mb': 1483.33984375}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.2995858192443848, 'lr': 0.00015761636107193227, 'memory_usage_mb': 1483.65234375}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.2544753551483154, 'lr': 0.00015550070521861775, 'memory_usage_mb': 1483.65234375}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.1952295303344727, 'lr': 0.00015338504936530324, 'memory_usage_mb': 1483.65234375}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.279215097427368, 'lr': 0.0001512693935119887, 'memory_usage_mb': 1483.65234375}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.193357467651367, 'lr': 0.00014915373765867417, 'memory_usage_mb': 1483.65234375}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.16154146194458, 'lr': 0.00014703808180535966, 'memory_usage_mb': 1483.65234375}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.954367633908987, 'accuracy': 0.836, 'f1_score': 0.8352716595340514, 'epoch_time_seconds': 9.015456676483154, 'memory_usage_mb': 1483.65234375}\n",
      "New best accuracy: 0.8360\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.205686092376709, 'lr': 0.00014492242595204514, 'memory_usage_mb': 1483.65234375}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.309033155441284, 'lr': 0.0001428067700987306, 'memory_usage_mb': 1483.65234375}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.170539140701294, 'lr': 0.00014069111424541608, 'memory_usage_mb': 1483.65234375}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.1770544052124023, 'lr': 0.00013857545839210153, 'memory_usage_mb': 1483.65234375}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.176305055618286, 'lr': 0.00013645980253878702, 'memory_usage_mb': 1483.65234375}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.170043468475342, 'lr': 0.0001343441466854725, 'memory_usage_mb': 1483.65234375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 945: {'epoch': 14, 'eval_loss': 1.9268530793488026, 'accuracy': 0.83, 'f1_score': 0.8287439938556067, 'epoch_time_seconds': 9.112598657608032, 'memory_usage_mb': 1483.65234375}\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 2.1732125282287598, 'lr': 0.00013222849083215795, 'memory_usage_mb': 1483.65234375}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 2.0955727100372314, 'lr': 0.00013011283497884344, 'memory_usage_mb': 1483.65234375}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 2.128443717956543, 'lr': 0.0001279971791255289, 'memory_usage_mb': 1483.65234375}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.2245070934295654, 'lr': 0.00012588152327221437, 'memory_usage_mb': 1483.65234375}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 2.148832321166992, 'lr': 0.00012376586741889986, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 2.1617379188537598, 'lr': 0.00012165021156558531, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.891578659415245, 'accuracy': 0.836, 'f1_score': 0.8350247257615678, 'epoch_time_seconds': 9.165855646133423, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 2.2274625301361084, 'lr': 0.0001195345557122708, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 2.1768996715545654, 'lr': 0.00011741889985895626, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 2.1096479892730713, 'lr': 0.00011530324400564175, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 2.152069568634033, 'lr': 0.00011318758815232721, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 2.1160547733306885, 'lr': 0.00011107193229901268, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 2.1287639141082764, 'lr': 0.00010895627644569815, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 2.0385894775390625, 'lr': 0.00010684062059238363, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.860382430255413, 'accuracy': 0.842, 'f1_score': 0.8413190748147078, 'epoch_time_seconds': 8.953906774520874, 'memory_usage_mb': 1483.65234375}\n",
      "New best accuracy: 0.8420\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 2.1181387901306152, 'lr': 0.0001047249647390691, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 2.2257983684539795, 'lr': 0.00010260930888575459, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 2.2394864559173584, 'lr': 0.00010049365303244004, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 2.0551364421844482, 'lr': 9.837799717912551e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 2.115326166152954, 'lr': 9.626234132581099e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 2.1237239837646484, 'lr': 9.414668547249648e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.837934248149395, 'accuracy': 0.848, 'f1_score': 0.8488672019984049, 'epoch_time_seconds': 8.95601487159729, 'memory_usage_mb': 1483.65234375}\n",
      "New best accuracy: 0.8480\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 2.064276695251465, 'lr': 9.203102961918194e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 2.051990270614624, 'lr': 8.99153737658674e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 2.3350472450256348, 'lr': 8.779971791255288e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 2.153439521789551, 'lr': 8.568406205923835e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 2.0542402267456055, 'lr': 8.356840620592383e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 2.082822322845459, 'lr': 8.14527503526093e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.8145131319761276, 'accuracy': 0.83, 'f1_score': 0.8276583338700432, 'epoch_time_seconds': 8.97919487953186, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 2.036792039871216, 'lr': 7.933709449929477e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.9580862522125244, 'lr': 7.722143864598024e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 2.0678818225860596, 'lr': 7.510578279266572e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 2.0635733604431152, 'lr': 7.299012693935119e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 2.0784592628479004, 'lr': 7.087447108603666e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 2.073784589767456, 'lr': 6.875881523272214e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.8104044198989868, 'accuracy': 0.838, 'f1_score': 0.8359096862208474, 'epoch_time_seconds': 8.968863487243652, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.9961735010147095, 'lr': 6.664315937940761e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 2.0191304683685303, 'lr': 6.452750352609308e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 2.0213565826416016, 'lr': 6.241184767277856e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.9912506341934204, 'lr': 6.0296191819464026e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 2.0567564964294434, 'lr': 5.81805359661495e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 2.049816370010376, 'lr': 5.606488011283497e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 2.074798345565796, 'lr': 5.394922425952045e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.783640343695879, 'accuracy': 0.856, 'f1_score': 0.8554052286839154, 'epoch_time_seconds': 8.956272602081299, 'memory_usage_mb': 1483.65234375}\n",
      "New best accuracy: 0.8560\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 2.0318706035614014, 'lr': 5.183356840620592e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.9587252140045166, 'lr': 4.971791255289139e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.989180088043213, 'lr': 4.760225669957687e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 2.0510077476501465, 'lr': 4.5486600846262336e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.9658812284469604, 'lr': 4.337094499294781e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 2.0214641094207764, 'lr': 4.125528913963329e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.7826007269322872, 'accuracy': 0.846, 'f1_score': 0.8445715983649952, 'epoch_time_seconds': 8.962743759155273, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 2.0127291679382324, 'lr': 3.913963328631875e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 2.01261568069458, 'lr': 3.7023977433004226e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 2.0088272094726562, 'lr': 3.49083215796897e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 2.0650064945220947, 'lr': 3.279266572637517e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 2.0042545795440674, 'lr': 3.0677009873060646e-05, 'memory_usage_mb': 1483.65234375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 2.158935546875, 'lr': 2.856135401974612e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 1.7749987058341503, 'accuracy': 0.848, 'f1_score': 0.8468280339434383, 'epoch_time_seconds': 9.012921333312988, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.9766334295272827, 'lr': 2.644569816643159e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 2.10902738571167, 'lr': 2.4330042313117063e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.9963916540145874, 'lr': 2.2214386459802535e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 2.0789954662323, 'lr': 2.009873060648801e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 2.0016908645629883, 'lr': 1.798307475317348e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.9540278911590576, 'lr': 1.5867418899858956e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.9617948532104492, 'lr': 1.3751763046544426e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.7678220123052597, 'accuracy': 0.856, 'f1_score': 0.8554566258181476, 'epoch_time_seconds': 8.9535813331604, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 2.090773344039917, 'lr': 1.16361071932299e-05, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.931485891342163, 'lr': 9.520451339915373e-06, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 2.02590274810791, 'lr': 7.404795486600846e-06, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 2.0159761905670166, 'lr': 5.289139633286318e-06, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.9608732461929321, 'lr': 3.1734837799717906e-06, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.9599515199661255, 'lr': 1.0578279266572636e-06, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.7681210674345493, 'accuracy': 0.852, 'f1_score': 0.8512462854498675, 'epoch_time_seconds': 8.970892190933228, 'memory_usage_mb': 1483.65234375}\n",
      "Step 1575: {'total_training_time_seconds': 229.111079454422, 'avg_epoch_time_seconds': 9.16229616165161, 'best_accuracy': 0.856, 'convergence_step': 'Not converged', 'memory_usage_mb': 1483.65234375}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 9.1623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.97089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.76812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.85125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1483.65234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 229.11108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.95995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.001_ag_news_20250421_172958\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/5829naf9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_172958-5829naf9/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.001 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312a8ef",
   "metadata": {},
   "source": [
    "### Plateau with F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285e5822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250421_174119-xj689ere\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_ag_news_20250421_174119\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/xj689ere\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: ag_news\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.238651275634766, 'lr': 0.0003, 'memory_usage_mb': 1365.96484375}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 32.374046325683594, 'lr': 0.0003, 'memory_usage_mb': 1430.78125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 27.264545440673828, 'lr': 0.0003, 'memory_usage_mb': 1431.40625}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 23.402685165405273, 'lr': 0.0003, 'memory_usage_mb': 1431.40625}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 19.61326789855957, 'lr': 0.0003, 'memory_usage_mb': 1431.40625}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 16.914587020874023, 'lr': 0.0003, 'memory_usage_mb': 1431.40625}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 11.505192756652832, 'lr': 0.0003, 'memory_usage_mb': 1431.40625}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 8.270396739244461, 'accuracy': 0.274, 'f1_score': 0.15758627634493952, 'epoch_time_seconds': 11.986407041549683, 'memory_usage_mb': 1534.015625}\n",
      "New best accuracy: 0.2740\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 9.201558113098145, 'lr': 0.0003, 'memory_usage_mb': 1535.890625}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 7.916651248931885, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 7.297325134277344, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 6.299482345581055, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 5.2289605140686035, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 4.7654218673706055, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 3.9304012283682823, 'accuracy': 0.678, 'f1_score': 0.663251515410228, 'epoch_time_seconds': 8.871753931045532, 'memory_usage_mb': 1536.203125}\n",
      "New best accuracy: 0.6780\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 4.394257545471191, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.143557548522949, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.030944347381592, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 3.919788360595703, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 3.841381311416626, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 3.836195468902588, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.5917198583483696, 'accuracy': 0.752, 'f1_score': 0.7427694440095739, 'epoch_time_seconds': 8.845530033111572, 'memory_usage_mb': 1536.203125}\n",
      "New best accuracy: 0.7520\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 3.694894552230835, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 3.7083663940429688, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.5852370262145996, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.606839895248413, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.4811999797821045, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.4577903747558594, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.419304609298706, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.310423344373703, 'accuracy': 0.788, 'f1_score': 0.7866016615431211, 'epoch_time_seconds': 8.861309289932251, 'memory_usage_mb': 1536.203125}\n",
      "New best accuracy: 0.7880\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.440838575363159, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.39803409576416, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.266427755355835, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.164292335510254, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.1569056510925293, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.065777540206909, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 2.930202327668667, 'accuracy': 0.728, 'f1_score': 0.7216832850753775, 'epoch_time_seconds': 8.855492353439331, 'memory_usage_mb': 1536.203125}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.1035866737365723, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.0767011642456055, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.0078330039978027, 'lr': 0.0003, 'memory_usage_mb': 1536.203125}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 2.897343158721924, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 2.883071184158325, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 2.8677000999450684, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 2.6256391927599907, 'accuracy': 0.778, 'f1_score': 0.7674454374404653, 'epoch_time_seconds': 8.829567670822144, 'memory_usage_mb': 1536.515625}\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 2.8615963459014893, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 2.892716884613037, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.719242811203003, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.6684505939483643, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.6563284397125244, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.712092876434326, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 2.6852588653564453, 'lr': 0.0003, 'memory_usage_mb': 1536.515625}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 2.4226046055555344, 'accuracy': 0.71, 'f1_score': 0.7067627141422346, 'epoch_time_seconds': 8.875693321228027, 'memory_usage_mb': 1536.515625}\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 2.680004835128784, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.5929830074310303, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.572615385055542, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.495389461517334, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.47910475730896, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.505985736846924, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.2699061930179596, 'accuracy': 0.81, 'f1_score': 0.8097096491613244, 'epoch_time_seconds': 8.854790687561035, 'memory_usage_mb': 1536.515625}\n",
      "New best accuracy: 0.8100\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.5082132816314697, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.5116193294525146, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.453382730484009, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.5082929134368896, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.429492712020874, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.4458374977111816, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.2044025287032127, 'accuracy': 0.8, 'f1_score': 0.7984030012334901, 'epoch_time_seconds': 8.845888376235962, 'memory_usage_mb': 1536.515625}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.433715581893921, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.4624953269958496, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.4343719482421875, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.4640469551086426, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.3392863273620605, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.5284693241119385, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.1185367330908775, 'accuracy': 0.822, 'f1_score': 0.8197831829460861, 'epoch_time_seconds': 8.874386072158813, 'memory_usage_mb': 1536.515625}\n",
      "New best accuracy: 0.8220\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.487638235092163, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.3243772983551025, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.4251227378845215, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.378406286239624, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.3646399974823, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.2916243076324463, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.2873318195343018, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.059249848127365, 'accuracy': 0.834, 'f1_score': 0.8336797938903202, 'epoch_time_seconds': 8.847042560577393, 'memory_usage_mb': 1536.515625}\n",
      "New best accuracy: 0.8340\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.303739070892334, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.2822651863098145, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.2986602783203125, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.263697862625122, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.336480140686035, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.391550064086914, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.0190591774880886, 'accuracy': 0.814, 'f1_score': 0.811214363283775, 'epoch_time_seconds': 8.86087942123413, 'memory_usage_mb': 1536.515625}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.2825160026550293, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.2692954540252686, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.3169384002685547, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.2117574214935303, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.223742723464966, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.2066617012023926, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 1.9536692909896374, 'accuracy': 0.844, 'f1_score': 0.8432169811876444, 'epoch_time_seconds': 8.86734676361084, 'memory_usage_mb': 1536.515625}\n",
      "New best accuracy: 0.8440\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.219115972518921, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.264714002609253, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.191225290298462, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.1597394943237305, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.2334606647491455, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.1289572715759277, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.084747791290283, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 1.8973931334912777, 'accuracy': 0.84, 'f1_score': 0.838891585619878, 'epoch_time_seconds': 8.909088373184204, 'memory_usage_mb': 1536.515625}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.155048131942749, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.263249158859253, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.128164291381836, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.1543900966644287, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.1251790523529053, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.097369432449341, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 1.8768148198723793, 'accuracy': 0.81, 'f1_score': 0.8070241904360759, 'epoch_time_seconds': 8.863396883010864, 'memory_usage_mb': 1536.515625}\n",
      "Step 950: {'epoch': 15, 'step': 950, 'train_loss': 2.1014795303344727, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 960: {'epoch': 15, 'step': 960, 'train_loss': 2.016002893447876, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 970: {'epoch': 15, 'step': 970, 'train_loss': 2.1078412532806396, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 980: {'epoch': 15, 'step': 980, 'train_loss': 2.231201171875, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 990: {'epoch': 15, 'step': 990, 'train_loss': 2.076962947845459, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1000: {'epoch': 15, 'step': 1000, 'train_loss': 2.099182367324829, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1008: {'epoch': 15, 'eval_loss': 1.821573507040739, 'accuracy': 0.85, 'f1_score': 0.8497505451979357, 'epoch_time_seconds': 9.024493217468262, 'memory_usage_mb': 1536.515625}\n",
      "New best accuracy: 0.8500\n",
      "Step 1010: {'epoch': 16, 'step': 1010, 'train_loss': 2.1356990337371826, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1020: {'epoch': 16, 'step': 1020, 'train_loss': 2.097686529159546, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1030: {'epoch': 16, 'step': 1030, 'train_loss': 2.049044370651245, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1040: {'epoch': 16, 'step': 1040, 'train_loss': 2.075263261795044, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1050: {'epoch': 16, 'step': 1050, 'train_loss': 2.06758713722229, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1060: {'epoch': 16, 'step': 1060, 'train_loss': 2.0696542263031006, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1070: {'epoch': 16, 'step': 1070, 'train_loss': 1.9731217622756958, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1071: {'epoch': 16, 'eval_loss': 1.788252156227827, 'accuracy': 0.854, 'f1_score': 0.853684096637495, 'epoch_time_seconds': 9.021052837371826, 'memory_usage_mb': 1536.515625}\n",
      "New best accuracy: 0.8540\n",
      "Step 1080: {'epoch': 17, 'step': 1080, 'train_loss': 2.065493106842041, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1090: {'epoch': 17, 'step': 1090, 'train_loss': 2.112428903579712, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1100: {'epoch': 17, 'step': 1100, 'train_loss': 2.177882432937622, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1110: {'epoch': 17, 'step': 1110, 'train_loss': 1.950877070426941, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1120: {'epoch': 17, 'step': 1120, 'train_loss': 2.013962745666504, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1130: {'epoch': 17, 'step': 1130, 'train_loss': 2.074125289916992, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1134: {'epoch': 17, 'eval_loss': 1.7662686482071877, 'accuracy': 0.846, 'f1_score': 0.8469802574116067, 'epoch_time_seconds': 9.001307487487793, 'memory_usage_mb': 1536.515625}\n",
      "Step 1140: {'epoch': 18, 'step': 1140, 'train_loss': 1.9847869873046875, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1150: {'epoch': 18, 'step': 1150, 'train_loss': 1.9245891571044922, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1160: {'epoch': 18, 'step': 1160, 'train_loss': 2.221170425415039, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1170: {'epoch': 18, 'step': 1170, 'train_loss': 2.051669120788574, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1180: {'epoch': 18, 'step': 1180, 'train_loss': 2.0053815841674805, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1190: {'epoch': 18, 'step': 1190, 'train_loss': 2.029726028442383, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1197: {'epoch': 18, 'eval_loss': 1.747698649764061, 'accuracy': 0.814, 'f1_score': 0.8110380676814231, 'epoch_time_seconds': 8.92140793800354, 'memory_usage_mb': 1536.515625}\n",
      "Step 1200: {'epoch': 19, 'step': 1200, 'train_loss': 1.9428144693374634, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1210: {'epoch': 19, 'step': 1210, 'train_loss': 1.8883187770843506, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1220: {'epoch': 19, 'step': 1220, 'train_loss': 1.9890739917755127, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1230: {'epoch': 19, 'step': 1230, 'train_loss': 1.9500658512115479, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1240: {'epoch': 19, 'step': 1240, 'train_loss': 1.973705530166626, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1250: {'epoch': 19, 'step': 1250, 'train_loss': 1.980206847190857, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1260: {'epoch': 19, 'eval_loss': 1.7271054275333881, 'accuracy': 0.854, 'f1_score': 0.8539635508763983, 'epoch_time_seconds': 8.924640893936157, 'memory_usage_mb': 1536.515625}\n",
      "Step 1260: {'epoch': 20, 'step': 1260, 'train_loss': 1.9370107650756836, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1270: {'epoch': 20, 'step': 1270, 'train_loss': 1.9066416025161743, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1280: {'epoch': 20, 'step': 1280, 'train_loss': 1.948865532875061, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1290: {'epoch': 20, 'step': 1290, 'train_loss': 1.8956317901611328, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1300: {'epoch': 20, 'step': 1300, 'train_loss': 1.9787869453430176, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1310: {'epoch': 20, 'step': 1310, 'train_loss': 1.9586955308914185, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1320: {'epoch': 20, 'step': 1320, 'train_loss': 1.985897183418274, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1323: {'epoch': 20, 'eval_loss': 1.6982866115868092, 'accuracy': 0.85, 'f1_score': 0.8501515594838912, 'epoch_time_seconds': 8.932209730148315, 'memory_usage_mb': 1536.515625}\n",
      "Step 1330: {'epoch': 21, 'step': 1330, 'train_loss': 1.9346195459365845, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1340: {'epoch': 21, 'step': 1340, 'train_loss': 1.8372613191604614, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1350: {'epoch': 21, 'step': 1350, 'train_loss': 1.8759154081344604, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1360: {'epoch': 21, 'step': 1360, 'train_loss': 1.9494529962539673, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1370: {'epoch': 21, 'step': 1370, 'train_loss': 1.861322283744812, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1380: {'epoch': 21, 'step': 1380, 'train_loss': 1.8845559358596802, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1386: {'epoch': 21, 'eval_loss': 1.6764161586761475, 'accuracy': 0.842, 'f1_score': 0.8406827750956639, 'epoch_time_seconds': 8.921769380569458, 'memory_usage_mb': 1536.515625}\n",
      "Step 1390: {'epoch': 22, 'step': 1390, 'train_loss': 1.8897268772125244, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1400: {'epoch': 22, 'step': 1400, 'train_loss': 1.9205490350723267, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1410: {'epoch': 22, 'step': 1410, 'train_loss': 1.875350832939148, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1420: {'epoch': 22, 'step': 1420, 'train_loss': 1.9195793867111206, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1430: {'epoch': 22, 'step': 1430, 'train_loss': 1.8832203149795532, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1440: {'epoch': 22, 'step': 1440, 'train_loss': 2.0192744731903076, 'lr': 0.00015, 'memory_usage_mb': 1536.515625}\n",
      "Step 1449: {'epoch': 22, 'eval_loss': 1.6770411394536495, 'accuracy': 0.766, 'f1_score': 0.762365890832218, 'epoch_time_seconds': 8.941946744918823, 'memory_usage_mb': 1536.515625}\n",
      "Step 1450: {'epoch': 23, 'step': 1450, 'train_loss': 1.8331403732299805, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1460: {'epoch': 23, 'step': 1460, 'train_loss': 1.9208770990371704, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1470: {'epoch': 23, 'step': 1470, 'train_loss': 1.8739979267120361, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1480: {'epoch': 23, 'step': 1480, 'train_loss': 1.9355300664901733, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1490: {'epoch': 23, 'step': 1490, 'train_loss': 1.8775677680969238, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1500: {'epoch': 23, 'step': 1500, 'train_loss': 1.8605220317840576, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1510: {'epoch': 23, 'step': 1510, 'train_loss': 1.8362575769424438, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1512: {'epoch': 23, 'eval_loss': 1.643386710435152, 'accuracy': 0.868, 'f1_score': 0.867595256930101, 'epoch_time_seconds': 8.942431449890137, 'memory_usage_mb': 1536.515625}\n",
      "New best accuracy: 0.8680\n",
      "Step 1520: {'epoch': 24, 'step': 1520, 'train_loss': 1.920883059501648, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1530: {'epoch': 24, 'step': 1530, 'train_loss': 1.8035463094711304, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1540: {'epoch': 24, 'step': 1540, 'train_loss': 1.9051650762557983, 'lr': 7.5e-05, 'memory_usage_mb': 1536.515625}\n",
      "Step 1550: {'epoch': 24, 'step': 1550, 'train_loss': 1.851379632949829, 'lr': 7.5e-05, 'memory_usage_mb': 1536.828125}\n",
      "Step 1560: {'epoch': 24, 'step': 1560, 'train_loss': 1.8107433319091797, 'lr': 7.5e-05, 'memory_usage_mb': 1536.828125}\n",
      "Step 1570: {'epoch': 24, 'step': 1570, 'train_loss': 1.8372802734375, 'lr': 7.5e-05, 'memory_usage_mb': 1536.828125}\n",
      "Step 1575: {'epoch': 24, 'eval_loss': 1.6351350881159306, 'accuracy': 0.846, 'f1_score': 0.8449387618825356, 'epoch_time_seconds': 8.926908254623413, 'memory_usage_mb': 1536.828125}\n",
      "Step 1575: {'total_training_time_seconds': 225.657860994339, 'avg_epoch_time_seconds': 9.024269628524781, 'best_accuracy': 0.868, 'convergence_step': 'Not converged', 'memory_usage_mb': 1536.828125}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy ‚ñÅ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    accuracy 0.846\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 9.02427\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.868\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 24\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 8.92691\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 1.63514\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    f1_score 0.84494\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 7e-05\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1536.82812\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 1570\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 225.65786\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 1.83728\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_ag_news_20250421_174119\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence/runs/xj689ere\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_174119-xj689ere/logs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python peft_training.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule plateau \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7884d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
