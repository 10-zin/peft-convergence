{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c051d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cf52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hice1/pgarg76/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c6977",
   "metadata": {},
   "source": [
    "### Full Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c026c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_190041-ezkcbr7i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfull_ft_adamw_warmup_lr3e-05_wd0.01_samsum_20250425_190041\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/ezkcbr7i\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Using full fine-tuning (no PEFT)\n",
      "Total parameters: 76961152\n",
      "Trainable parameters: 76961152 (100.00%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 38.448463439941406, 'lr': 0.0, 'memory_usage_mb': 1371.75390625}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.31623840332031, 'lr': 3.1914893617021277e-06, 'memory_usage_mb': 1438.87890625}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 40.869197845458984, 'lr': 6.3829787234042555e-06, 'memory_usage_mb': 1439.50390625}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 36.898094177246094, 'lr': 9.574468085106385e-06, 'memory_usage_mb': 1440.12890625}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 34.05741500854492, 'lr': 1.2765957446808511e-05, 'memory_usage_mb': 1440.12890625}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 34.19036102294922, 'lr': 1.5957446808510637e-05, 'memory_usage_mb': 1440.12890625}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 30.270198822021484, 'lr': 1.914893617021277e-05, 'memory_usage_mb': 1440.12890625}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 29.414682924747467, 'rougeL': 0.3190520158499688, 'epoch_time_seconds': 61.530935764312744, 'memory_usage_mb': 1571.94921875}\n",
      "New best accuracy: 0.3191\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 30.157299041748047, 'lr': 2.2340425531914894e-05, 'memory_usage_mb': 1571.94921875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 24.646257400512695, 'lr': 2.5531914893617022e-05, 'memory_usage_mb': 1571.94921875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 22.424026489257812, 'lr': 2.872340425531915e-05, 'memory_usage_mb': 1571.94921875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 18.38990020751953, 'lr': 2.9788484136310225e-05, 'memory_usage_mb': 1571.94921875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 15.352380752563477, 'lr': 2.9435957696827263e-05, 'memory_usage_mb': 1571.94921875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 11.944916725158691, 'lr': 2.90834312573443e-05, 'memory_usage_mb': 1571.94921875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 6.999547764658928, 'rougeL': 0.14588082819957987, 'epoch_time_seconds': 33.26512145996094, 'memory_usage_mb': 1572.26171875}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 8.441810607910156, 'lr': 2.873090481786134e-05, 'memory_usage_mb': 1572.26171875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 7.406297206878662, 'lr': 2.8378378378378378e-05, 'memory_usage_mb': 1572.26171875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 6.642590045928955, 'lr': 2.8025851938895417e-05, 'memory_usage_mb': 1572.26171875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 6.281435966491699, 'lr': 2.767332549941246e-05, 'memory_usage_mb': 1572.26171875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 5.596002101898193, 'lr': 2.7320799059929497e-05, 'memory_usage_mb': 1572.26171875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 5.410026550292969, 'lr': 2.6968272620446535e-05, 'memory_usage_mb': 1572.26171875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.991566814482212, 'rougeL': 0.14420426619476734, 'epoch_time_seconds': 35.52443552017212, 'memory_usage_mb': 1572.57421875}\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.945826053619385, 'lr': 2.661574618096357e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.37682580947876, 'lr': 2.6263219741480612e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.268164157867432, 'lr': 2.591069330199765e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.293353080749512, 'lr': 2.555816686251469e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.9722039699554443, 'lr': 2.5205640423031728e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.817765235900879, 'lr': 2.4853113983548766e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.59090518951416, 'lr': 2.4500587544065808e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.0273430347442627, 'rougeL': 0.20494266413648368, 'epoch_time_seconds': 37.492408990859985, 'memory_usage_mb': 1572.57421875}\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.584094285964966, 'lr': 2.4148061104582846e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.327974319458008, 'lr': 2.379553466509988e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.1528637409210205, 'lr': 2.344300822561692e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.089378595352173, 'lr': 2.309048178613396e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 2.94122052192688, 'lr': 2.2737955346651e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 2.9050517082214355, 'lr': 2.238542890716804e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 2.3999498710036278, 'rougeL': 0.231094273792537, 'epoch_time_seconds': 51.84300374984741, 'memory_usage_mb': 1572.57421875}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 2.7766222953796387, 'lr': 2.2032902467685077e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 2.7032387256622314, 'lr': 2.1680376028202115e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 2.589606285095215, 'lr': 2.1327849588719157e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 2.5057997703552246, 'lr': 2.0975323149236192e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 2.425419330596924, 'lr': 2.062279670975323e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 2.3704280853271484, 'lr': 2.027027027027027e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 1.8822272941470146, 'rougeL': 0.24407387446448162, 'epoch_time_seconds': 66.68598985671997, 'memory_usage_mb': 1572.57421875}\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 2.3641767501831055, 'lr': 1.991774383078731e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 2.214564323425293, 'lr': 1.956521739130435e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.070436477661133, 'lr': 1.9212690951821388e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.136025905609131, 'lr': 1.8860164512338426e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.1050877571105957, 'lr': 1.8507638072855465e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.0129776000976562, 'lr': 1.8155111633372503e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 1.8307902812957764, 'lr': 1.7802585193889542e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 1.4914898052811623, 'rougeL': 0.2564684488977127, 'epoch_time_seconds': 74.0044629573822, 'memory_usage_mb': 1572.57421875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 1.878329873085022, 'lr': 1.745005875440658e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 1.9167723655700684, 'lr': 1.709753231492362e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 1.765771508216858, 'lr': 1.674500587544066e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 1.7415722608566284, 'lr': 1.63924794359577e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 1.615600824356079, 'lr': 1.6039952996474737e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 1.6021077632904053, 'lr': 1.5687426556991776e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 1.222419910132885, 'rougeL': 0.2590421649241264, 'epoch_time_seconds': 75.4749505519867, 'memory_usage_mb': 1572.57421875}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 1.573095440864563, 'lr': 1.533490011750881e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 1.5641639232635498, 'lr': 1.4982373678025853e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 1.5317405462265015, 'lr': 1.4629847238542891e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 1.427839756011963, 'lr': 1.427732079905993e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 1.4550716876983643, 'lr': 1.392479435957697e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 1.3402955532073975, 'lr': 1.3572267920094008e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 1.0251339487731457, 'rougeL': 0.26445596391306003, 'epoch_time_seconds': 71.82787108421326, 'memory_usage_mb': 1572.57421875}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 1.4118733406066895, 'lr': 1.3219741480611045e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 1.3622807264328003, 'lr': 1.2867215041128085e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 1.3190919160842896, 'lr': 1.2514688601645123e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 1.2826805114746094, 'lr': 1.2162162162162164e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 1.365385890007019, 'lr': 1.18096357226792e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 1.2883920669555664, 'lr': 1.145710928319624e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 0.9001978654414415, 'rougeL': 0.26583328124971545, 'epoch_time_seconds': 66.77372765541077, 'memory_usage_mb': 1572.57421875}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 1.273720383644104, 'lr': 1.1104582843713279e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 1.2720507383346558, 'lr': 1.0752056404230319e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 1.26948881149292, 'lr': 1.0399529964747356e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 1.274625539779663, 'lr': 1.0047003525264394e-05, 'memory_usage_mb': 1572.57421875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 1.2208834886550903, 'lr': 9.694477085781434e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 1.2401881217956543, 'lr': 9.341950646298473e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 1.2344000339508057, 'lr': 8.989424206815511e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 0.8162698373198509, 'rougeL': 0.2684485930822001, 'epoch_time_seconds': 58.60164833068848, 'memory_usage_mb': 1572.57421875}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 1.0842158794403076, 'lr': 8.63689776733255e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 1.092478632926941, 'lr': 8.28437132784959e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 1.2137187719345093, 'lr': 7.931844888366628e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 1.1816669702529907, 'lr': 7.579318448883666e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 0.992797315120697, 'lr': 7.226792009400705e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 1.1354304552078247, 'lr': 6.8742655699177444e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 0.761617386713624, 'rougeL': 0.2726598924517323, 'epoch_time_seconds': 59.28798723220825, 'memory_usage_mb': 1572.57421875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 1.041146159172058, 'lr': 6.521739130434783e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 1.0692070722579956, 'lr': 6.169212690951822e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 1.1606606245040894, 'lr': 5.81668625146886e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 1.0781395435333252, 'lr': 5.464159811985899e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 0.9800068736076355, 'lr': 5.1116333725029375e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 1.0399258136749268, 'lr': 4.759106933019977e-06, 'memory_usage_mb': 1572.57421875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 0.7277833633124828, 'rougeL': 0.27455644767161524, 'epoch_time_seconds': 57.53468298912048, 'memory_usage_mb': 1572.88671875}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 1.0844603776931763, 'lr': 4.406580493537015e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 1.0522847175598145, 'lr': 4.0540540540540545e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 1.1993950605392456, 'lr': 3.7015276145710925e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 1.0511730909347534, 'lr': 3.3490011750881314e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 1.091745138168335, 'lr': 2.9964747356051703e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 0.9715118408203125, 'lr': 2.643948296122209e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 0.9549306035041809, 'lr': 2.291421856639248e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 0.7092629447579384, 'rougeL': 0.27652290144087566, 'epoch_time_seconds': 56.08072018623352, 'memory_usage_mb': 1572.88671875}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 1.0483734607696533, 'lr': 1.9388954171562864e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 1.1278592348098755, 'lr': 1.5863689776733255e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 1.0034533739089966, 'lr': 1.2338425381903644e-06, 'memory_usage_mb': 1572.88671875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 1.10233736038208, 'lr': 8.81316098707403e-07, 'memory_usage_mb': 1572.88671875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 0.9461150169372559, 'lr': 5.287896592244418e-07, 'memory_usage_mb': 1572.88671875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 0.9206129908561707, 'lr': 1.7626321974148062e-07, 'memory_usage_mb': 1572.88671875}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 0.7033573240041733, 'rougeL': 0.27795208008700667, 'epoch_time_seconds': 56.01452374458313, 'memory_usage_mb': 1572.88671875}\n",
      "Step 945: {'total_training_time_seconds': 861.9648079872131, 'avg_epoch_time_seconds': 57.462831338246666, 'best_accuracy': 0.3190520158499688, 'convergence_step': 'Not converged', 'memory_usage_mb': 1572.88671875}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 57.46283\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.31905\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 56.01452\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 0.70336\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1572.88672\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.27795\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 861.96481\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 0.92061\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mfull_ft_adamw_warmup_lr3e-05_wd0.01_samsum_20250425_190041\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/ezkcbr7i\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_190041-ezkcbr7i/logs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --full_ft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8999c",
   "metadata": {},
   "source": [
    "### Default LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687a148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_191523-odw9nsee\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.01_samsum_20250425_191522\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/odw9nsee\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0, 'memory_usage_mb': 1327.55078125}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.77180862426758, 'lr': 3.1914893617021275e-05, 'memory_usage_mb': 1394.0546875}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 36.80936813354492, 'lr': 6.382978723404255e-05, 'memory_usage_mb': 1394.9921875}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 39.396976470947266, 'lr': 9.574468085106382e-05, 'memory_usage_mb': 1395.3046875}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 36.496707916259766, 'lr': 0.0001276595744680851, 'memory_usage_mb': 1395.6171875}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 34.96847152709961, 'lr': 0.00015957446808510637, 'memory_usage_mb': 1395.9296875}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 30.884443283081055, 'lr': 0.00019148936170212765, 'memory_usage_mb': 1395.9296875}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 26.72245180606842, 'rougeL': 0.2701288314609299, 'epoch_time_seconds': 74.96640539169312, 'memory_usage_mb': 1532.0234375}\n",
      "New best accuracy: 0.2701\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 23.982980728149414, 'lr': 0.0002234042553191489, 'memory_usage_mb': 1532.0234375}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 19.47930908203125, 'lr': 0.0002553191489361702, 'memory_usage_mb': 1532.0234375}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 13.944292068481445, 'lr': 0.0002872340425531915, 'memory_usage_mb': 1532.0234375}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 8.665853500366211, 'lr': 0.00029788484136310223, 'memory_usage_mb': 1532.0234375}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 7.545267581939697, 'lr': 0.0002943595769682726, 'memory_usage_mb': 1532.0234375}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 6.468618392944336, 'lr': 0.000290834312573443, 'memory_usage_mb': 1532.0234375}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 5.017876476049423, 'rougeL': 0.1684053070332987, 'epoch_time_seconds': 65.52253651618958, 'memory_usage_mb': 1532.3359375}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 5.623924732208252, 'lr': 0.00028730904817861336, 'memory_usage_mb': 1532.6484375}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 5.08833646774292, 'lr': 0.00028378378378378377, 'memory_usage_mb': 1532.6484375}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.904491424560547, 'lr': 0.0002802585193889541, 'memory_usage_mb': 1532.6484375}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 4.70645809173584, 'lr': 0.00027673325499412454, 'memory_usage_mb': 1532.6484375}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 4.6221137046813965, 'lr': 0.00027320799059929495, 'memory_usage_mb': 1532.6484375}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.477416515350342, 'lr': 0.0002696827262044653, 'memory_usage_mb': 1532.6484375}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.331884041428566, 'rougeL': 0.2537884780698574, 'epoch_time_seconds': 64.90180349349976, 'memory_usage_mb': 1532.6484375}\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.592334270477295, 'lr': 0.00026615746180963566, 'memory_usage_mb': 1532.6484375}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.422481536865234, 'lr': 0.0002626321974148061, 'memory_usage_mb': 1532.6484375}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.412388801574707, 'lr': 0.0002591069330199765, 'memory_usage_mb': 1532.6484375}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.360157489776611, 'lr': 0.00025558166862514684, 'memory_usage_mb': 1532.6484375}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 4.31210994720459, 'lr': 0.00025205640423031726, 'memory_usage_mb': 1532.6484375}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.360479354858398, 'lr': 0.00024853113983548767, 'memory_usage_mb': 1532.6484375}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 4.244153022766113, 'lr': 0.000245005875440658, 'memory_usage_mb': 1532.6484375}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 4.051990769803524, 'rougeL': 0.25772244763605756, 'epoch_time_seconds': 84.60125160217285, 'memory_usage_mb': 1532.9609375}\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.244002342224121, 'lr': 0.00024148061104582844, 'memory_usage_mb': 1532.9609375}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 4.089375019073486, 'lr': 0.0002379553466509988, 'memory_usage_mb': 1532.9609375}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 4.21914005279541, 'lr': 0.00023443008225616918, 'memory_usage_mb': 1532.9609375}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 4.202630519866943, 'lr': 0.00023090481786133956, 'memory_usage_mb': 1532.9609375}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 4.096890449523926, 'lr': 0.00022737955346650997, 'memory_usage_mb': 1532.9609375}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 4.094822406768799, 'lr': 0.00022385428907168036, 'memory_usage_mb': 1532.9609375}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.8239381089806557, 'rougeL': 0.2641814259414279, 'epoch_time_seconds': 74.5143632888794, 'memory_usage_mb': 1532.9609375}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.9383022785186768, 'lr': 0.00022032902467685074, 'memory_usage_mb': 1532.9609375}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 4.013800144195557, 'lr': 0.00021680376028202115, 'memory_usage_mb': 1532.9609375}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.9790282249450684, 'lr': 0.00021327849588719154, 'memory_usage_mb': 1532.9609375}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.9889841079711914, 'lr': 0.0002097532314923619, 'memory_usage_mb': 1532.9609375}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.830626964569092, 'lr': 0.00020622796709753228, 'memory_usage_mb': 1532.9609375}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.805612087249756, 'lr': 0.0002027027027027027, 'memory_usage_mb': 1532.9609375}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.6033721417188644, 'rougeL': 0.2811206499173208, 'epoch_time_seconds': 68.36313128471375, 'memory_usage_mb': 1532.9609375}\n",
      "New best accuracy: 0.2811\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.8383095264434814, 'lr': 0.00019917743830787308, 'memory_usage_mb': 1532.9609375}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.8260629177093506, 'lr': 0.00019565217391304346, 'memory_usage_mb': 1532.9609375}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.680307388305664, 'lr': 0.00019212690951821385, 'memory_usage_mb': 1532.9609375}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.660594940185547, 'lr': 0.00018860164512338426, 'memory_usage_mb': 1532.9609375}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.5705931186676025, 'lr': 0.00018507638072855464, 'memory_usage_mb': 1532.9609375}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.6798672676086426, 'lr': 0.000181551116333725, 'memory_usage_mb': 1532.9609375}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.6407082080841064, 'lr': 0.00017802585193889538, 'memory_usage_mb': 1532.9609375}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 3.37519334256649, 'rougeL': 0.2830618436500855, 'epoch_time_seconds': 56.364694595336914, 'memory_usage_mb': 1532.9609375}\n",
      "New best accuracy: 0.2831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.597119092941284, 'lr': 0.0001745005875440658, 'memory_usage_mb': 1532.9609375}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.5575599670410156, 'lr': 0.00017097532314923618, 'memory_usage_mb': 1532.9609375}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.558748245239258, 'lr': 0.00016745005875440656, 'memory_usage_mb': 1532.9609375}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.4376649856567383, 'lr': 0.00016392479435957695, 'memory_usage_mb': 1532.9609375}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.4120635986328125, 'lr': 0.00016039952996474736, 'memory_usage_mb': 1532.9609375}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.487292528152466, 'lr': 0.00015687426556991774, 'memory_usage_mb': 1532.9609375}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 3.1805575266480446, 'rougeL': 0.28686116779615145, 'epoch_time_seconds': 54.17581582069397, 'memory_usage_mb': 1532.9609375}\n",
      "New best accuracy: 0.2869\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.4115936756134033, 'lr': 0.0001533490011750881, 'memory_usage_mb': 1532.9609375}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.387363910675049, 'lr': 0.0001498237367802585, 'memory_usage_mb': 1533.2734375}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.4027726650238037, 'lr': 0.0001462984723854289, 'memory_usage_mb': 1533.2734375}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 3.3577959537506104, 'lr': 0.00014277320799059928, 'memory_usage_mb': 1533.2734375}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 3.3308792114257812, 'lr': 0.00013924794359576967, 'memory_usage_mb': 1533.2734375}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 3.3349666595458984, 'lr': 0.00013572267920094008, 'memory_usage_mb': 1533.2734375}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 3.0377723574638367, 'rougeL': 0.28795424536665654, 'epoch_time_seconds': 42.67480230331421, 'memory_usage_mb': 1533.2734375}\n",
      "New best accuracy: 0.2880\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 3.3088018894195557, 'lr': 0.00013219741480611043, 'memory_usage_mb': 1533.2734375}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 3.260558843612671, 'lr': 0.00012867215041128085, 'memory_usage_mb': 1533.2734375}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 3.339526653289795, 'lr': 0.00012514688601645123, 'memory_usage_mb': 1533.2734375}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 3.2313318252563477, 'lr': 0.00012162162162162162, 'memory_usage_mb': 1533.2734375}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 3.18861722946167, 'lr': 0.000118096357226792, 'memory_usage_mb': 1533.2734375}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 3.1551873683929443, 'lr': 0.00011457109283196238, 'memory_usage_mb': 1533.2734375}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.8986993804574013, 'rougeL': 0.29347999443200457, 'epoch_time_seconds': 47.9376654624939, 'memory_usage_mb': 1533.2734375}\n",
      "New best accuracy: 0.2935\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 3.194765090942383, 'lr': 0.00011104582843713278, 'memory_usage_mb': 1533.2734375}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 3.22822642326355, 'lr': 0.00010752056404230317, 'memory_usage_mb': 1533.2734375}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 3.186267852783203, 'lr': 0.00010399529964747355, 'memory_usage_mb': 1533.2734375}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 3.219961643218994, 'lr': 0.00010047003525264394, 'memory_usage_mb': 1533.2734375}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 3.1649913787841797, 'lr': 9.694477085781433e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 3.1243538856506348, 'lr': 9.341950646298472e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 3.042372941970825, 'lr': 8.98942420681551e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.80212464928627, 'rougeL': 0.29383550234581324, 'epoch_time_seconds': 46.01128029823303, 'memory_usage_mb': 1533.2734375}\n",
      "New best accuracy: 0.2938\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 3.1563777923583984, 'lr': 8.636897767332549e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 3.109586238861084, 'lr': 8.284371327849588e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 3.065502405166626, 'lr': 7.931844888366627e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 3.090000867843628, 'lr': 7.579318448883665e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 3.142131805419922, 'lr': 7.226792009400704e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 3.0347132682800293, 'lr': 6.874265569917744e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.7494882717728615, 'rougeL': 0.2915958325722548, 'epoch_time_seconds': 47.7108678817749, 'memory_usage_mb': 1533.2734375}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 3.110464572906494, 'lr': 6.521739130434782e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 3.0980446338653564, 'lr': 6.16921269095182e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 3.0769941806793213, 'lr': 5.8166862514688596e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 3.025589942932129, 'lr': 5.464159811985899e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 3.0980496406555176, 'lr': 5.111633372502937e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 3.074812650680542, 'lr': 4.759106933019976e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.704396516084671, 'rougeL': 0.2923086047751141, 'epoch_time_seconds': 44.668235063552856, 'memory_usage_mb': 1533.2734375}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 3.085840940475464, 'lr': 4.406580493537015e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 3.048166513442993, 'lr': 4.054054054054054e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 3.0907328128814697, 'lr': 3.701527614571092e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 3.089595317840576, 'lr': 3.3490011750881314e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 3.0233304500579834, 'lr': 2.99647473560517e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.9538350105285645, 'lr': 2.6439482961222086e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 3.0079474449157715, 'lr': 2.2914218566392474e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.677124574780464, 'rougeL': 0.29084976802213325, 'epoch_time_seconds': 48.70691919326782, 'memory_usage_mb': 1533.2734375}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.9577555656433105, 'lr': 1.9388954171562862e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.9980692863464355, 'lr': 1.5863689776733253e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 3.024571180343628, 'lr': 1.2338425381903641e-05, 'memory_usage_mb': 1533.2734375}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 3.0994675159454346, 'lr': 8.81316098707403e-06, 'memory_usage_mb': 1533.2734375}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.9982011318206787, 'lr': 5.287896592244418e-06, 'memory_usage_mb': 1533.2734375}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 3.0287539958953857, 'lr': 1.7626321974148059e-06, 'memory_usage_mb': 1533.2734375}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.665367931127548, 'rougeL': 0.29092241129998164, 'epoch_time_seconds': 46.633857011795044, 'memory_usage_mb': 1533.2734375}\n",
      "Step 945: {'total_training_time_seconds': 867.7788281440735, 'avg_epoch_time_seconds': 57.85024194717407, 'best_accuracy': 0.29383550234581324, 'convergence_step': 'Not converged', 'memory_usage_mb': 1533.2734375}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñá‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 57.85024\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.29384\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 46.63386\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.66537\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1533.27344\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.29092\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 867.77883\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 3.02875\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.01_samsum_20250425_191522\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/odw9nsee\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_191523-odw9nsee/logs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4f173",
   "metadata": {},
   "source": [
    "### AdaFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c21533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_193009-x8ll7b7z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adafactor_warmup_lr0.0003_wd0.01_samsum_20250425_193009\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/x8ll7b7z\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0, 'memory_usage_mb': 1327.73046875}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.77313995361328, 'lr': 3.1914893617021275e-05, 'memory_usage_mb': 1365.890625}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 36.80743408203125, 'lr': 6.382978723404255e-05, 'memory_usage_mb': 1366.828125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 39.44239807128906, 'lr': 9.574468085106382e-05, 'memory_usage_mb': 1367.140625}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 36.899024963378906, 'lr': 0.0001276595744680851, 'memory_usage_mb': 1367.453125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 36.37972640991211, 'lr': 0.00015957446808510637, 'memory_usage_mb': 1367.453125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 33.09724044799805, 'lr': 0.00019148936170212765, 'memory_usage_mb': 1367.453125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 29.26657623052597, 'rougeL': 0.29448826837023523, 'epoch_time_seconds': 66.09053468704224, 'memory_usage_mb': 1501.59765625}\n",
      "New best accuracy: 0.2945\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 26.45534896850586, 'lr': 0.0002234042553191489, 'memory_usage_mb': 1501.59765625}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 21.491966247558594, 'lr': 0.0002553191489361702, 'memory_usage_mb': 1501.59765625}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 13.271239280700684, 'lr': 0.0002872340425531915, 'memory_usage_mb': 1501.59765625}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 9.54855728149414, 'lr': 0.00029788484136310223, 'memory_usage_mb': 1501.59765625}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 8.19016170501709, 'lr': 0.0002943595769682726, 'memory_usage_mb': 1501.59765625}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 6.762300491333008, 'lr': 0.000290834312573443, 'memory_usage_mb': 1501.59765625}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 4.770940333604813, 'rougeL': 0.19029378431028707, 'epoch_time_seconds': 52.754138231277466, 'memory_usage_mb': 1501.91015625}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 5.366670608520508, 'lr': 0.00028730904817861336, 'memory_usage_mb': 1501.91015625}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.737655162811279, 'lr': 0.00028378378378378377, 'memory_usage_mb': 1501.91015625}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.603109359741211, 'lr': 0.0002802585193889541, 'memory_usage_mb': 1501.91015625}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 4.439382076263428, 'lr': 0.00027673325499412454, 'memory_usage_mb': 1501.91015625}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 4.364803314208984, 'lr': 0.00027320799059929495, 'memory_usage_mb': 1501.91015625}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.237698078155518, 'lr': 0.0002696827262044653, 'memory_usage_mb': 1501.91015625}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.083076260983944, 'rougeL': 0.2615683936316775, 'epoch_time_seconds': 61.50890135765076, 'memory_usage_mb': 1502.22265625}\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.313138484954834, 'lr': 0.00026615746180963566, 'memory_usage_mb': 1502.22265625}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.185029029846191, 'lr': 0.0002626321974148061, 'memory_usage_mb': 1502.22265625}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.121047019958496, 'lr': 0.0002591069330199765, 'memory_usage_mb': 1502.22265625}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.080964088439941, 'lr': 0.00025558166862514684, 'memory_usage_mb': 1502.22265625}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.98063063621521, 'lr': 0.00025205640423031726, 'memory_usage_mb': 1502.22265625}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.0767502784729, 'lr': 0.00024853113983548767, 'memory_usage_mb': 1502.22265625}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.962094306945801, 'lr': 0.000245005875440658, 'memory_usage_mb': 1502.22265625}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.7783396393060684, 'rougeL': 0.2716700660771992, 'epoch_time_seconds': 75.0941653251648, 'memory_usage_mb': 1502.22265625}\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.954374074935913, 'lr': 0.00024148061104582844, 'memory_usage_mb': 1502.22265625}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.7969932556152344, 'lr': 0.0002379553466509988, 'memory_usage_mb': 1502.22265625}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.9149703979492188, 'lr': 0.00023443008225616918, 'memory_usage_mb': 1502.22265625}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.861203193664551, 'lr': 0.00023090481786133956, 'memory_usage_mb': 1502.22265625}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.7370808124542236, 'lr': 0.00022737955346650997, 'memory_usage_mb': 1502.22265625}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.7124593257904053, 'lr': 0.00022385428907168036, 'memory_usage_mb': 1502.22265625}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.405445098876953, 'rougeL': 0.28026929672343937, 'epoch_time_seconds': 75.9002137184143, 'memory_usage_mb': 1502.22265625}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.5597965717315674, 'lr': 0.00022032902467685074, 'memory_usage_mb': 1502.22265625}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.5904619693756104, 'lr': 0.00021680376028202115, 'memory_usage_mb': 1502.22265625}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.533482074737549, 'lr': 0.00021327849588719154, 'memory_usage_mb': 1502.22265625}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.5237514972686768, 'lr': 0.0002097532314923619, 'memory_usage_mb': 1502.22265625}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.3673765659332275, 'lr': 0.00020622796709753228, 'memory_usage_mb': 1502.22265625}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.33560848236084, 'lr': 0.0002027027027027027, 'memory_usage_mb': 1502.22265625}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.061294049024582, 'rougeL': 0.29077123629385515, 'epoch_time_seconds': 61.94248390197754, 'memory_usage_mb': 1502.22265625}\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.3629117012023926, 'lr': 0.00019917743830787308, 'memory_usage_mb': 1502.22265625}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.3268043994903564, 'lr': 0.00019565217391304346, 'memory_usage_mb': 1502.22265625}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.1678943634033203, 'lr': 0.00019212690951821385, 'memory_usage_mb': 1502.22265625}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.2050015926361084, 'lr': 0.00018860164512338426, 'memory_usage_mb': 1502.22265625}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.0687503814697266, 'lr': 0.00018507638072855464, 'memory_usage_mb': 1502.22265625}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.1296470165252686, 'lr': 0.000181551116333725, 'memory_usage_mb': 1502.22265625}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.090965747833252, 'lr': 0.00017802585193889538, 'memory_usage_mb': 1502.22265625}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 2.719500057399273, 'rougeL': 0.2915436513807834, 'epoch_time_seconds': 55.18474626541138, 'memory_usage_mb': 1502.22265625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.070195198059082, 'lr': 0.0001745005875440658, 'memory_usage_mb': 1502.22265625}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.0515193939208984, 'lr': 0.00017097532314923618, 'memory_usage_mb': 1502.22265625}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.9960336685180664, 'lr': 0.00016745005875440656, 'memory_usage_mb': 1502.22265625}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.885329484939575, 'lr': 0.00016392479435957695, 'memory_usage_mb': 1502.22265625}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.883204698562622, 'lr': 0.00016039952996474736, 'memory_usage_mb': 1502.22265625}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.9156124591827393, 'lr': 0.00015687426556991774, 'memory_usage_mb': 1502.22265625}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.545763447880745, 'rougeL': 0.28707380371298596, 'epoch_time_seconds': 48.79626393318176, 'memory_usage_mb': 1502.53515625}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.871922731399536, 'lr': 0.0001533490011750881, 'memory_usage_mb': 1502.53515625}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.8313121795654297, 'lr': 0.0001498237367802585, 'memory_usage_mb': 1502.53515625}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.8255205154418945, 'lr': 0.0001462984723854289, 'memory_usage_mb': 1502.53515625}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.8112289905548096, 'lr': 0.00014277320799059928, 'memory_usage_mb': 1502.53515625}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.8167712688446045, 'lr': 0.00013924794359576967, 'memory_usage_mb': 1502.53515625}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.7788755893707275, 'lr': 0.00013572267920094008, 'memory_usage_mb': 1502.53515625}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.417126417160034, 'rougeL': 0.28852171208943445, 'epoch_time_seconds': 57.621092319488525, 'memory_usage_mb': 1505.34765625}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.739116668701172, 'lr': 0.00013219741480611043, 'memory_usage_mb': 1505.34765625}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.731288433074951, 'lr': 0.00012867215041128085, 'memory_usage_mb': 1505.34765625}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.7848167419433594, 'lr': 0.00012514688601645123, 'memory_usage_mb': 1505.34765625}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.6817307472229004, 'lr': 0.00012162162162162162, 'memory_usage_mb': 1505.34765625}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.670051097869873, 'lr': 0.000118096357226792, 'memory_usage_mb': 1505.34765625}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.6275999546051025, 'lr': 0.00011457109283196238, 'memory_usage_mb': 1505.34765625}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.3310331404209137, 'rougeL': 0.2885154184099638, 'epoch_time_seconds': 45.66163492202759, 'memory_usage_mb': 1522.53515625}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.673004627227783, 'lr': 0.00011104582843713278, 'memory_usage_mb': 1522.53515625}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.6752443313598633, 'lr': 0.00010752056404230317, 'memory_usage_mb': 1523.47265625}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.6870808601379395, 'lr': 0.00010399529964747355, 'memory_usage_mb': 1523.47265625}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.703685998916626, 'lr': 0.00010047003525264394, 'memory_usage_mb': 1523.47265625}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.63535213470459, 'lr': 9.694477085781433e-05, 'memory_usage_mb': 1523.47265625}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.575791358947754, 'lr': 9.341950646298472e-05, 'memory_usage_mb': 1528.16015625}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.548218011856079, 'lr': 8.98942420681551e-05, 'memory_usage_mb': 1528.16015625}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.266993761062622, 'rougeL': 0.2886647336856397, 'epoch_time_seconds': 41.35298752784729, 'memory_usage_mb': 1538.16015625}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.651292085647583, 'lr': 8.636897767332549e-05, 'memory_usage_mb': 1550.97265625}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.5858519077301025, 'lr': 8.284371327849588e-05, 'memory_usage_mb': 1550.97265625}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.561821937561035, 'lr': 7.931844888366627e-05, 'memory_usage_mb': 1550.97265625}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.568141222000122, 'lr': 7.579318448883665e-05, 'memory_usage_mb': 1550.97265625}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.601032257080078, 'lr': 7.226792009400704e-05, 'memory_usage_mb': 1561.59765625}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.518808603286743, 'lr': 6.874265569917744e-05, 'memory_usage_mb': 1561.59765625}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.218460336327553, 'rougeL': 0.298192932994101, 'epoch_time_seconds': 47.22235298156738, 'memory_usage_mb': 1584.72265625}\n",
      "New best accuracy: 0.2982\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.5791313648223877, 'lr': 6.521739130434782e-05, 'memory_usage_mb': 1584.72265625}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.5827152729034424, 'lr': 6.16921269095182e-05, 'memory_usage_mb': 1584.72265625}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.560029983520508, 'lr': 5.8166862514688596e-05, 'memory_usage_mb': 1586.28515625}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.5347955226898193, 'lr': 5.464159811985899e-05, 'memory_usage_mb': 1586.28515625}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.5906832218170166, 'lr': 5.111633372502937e-05, 'memory_usage_mb': 1586.28515625}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.564054250717163, 'lr': 4.759106933019976e-05, 'memory_usage_mb': 1586.28515625}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.2008105516433716, 'rougeL': 0.2962333523312425, 'epoch_time_seconds': 46.34905552864075, 'memory_usage_mb': 1599.72265625}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.5507872104644775, 'lr': 4.406580493537015e-05, 'memory_usage_mb': 1599.72265625}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.5595297813415527, 'lr': 4.054054054054054e-05, 'memory_usage_mb': 1599.72265625}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.5583674907684326, 'lr': 3.701527614571092e-05, 'memory_usage_mb': 1599.72265625}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.6087071895599365, 'lr': 3.3490011750881314e-05, 'memory_usage_mb': 1599.72265625}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.5953407287597656, 'lr': 2.99647473560517e-05, 'memory_usage_mb': 1603.16015625}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.4878604412078857, 'lr': 2.6439482961222086e-05, 'memory_usage_mb': 1603.16015625}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.5256145000457764, 'lr': 2.2914218566392474e-05, 'memory_usage_mb': 1603.16015625}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.1752935722470284, 'rougeL': 0.2966213841197692, 'epoch_time_seconds': 47.44605994224548, 'memory_usage_mb': 1624.09765625}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.4843556880950928, 'lr': 1.9388954171562862e-05, 'memory_usage_mb': 1624.09765625}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.515371799468994, 'lr': 1.5863689776733253e-05, 'memory_usage_mb': 1634.41015625}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.517335891723633, 'lr': 1.2338425381903641e-05, 'memory_usage_mb': 1634.41015625}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.598872184753418, 'lr': 8.81316098707403e-06, 'memory_usage_mb': 1634.41015625}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.5022687911987305, 'lr': 5.287896592244418e-06, 'memory_usage_mb': 1634.41015625}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.528595447540283, 'lr': 1.7626321974148059e-06, 'memory_usage_mb': 1641.91015625}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.1749430298805237, 'rougeL': 0.2993577525417329, 'epoch_time_seconds': 46.238696813583374, 'memory_usage_mb': 1657.84765625}\n",
      "New best accuracy: 0.2994\n",
      "Step 945: {'total_training_time_seconds': 829.1866865158081, 'avg_epoch_time_seconds': 55.27755516370137, 'best_accuracy': 0.2993577525417329, 'convergence_step': 'Not converged', 'memory_usage_mb': 1657.84765625}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÇ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 55.27756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.29936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 46.2387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.17494\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1657.84766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.29936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 829.18669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 2.5286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adafactor_warmup_lr0.0003_wd0.01_samsum_20250425_193009\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/x8ll7b7z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_193009-x8ll7b7z/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adafactor \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b956f6",
   "metadata": {},
   "source": [
    "### Lion Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe68e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_194416-v4jl1ec8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_lion_warmup_lr0.0003_wd0.01_samsum_20250425_194416\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/v4jl1ec8\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0, 'memory_usage_mb': 1327.54296875}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.76193618774414, 'lr': 3.1914893617021275e-05, 'memory_usage_mb': 1331.60546875}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 36.74911880493164, 'lr': 6.382978723404255e-05, 'memory_usage_mb': 1332.54296875}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 39.267112731933594, 'lr': 9.574468085106382e-05, 'memory_usage_mb': 1332.85546875}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 36.549293518066406, 'lr': 0.0001276595744680851, 'memory_usage_mb': 1333.16796875}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 35.78506088256836, 'lr': 0.00015957446808510637, 'memory_usage_mb': 1333.16796875}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 32.83722686767578, 'lr': 0.00019148936170212765, 'memory_usage_mb': 1333.16796875}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 29.40084570646286, 'rougeL': 0.28099417008280214, 'epoch_time_seconds': 98.24949049949646, 'memory_usage_mb': 1469.95703125}\n",
      "New best accuracy: 0.2810\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 27.001230239868164, 'lr': 0.0002234042553191489, 'memory_usage_mb': 1469.95703125}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 23.64933967590332, 'lr': 0.0002553191489361702, 'memory_usage_mb': 1469.95703125}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 19.80647850036621, 'lr': 0.0002872340425531915, 'memory_usage_mb': 1469.95703125}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 17.016033172607422, 'lr': 0.00029788484136310223, 'memory_usage_mb': 1469.95703125}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 14.646967887878418, 'lr': 0.0002943595769682726, 'memory_usage_mb': 1469.95703125}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 11.594871520996094, 'lr': 0.000290834312573443, 'memory_usage_mb': 1469.95703125}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 7.35433603823185, 'rougeL': 0.04628277580347746, 'epoch_time_seconds': 37.77571129798889, 'memory_usage_mb': 1470.26953125}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 8.843853950500488, 'lr': 0.00028730904817861336, 'memory_usage_mb': 1470.26953125}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 7.864124298095703, 'lr': 0.00028378378378378377, 'memory_usage_mb': 1470.26953125}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 7.500796794891357, 'lr': 0.0002802585193889541, 'memory_usage_mb': 1470.26953125}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 6.915722846984863, 'lr': 0.00027673325499412454, 'memory_usage_mb': 1470.26953125}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 6.220320224761963, 'lr': 0.00027320799059929495, 'memory_usage_mb': 1470.26953125}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 5.728384971618652, 'lr': 0.0002696827262044653, 'memory_usage_mb': 1470.26953125}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 5.251355454325676, 'rougeL': 0.0551746965415679, 'epoch_time_seconds': 107.64472365379333, 'memory_usage_mb': 1470.58203125}\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 5.618020534515381, 'lr': 0.00026615746180963566, 'memory_usage_mb': 1470.58203125}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 5.602945804595947, 'lr': 0.0002626321974148061, 'memory_usage_mb': 1470.58203125}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 5.423239231109619, 'lr': 0.0002591069330199765, 'memory_usage_mb': 1470.58203125}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 5.44376802444458, 'lr': 0.00025558166862514684, 'memory_usage_mb': 1470.58203125}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 5.341902256011963, 'lr': 0.00025205640423031726, 'memory_usage_mb': 1470.58203125}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 5.2711501121521, 'lr': 0.00024853113983548767, 'memory_usage_mb': 1470.58203125}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 5.3376336097717285, 'lr': 0.000245005875440658, 'memory_usage_mb': 1470.58203125}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 5.066246181726456, 'rougeL': 0.030480652352303364, 'epoch_time_seconds': 108.28190493583679, 'memory_usage_mb': 1470.89453125}\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 5.206808090209961, 'lr': 0.00024148061104582844, 'memory_usage_mb': 1470.89453125}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 5.23574686050415, 'lr': 0.0002379553466509988, 'memory_usage_mb': 1470.89453125}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 5.081719398498535, 'lr': 0.00023443008225616918, 'memory_usage_mb': 1470.89453125}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 5.064196586608887, 'lr': 0.00023090481786133956, 'memory_usage_mb': 1470.89453125}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 4.999166011810303, 'lr': 0.00022737955346650997, 'memory_usage_mb': 1470.89453125}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 5.040241718292236, 'lr': 0.00022385428907168036, 'memory_usage_mb': 1470.89453125}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 4.824763908982277, 'rougeL': 0.0003686274509803922, 'epoch_time_seconds': 23.732447624206543, 'memory_usage_mb': 1470.89453125}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 5.045652389526367, 'lr': 0.00022032902467685074, 'memory_usage_mb': 1470.89453125}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 4.891512393951416, 'lr': 0.00021680376028202115, 'memory_usage_mb': 1470.89453125}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 4.989028453826904, 'lr': 0.00021327849588719154, 'memory_usage_mb': 1470.89453125}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 4.858231544494629, 'lr': 0.0002097532314923619, 'memory_usage_mb': 1470.89453125}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 4.7696309089660645, 'lr': 0.00020622796709753228, 'memory_usage_mb': 1470.89453125}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 4.871938705444336, 'lr': 0.0002027027027027027, 'memory_usage_mb': 1470.89453125}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 4.567590832710266, 'rougeL': 0.00011764705882352943, 'epoch_time_seconds': 22.196388959884644, 'memory_usage_mb': 1470.89453125}\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 4.7004265785217285, 'lr': 0.00019917743830787308, 'memory_usage_mb': 1470.89453125}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 4.6417412757873535, 'lr': 0.00019565217391304346, 'memory_usage_mb': 1470.89453125}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 4.58561372756958, 'lr': 0.00019212690951821385, 'memory_usage_mb': 1470.89453125}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 4.597121238708496, 'lr': 0.00018860164512338426, 'memory_usage_mb': 1470.89453125}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 4.479081153869629, 'lr': 0.00018507638072855464, 'memory_usage_mb': 1470.89453125}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 4.48171854019165, 'lr': 0.000181551116333725, 'memory_usage_mb': 1470.89453125}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 4.420103549957275, 'lr': 0.00017802585193889538, 'memory_usage_mb': 1471.20703125}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 4.208790436387062, 'rougeL': 0.0025463067909121855, 'epoch_time_seconds': 87.56635046005249, 'memory_usage_mb': 1474.33203125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 4.439030647277832, 'lr': 0.0001745005875440658, 'memory_usage_mb': 1474.33203125}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 4.385509014129639, 'lr': 0.00017097532314923618, 'memory_usage_mb': 1474.33203125}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 4.324909687042236, 'lr': 0.00016745005875440656, 'memory_usage_mb': 1474.33203125}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 4.233858585357666, 'lr': 0.00016392479435957695, 'memory_usage_mb': 1474.33203125}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 4.225745677947998, 'lr': 0.00016039952996474736, 'memory_usage_mb': 1474.33203125}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 4.209823131561279, 'lr': 0.00015687426556991774, 'memory_usage_mb': 1474.33203125}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 3.9325846657156944, 'rougeL': 0.07145295502445316, 'epoch_time_seconds': 29.637897968292236, 'memory_usage_mb': 1474.95703125}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 4.17268180847168, 'lr': 0.0001533490011750881, 'memory_usage_mb': 1474.95703125}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 4.050782203674316, 'lr': 0.0001498237367802585, 'memory_usage_mb': 1474.95703125}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 4.034584999084473, 'lr': 0.0001462984723854289, 'memory_usage_mb': 1474.95703125}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 3.913588047027588, 'lr': 0.00014277320799059928, 'memory_usage_mb': 1476.51953125}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 4.049966812133789, 'lr': 0.00013924794359576967, 'memory_usage_mb': 1476.51953125}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 3.8575196266174316, 'lr': 0.00013572267920094008, 'memory_usage_mb': 1476.51953125}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 3.6939244270324707, 'rougeL': 0.06897001190815424, 'epoch_time_seconds': 30.40425968170166, 'memory_usage_mb': 1498.08203125}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 3.896279811859131, 'lr': 0.00013219741480611043, 'memory_usage_mb': 1498.08203125}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 3.835400342941284, 'lr': 0.00012867215041128085, 'memory_usage_mb': 1498.08203125}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 3.8349316120147705, 'lr': 0.00012514688601645123, 'memory_usage_mb': 1498.08203125}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 3.737055778503418, 'lr': 0.00012162162162162162, 'memory_usage_mb': 1498.08203125}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 3.857332706451416, 'lr': 0.000118096357226792, 'memory_usage_mb': 1510.58203125}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 3.8329005241394043, 'lr': 0.00011457109283196238, 'memory_usage_mb': 1510.58203125}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 3.476768836379051, 'rougeL': 0.05789305592761227, 'epoch_time_seconds': 25.707072257995605, 'memory_usage_mb': 1517.76953125}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 3.7790637016296387, 'lr': 0.00011104582843713278, 'memory_usage_mb': 1517.76953125}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 3.5986971855163574, 'lr': 0.00010752056404230317, 'memory_usage_mb': 1517.76953125}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 3.6442456245422363, 'lr': 0.00010399529964747355, 'memory_usage_mb': 1520.58203125}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 3.7248544692993164, 'lr': 0.00010047003525264394, 'memory_usage_mb': 1520.58203125}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 3.681340217590332, 'lr': 9.694477085781433e-05, 'memory_usage_mb': 1520.58203125}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 3.565805435180664, 'lr': 9.341950646298472e-05, 'memory_usage_mb': 1520.58203125}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 3.6803760528564453, 'lr': 8.98942420681551e-05, 'memory_usage_mb': 1526.83203125}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 3.3127269968390465, 'rougeL': 0.05862516263544816, 'epoch_time_seconds': 25.627989530563354, 'memory_usage_mb': 1532.14453125}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 3.5404014587402344, 'lr': 8.636897767332549e-05, 'memory_usage_mb': 1532.14453125}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 3.5413639545440674, 'lr': 8.284371327849588e-05, 'memory_usage_mb': 1532.14453125}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 3.586599349975586, 'lr': 7.931844888366627e-05, 'memory_usage_mb': 1532.14453125}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 3.47001576423645, 'lr': 7.579318448883665e-05, 'memory_usage_mb': 1541.83203125}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 3.472832679748535, 'lr': 7.226792009400704e-05, 'memory_usage_mb': 1541.83203125}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 3.3286261558532715, 'lr': 6.874265569917744e-05, 'memory_usage_mb': 1541.83203125}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 3.203151948750019, 'rougeL': 0.06138962419035955, 'epoch_time_seconds': 26.40072274208069, 'memory_usage_mb': 1543.39453125}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 3.368236541748047, 'lr': 6.521739130434782e-05, 'memory_usage_mb': 1543.39453125}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 3.4588444232940674, 'lr': 6.16921269095182e-05, 'memory_usage_mb': 1553.08203125}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 3.4065895080566406, 'lr': 5.8166862514688596e-05, 'memory_usage_mb': 1553.08203125}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 3.409106492996216, 'lr': 5.464159811985899e-05, 'memory_usage_mb': 1553.08203125}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 3.492837905883789, 'lr': 5.111633372502937e-05, 'memory_usage_mb': 1553.08203125}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 3.301828622817993, 'lr': 4.759106933019976e-05, 'memory_usage_mb': 1554.95703125}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 3.132490500807762, 'rougeL': 0.06101373967648935, 'epoch_time_seconds': 25.949715852737427, 'memory_usage_mb': 1559.64453125}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 3.3545215129852295, 'lr': 4.406580493537015e-05, 'memory_usage_mb': 1559.64453125}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 3.5089633464813232, 'lr': 4.054054054054054e-05, 'memory_usage_mb': 1559.64453125}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 3.379405975341797, 'lr': 3.701527614571092e-05, 'memory_usage_mb': 1559.64453125}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 3.505415201187134, 'lr': 3.3490011750881314e-05, 'memory_usage_mb': 1564.64453125}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 3.640124559402466, 'lr': 2.99647473560517e-05, 'memory_usage_mb': 1564.64453125}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 3.420286178588867, 'lr': 2.6439482961222086e-05, 'memory_usage_mb': 1564.64453125}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 3.329590082168579, 'lr': 2.2914218566392474e-05, 'memory_usage_mb': 1564.64453125}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 3.0840165987610817, 'rougeL': 0.06467622087827507, 'epoch_time_seconds': 27.052937507629395, 'memory_usage_mb': 1569.33203125}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 3.379033088684082, 'lr': 1.9388954171562862e-05, 'memory_usage_mb': 1574.95703125}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 3.44232439994812, 'lr': 1.5863689776733253e-05, 'memory_usage_mb': 1574.95703125}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 3.272599697113037, 'lr': 1.2338425381903641e-05, 'memory_usage_mb': 1574.95703125}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 3.364616870880127, 'lr': 8.81316098707403e-06, 'memory_usage_mb': 1574.95703125}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 3.261054277420044, 'lr': 5.287896592244418e-06, 'memory_usage_mb': 1584.33203125}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 3.2572154998779297, 'lr': 1.7626321974148059e-06, 'memory_usage_mb': 1584.33203125}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 3.073832370340824, 'rougeL': 0.07091011611704878, 'epoch_time_seconds': 28.26486325263977, 'memory_usage_mb': 1590.89453125}\n",
      "Step 945: {'total_training_time_seconds': 704.5166552066803, 'avg_epoch_time_seconds': 46.96616508165995, 'best_accuracy': 0.28099417008280214, 'convergence_step': 'Not converged', 'memory_usage_mb': 1590.89453125}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñá‚ñÇ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 46.96617\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.28099\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 28.26486\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 3.07383\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1590.89453\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.07091\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 704.51666\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 3.25722\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_lion_warmup_lr0.0003_wd0.01_samsum_20250425_194416\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/v4jl1ec8\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_194416-v4jl1ec8/logs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer lion \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c329f83",
   "metadata": {},
   "source": [
    "### CosineAnnealing LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91db027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_195619-spanbapu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_cosine_lr0.0003_wd0.01_samsum_20250425_195619\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/spanbapu\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0003, 'memory_usage_mb': 1328.36328125}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 37.646705627441406, 'lr': 0.00029991711853990133, 'memory_usage_mb': 1394.87109375}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 28.87441635131836, 'lr': 0.0002996685657507577, 'memory_usage_mb': 1395.80859375}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 25.631675720214844, 'lr': 0.0002992546163048102, 'memory_usage_mb': 1396.12109375}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 20.628747940063477, 'lr': 0.0002986757276518519, 'memory_usage_mb': 1396.43359375}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 16.22234344482422, 'lr': 0.0002979325395137067, 'memory_usage_mb': 1396.43359375}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 11.553171157836914, 'lr': 0.00029702587317728153, 'memory_usage_mb': 1396.74609375}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 7.499348223209381, 'rougeL': 0.10847674108667686, 'epoch_time_seconds': 26.376763343811035, 'memory_usage_mb': 1532.375}\n",
      "New best accuracy: 0.1085\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 8.651894569396973, 'lr': 0.00029595673058697357, 'memory_usage_mb': 1532.375}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 7.588230609893799, 'lr': 0.0002947262932374352, 'memory_usage_mb': 1532.6875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 6.462451457977295, 'lr': 0.00029333592086792107, 'memory_usage_mb': 1532.6875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 5.47172212600708, 'lr': 0.0002917871499596587, 'memory_usage_mb': 1532.6875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 4.989096641540527, 'lr': 0.0002900816920379045, 'memory_usage_mb': 1532.6875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 4.858094692230225, 'lr': 0.00028822143178056114, 'memory_usage_mb': 1532.6875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 4.479443520307541, 'rougeL': 0.23043755049481018, 'epoch_time_seconds': 49.545228242874146, 'memory_usage_mb': 1533.3125}\n",
      "New best accuracy: 0.2304\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 4.757743835449219, 'lr': 0.0002862084249354457, 'memory_usage_mb': 1533.3125}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.649380683898926, 'lr': 0.0002840448960485118, 'memory_usage_mb': 1533.3125}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.613075256347656, 'lr': 0.0002817332360055343, 'memory_usage_mb': 1533.3125}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 4.526641368865967, 'lr': 0.0002792759993899746, 'memory_usage_mb': 1533.3125}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 4.496400833129883, 'lr': 0.00027667590165994613, 'memory_usage_mb': 1533.3125}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.369304656982422, 'lr': 0.00027393581614739923, 'memory_usage_mb': 1533.3125}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.201452478766441, 'rougeL': 0.24912044114410478, 'epoch_time_seconds': 75.30099868774414, 'memory_usage_mb': 1533.625}\n",
      "New best accuracy: 0.2491\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.490802764892578, 'lr': 0.00027105877088284136, 'memory_usage_mb': 1533.625}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.297133445739746, 'lr': 0.0002680479452491033, 'memory_usage_mb': 1533.625}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.30040168762207, 'lr': 0.00026490666646784665, 'memory_usage_mb': 1533.625}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.255722522735596, 'lr': 0.0002616384059226977, 'memory_usage_mb': 1533.625}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 4.176148891448975, 'lr': 0.0002582467753230693, 'memory_usage_mb': 1533.9375}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.228904724121094, 'lr': 0.0002547355227129109, 'memory_usage_mb': 1533.9375}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 4.146730899810791, 'lr': 0.00025110852832879726, 'memory_usage_mb': 1533.9375}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.9359873309731483, 'rougeL': 0.251952889172772, 'epoch_time_seconds': 81.23852849006653, 'memory_usage_mb': 1536.75}\n",
      "New best accuracy: 0.2520\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.121213912963867, 'lr': 0.00024736980031193277, 'memory_usage_mb': 1536.75}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.970689535140991, 'lr': 0.00024352347027881003, 'memory_usage_mb': 1536.75}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 4.091334819793701, 'lr': 0.00023957378875541792, 'memory_usage_mb': 1536.75}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 4.0558576583862305, 'lr': 0.00023552512048004425, 'memory_usage_mb': 1536.75}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.9660093784332275, 'lr': 0.0002313819395798639, 'memory_usage_mb': 1536.75}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.9492740631103516, 'lr': 0.00022714882462664303, 'memory_usage_mb': 1536.75}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.664368160068989, 'rougeL': 0.26223399342079967, 'epoch_time_seconds': 64.24388980865479, 'memory_usage_mb': 1566.4375}\n",
      "New best accuracy: 0.2622\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.7960410118103027, 'lr': 0.0002228304535770228, 'memory_usage_mb': 1566.4375}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.8511438369750977, 'lr': 0.00021843159860297442, 'memory_usage_mb': 1566.4375}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.826017141342163, 'lr': 0.00021395712081813807, 'memory_usage_mb': 1574.5625}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.827993154525757, 'lr': 0.0002094119649058735, 'memory_usage_mb': 1574.5625}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.6746506690979004, 'lr': 0.00020480115365495926, 'memory_usage_mb': 1574.5625}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.6564226150512695, 'lr': 0.00020012978240897814, 'memory_usage_mb': 1574.5625}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.4197407364845276, 'rougeL': 0.26785460987894166, 'epoch_time_seconds': 58.70610427856445, 'memory_usage_mb': 1603.9375}\n",
      "New best accuracy: 0.2679\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.6784584522247314, 'lr': 0.00019540301343552388, 'memory_usage_mb': 1603.9375}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.661304473876953, 'lr': 0.00019062607022145078, 'memory_usage_mb': 1603.9375}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.5441696643829346, 'lr': 0.00018580423170047068, 'memory_usage_mb': 1603.9375}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.5322093963623047, 'lr': 0.00018094282641947665, 'memory_usage_mb': 1603.9375}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.4542388916015625, 'lr': 0.00017604722665003956, 'memory_usage_mb': 1605.5}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.5424978733062744, 'lr': 0.0001711228424515855, 'memory_usage_mb': 1605.5}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.539907217025757, 'lr': 0.0001661751156928138, 'memory_usage_mb': 1605.5}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 3.256255328655243, 'rougeL': 0.27626973601195803, 'epoch_time_seconds': 51.00785446166992, 'memory_usage_mb': 1626.75}\n",
      "New best accuracy: 0.2763\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.4979896545410156, 'lr': 0.00016120951403796364, 'memory_usage_mb': 1626.75}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.4769511222839355, 'lr': 0.00015623152490457402, 'memory_usage_mb': 1626.75}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.471003770828247, 'lr': 0.00015124664939941457, 'memory_usage_mb': 1626.75}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.365635871887207, 'lr': 0.00014626039623928907, 'memory_usage_mb': 1631.4375}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.3365285396575928, 'lr': 0.00014127827566342863, 'memory_usage_mb': 1631.4375}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.420769453048706, 'lr': 0.00013630579334420275, 'memory_usage_mb': 1631.4375}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 3.1086250245571136, 'rougeL': 0.2867720322526579, 'epoch_time_seconds': 53.02260613441467, 'memory_usage_mb': 1661.4375}\n",
      "New best accuracy: 0.2868\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.3398263454437256, 'lr': 0.00013134844430287725, 'memory_usage_mb': 1661.4375}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.326843738555908, 'lr': 0.00012641170683714222, 'memory_usage_mb': 1661.4375}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.3432586193084717, 'lr': 0.00012150103646712153, 'memory_usage_mb': 1661.4375}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 3.307368278503418, 'lr': 0.00011662185990655284, 'memory_usage_mb': 1669.5625}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 3.295438051223755, 'lr': 0.0001117795690658018, 'memory_usage_mb': 1669.5625}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 3.3078694343566895, 'lr': 0.0001069795150933365, 'memory_usage_mb': 1669.5625}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 3.0073609054088593, 'rougeL': 0.2878579242662781, 'epoch_time_seconds': 44.49725890159607, 'memory_usage_mb': 1683.3125}\n",
      "New best accuracy: 0.2879\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 3.2697150707244873, 'lr': 0.00010222700246224735, 'memory_usage_mb': 1683.3125}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 3.2372055053710938, 'lr': 9.752728310834782e-05, 'memory_usage_mb': 1690.1875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 3.3167316913604736, 'lr': 9.288555062633256e-05, 'memory_usage_mb': 1690.1875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 3.2268645763397217, 'lr': 8.830693453040829e-05, 'memory_usage_mb': 1690.1875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 3.1754422187805176, 'lr': 8.37964945857384e-05, 'memory_usage_mb': 1690.1875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 3.145792245864868, 'lr': 7.935921521696702e-05, 'memory_usage_mb': 1690.1875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.9187297746539116, 'rougeL': 0.29133015243243277, 'epoch_time_seconds': 49.19815754890442, 'memory_usage_mb': 1714.5625}\n",
      "New best accuracy: 0.2913\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 3.214860200881958, 'lr': 7.500000000000002e-05, 'memory_usage_mb': 1714.5625}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 3.2449393272399902, 'lr': 7.072366624313169e-05, 'memory_usage_mb': 1714.5625}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 3.2058465480804443, 'lr': 6.653493966350389e-05, 'memory_usage_mb': 1720.1875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 3.2292251586914062, 'lr': 6.243844916478155e-05, 'memory_usage_mb': 1720.1875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 3.2062785625457764, 'lr': 5.8438721721815536e-05, 'memory_usage_mb': 1720.1875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 3.1630728244781494, 'lr': 5.4540177377945465e-05, 'memory_usage_mb': 1720.1875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 3.07446026802063, 'lr': 5.074712436047112e-05, 'memory_usage_mb': 1720.1875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.856429599225521, 'rougeL': 0.2898607912425938, 'epoch_time_seconds': 46.88658261299133, 'memory_usage_mb': 1739.25}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 3.2051877975463867, 'lr': 4.706375431968997e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 3.147852659225464, 'lr': 4.3494137696762955e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 3.1046111583709717, 'lr': 4.004221922552608e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 3.1374006271362305, 'lr': 3.67118135732198e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 3.1949520111083984, 'lr': 3.350660112495324e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 3.1052944660186768, 'lr': 3.0430123916561672e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.826373003423214, 'rougeL': 0.28874522390204815, 'epoch_time_seconds': 45.72681140899658, 'memory_usage_mb': 1739.25}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 3.1850838661193848, 'lr': 2.7485781720351518e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 3.158167600631714, 'lr': 2.4676828288059558e-05, 'memory_usage_mb': 1739.25}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 3.146919012069702, 'lr': 2.2006367755176655e-05, 'memory_usage_mb': 1744.25}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 3.1057567596435547, 'lr': 1.9477351210610877e-05, 'memory_usage_mb': 1744.25}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 3.1749465465545654, 'lr': 1.709257343547986e-05, 'memory_usage_mb': 1744.25}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 3.15840220451355, 'lr': 1.4854669814637143e-05, 'memory_usage_mb': 1744.25}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.811624623835087, 'rougeL': 0.29098593790119787, 'epoch_time_seconds': 45.360843658447266, 'memory_usage_mb': 1744.25}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 3.164771318435669, 'lr': 1.2766113424344814e-05, 'memory_usage_mb': 1744.25}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 3.1346423625946045, 'lr': 1.0829212299311197e-05, 'memory_usage_mb': 1744.25}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 3.1960997581481934, 'lr': 9.046106882113751e-06, 'memory_usage_mb': 1744.25}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 3.1918764114379883, 'lr': 7.418767657825675e-06, 'memory_usage_mb': 1744.25}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 3.111496686935425, 'lr': 5.948992976460037e-06, 'memory_usage_mb': 1744.25}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 3.049433469772339, 'lr': 4.638407065638322e-06, 'memory_usage_mb': 1744.25}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 3.108147382736206, 'lr': 3.4884582356788206e-06, 'memory_usage_mb': 1744.25}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.80304054915905, 'rougeL': 0.2909043158288154, 'epoch_time_seconds': 45.50532507896423, 'memory_usage_mb': 1744.875}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 3.059447765350342, 'lr': 2.5004172790890896e-06, 'memory_usage_mb': 1744.875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 3.108623743057251, 'lr': 1.6753760662307215e-06, 'memory_usage_mb': 1744.875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 3.1379518508911133, 'lr': 1.0142463387085464e-06, 'memory_usage_mb': 1744.875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 3.2092607021331787, 'lr': 5.177587018176777e-07, 'memory_usage_mb': 1744.875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 3.1192288398742676, 'lr': 1.8646181716164831e-07, 'memory_usage_mb': 1744.875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 3.13529372215271, 'lr': 2.072179633414994e-08, 'memory_usage_mb': 1744.875}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.8017757907509804, 'rougeL': 0.2912786248738549, 'epoch_time_seconds': 45.505096673965454, 'memory_usage_mb': 1744.875}\n",
      "Step 945: {'total_training_time_seconds': 782.1472218036652, 'avg_epoch_time_seconds': 52.141469955444336, 'best_accuracy': 0.29133015243243277, 'convergence_step': 'Not converged', 'memory_usage_mb': 1744.875}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 52.14147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.29133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 45.5051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.80178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1744.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.29128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 782.14722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 3.13529\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_cosine_lr0.0003_wd0.01_samsum_20250425_195619\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/spanbapu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_195619-spanbapu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule cosine \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5ffce",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4d9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_200940-mxvbmq8b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_samsum_20250425_200939\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/mxvbmq8b\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0003, 'memory_usage_mb': 1327.6875}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 37.64632797241211, 'lr': 0.0003, 'memory_usage_mb': 1394.20703125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 28.870807647705078, 'lr': 0.0003, 'memory_usage_mb': 1395.14453125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 25.61958885192871, 'lr': 0.0003, 'memory_usage_mb': 1395.45703125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 20.60686683654785, 'lr': 0.0003, 'memory_usage_mb': 1396.08203125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 16.18609619140625, 'lr': 0.0003, 'memory_usage_mb': 1396.08203125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 11.457185745239258, 'lr': 0.0003, 'memory_usage_mb': 1396.08203125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 7.486599639058113, 'rougeL': 0.10852145413992983, 'epoch_time_seconds': 26.33938694000244, 'memory_usage_mb': 1533.66796875}\n",
      "New best accuracy: 0.1085\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 8.625401496887207, 'lr': 0.0003, 'memory_usage_mb': 1533.98046875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 7.54392147064209, 'lr': 0.0003, 'memory_usage_mb': 1533.98046875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 6.413055896759033, 'lr': 0.0003, 'memory_usage_mb': 1533.98046875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 5.427874565124512, 'lr': 0.0003, 'memory_usage_mb': 1533.98046875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 4.956932544708252, 'lr': 0.0003, 'memory_usage_mb': 1533.98046875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 4.836333274841309, 'lr': 0.0003, 'memory_usage_mb': 1533.98046875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 4.471572354435921, 'rougeL': 0.2324372897155914, 'epoch_time_seconds': 54.213618516922, 'memory_usage_mb': 1534.60546875}\n",
      "New best accuracy: 0.2324\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 4.742351531982422, 'lr': 0.0003, 'memory_usage_mb': 1534.60546875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.63507080078125, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.595230579376221, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 4.510801315307617, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 4.4730544090271, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.3418049812316895, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.1639465391635895, 'rougeL': 0.24763484259013666, 'epoch_time_seconds': 80.02927041053772, 'memory_usage_mb': 1534.91796875}\n",
      "New best accuracy: 0.2476\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.446924209594727, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.246571063995361, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.237246513366699, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.1782612800598145, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 4.125941753387451, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.164710521697998, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 4.06135368347168, 'lr': 0.0003, 'memory_usage_mb': 1534.91796875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.8677202239632607, 'rougeL': 0.25625775639564474, 'epoch_time_seconds': 85.7011890411377, 'memory_usage_mb': 1535.23046875}\n",
      "New best accuracy: 0.2563\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.043960094451904, 'lr': 0.0003, 'memory_usage_mb': 1535.23046875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.8931634426116943, 'lr': 0.0003, 'memory_usage_mb': 1535.23046875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.9922709465026855, 'lr': 0.0003, 'memory_usage_mb': 1535.23046875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.948561429977417, 'lr': 0.0003, 'memory_usage_mb': 1535.23046875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.8664119243621826, 'lr': 0.0003, 'memory_usage_mb': 1535.23046875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.8350343704223633, 'lr': 0.0003, 'memory_usage_mb': 1535.23046875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.5313548371195793, 'rougeL': 0.2731208697170299, 'epoch_time_seconds': 61.8367486000061, 'memory_usage_mb': 1535.23046875}\n",
      "New best accuracy: 0.2731\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.6791090965270996, 'lr': 0.0003, 'memory_usage_mb': 1535.23046875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.7128453254699707, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.6799001693725586, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.6851255893707275, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.5286591053009033, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.5068371295928955, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.2501608952879906, 'rougeL': 0.277661282798527, 'epoch_time_seconds': 53.08768105506897, 'memory_usage_mb': 1535.54296875}\n",
      "New best accuracy: 0.2777\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.5173518657684326, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.482165575027466, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.3491673469543457, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.353917360305786, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.249485492706299, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.3345422744750977, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.318725347518921, 'lr': 0.0003, 'memory_usage_mb': 1535.54296875}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 3.010710656642914, 'rougeL': 0.2908982836720264, 'epoch_time_seconds': 45.24360394477844, 'memory_usage_mb': 1541.16796875}\n",
      "New best accuracy: 0.2909\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.2751007080078125, 'lr': 0.0003, 'memory_usage_mb': 1541.16796875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.2293541431427, 'lr': 0.0003, 'memory_usage_mb': 1541.16796875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.194256544113159, 'lr': 0.0003, 'memory_usage_mb': 1548.66796875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.094001054763794, 'lr': 0.0003, 'memory_usage_mb': 1548.66796875}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.0731868743896484, 'lr': 0.0003, 'memory_usage_mb': 1548.66796875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.128371477127075, 'lr': 0.0003, 'memory_usage_mb': 1548.66796875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.7620585039258003, 'rougeL': 0.2937801651586734, 'epoch_time_seconds': 61.26832127571106, 'memory_usage_mb': 1577.41796875}\n",
      "New best accuracy: 0.2938\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.0276377201080322, 'lr': 0.0003, 'memory_usage_mb': 1577.41796875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.0204708576202393, 'lr': 0.0003, 'memory_usage_mb': 1577.41796875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.032094717025757, 'lr': 0.0003, 'memory_usage_mb': 1590.23046875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.984502077102661, 'lr': 0.0003, 'memory_usage_mb': 1590.23046875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.9850823879241943, 'lr': 0.0003, 'memory_usage_mb': 1590.23046875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.942432403564453, 'lr': 0.0003, 'memory_usage_mb': 1590.23046875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.5903896391391754, 'rougeL': 0.2897302688885677, 'epoch_time_seconds': 47.33792495727539, 'memory_usage_mb': 1619.29296875}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.904409170150757, 'lr': 0.0003, 'memory_usage_mb': 1619.29296875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.879695177078247, 'lr': 0.0003, 'memory_usage_mb': 1619.29296875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.9508657455444336, 'lr': 0.0003, 'memory_usage_mb': 1619.29296875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.8516664505004883, 'lr': 0.0003, 'memory_usage_mb': 1619.29296875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.7939882278442383, 'lr': 0.0003, 'memory_usage_mb': 1619.29296875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.7505569458007812, 'lr': 0.0003, 'memory_usage_mb': 1619.29296875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.461208552122116, 'rougeL': 0.2896400015155912, 'epoch_time_seconds': 51.174450397491455, 'memory_usage_mb': 1633.66796875}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.7921698093414307, 'lr': 0.0003, 'memory_usage_mb': 1633.66796875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.822604179382324, 'lr': 0.0003, 'memory_usage_mb': 1633.66796875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.7891666889190674, 'lr': 0.0003, 'memory_usage_mb': 1633.66796875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.797031879425049, 'lr': 0.0003, 'memory_usage_mb': 1633.66796875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.7323460578918457, 'lr': 0.0003, 'memory_usage_mb': 1633.66796875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.676342010498047, 'lr': 0.0003, 'memory_usage_mb': 1638.98046875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.6337578296661377, 'lr': 0.0003, 'memory_usage_mb': 1638.98046875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.3660067170858383, 'rougeL': 0.28560257900733454, 'epoch_time_seconds': 52.10489749908447, 'memory_usage_mb': 1671.79296875}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.736621856689453, 'lr': 0.00015, 'memory_usage_mb': 1671.79296875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.6642613410949707, 'lr': 0.00015, 'memory_usage_mb': 1671.79296875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.64775013923645, 'lr': 0.00015, 'memory_usage_mb': 1671.79296875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.6680397987365723, 'lr': 0.00015, 'memory_usage_mb': 1671.79296875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.7023799419403076, 'lr': 0.00015, 'memory_usage_mb': 1676.79296875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.609682321548462, 'lr': 0.00015, 'memory_usage_mb': 1676.79296875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.314431220293045, 'rougeL': 0.2863935624880315, 'epoch_time_seconds': 49.41326928138733, 'memory_usage_mb': 1692.73046875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.6701014041900635, 'lr': 0.00015, 'memory_usage_mb': 1701.16796875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.6695258617401123, 'lr': 0.00015, 'memory_usage_mb': 1701.16796875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.648206949234009, 'lr': 0.00015, 'memory_usage_mb': 1701.16796875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.6030454635620117, 'lr': 0.00015, 'memory_usage_mb': 1701.16796875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.685669422149658, 'lr': 0.00015, 'memory_usage_mb': 1701.16796875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.645556688308716, 'lr': 0.00015, 'memory_usage_mb': 1707.10546875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.282509133219719, 'rougeL': 0.287075961615741, 'epoch_time_seconds': 51.061325788497925, 'memory_usage_mb': 1723.04296875}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.638697624206543, 'lr': 0.00015, 'memory_usage_mb': 1723.04296875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.62919545173645, 'lr': 0.00015, 'memory_usage_mb': 1728.98046875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.640045642852783, 'lr': 0.00015, 'memory_usage_mb': 1728.98046875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.6689743995666504, 'lr': 0.00015, 'memory_usage_mb': 1728.98046875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.652708053588867, 'lr': 0.00015, 'memory_usage_mb': 1728.98046875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.532201051712036, 'lr': 0.00015, 'memory_usage_mb': 1738.35546875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.5667011737823486, 'lr': 0.00015, 'memory_usage_mb': 1738.35546875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.2335950434207916, 'rougeL': 0.28841782637476066, 'epoch_time_seconds': 50.82592248916626, 'memory_usage_mb': 1741.79296875}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.5461266040802, 'lr': 7.5e-05, 'memory_usage_mb': 1741.79296875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.5811381340026855, 'lr': 7.5e-05, 'memory_usage_mb': 1741.79296875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.5783512592315674, 'lr': 7.5e-05, 'memory_usage_mb': 1741.79296875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.654680013656616, 'lr': 7.5e-05, 'memory_usage_mb': 1741.79296875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.544262647628784, 'lr': 7.5e-05, 'memory_usage_mb': 1741.79296875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.5722267627716064, 'lr': 7.5e-05, 'memory_usage_mb': 1741.79296875}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.2335100919008255, 'rougeL': 0.28817312230597025, 'epoch_time_seconds': 47.192999839782715, 'memory_usage_mb': 1746.79296875}\n",
      "Step 945: {'total_training_time_seconds': 816.8559155464172, 'avg_epoch_time_seconds': 54.455374002456665, 'best_accuracy': 0.2937801651586734, 'convergence_step': 'Not converged', 'memory_usage_mb': 1746.79296875}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 54.45537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.29378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 47.193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.23351\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1746.79297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.28817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 816.85592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 2.57223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_samsum_20250425_200939\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/mxvbmq8b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_200940-mxvbmq8b/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule plateau \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06f50d",
   "metadata": {},
   "source": [
    "### Lower LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6ee87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_202335-abu3jfyk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0001_wd0.01_samsum_20250425_202335\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/abu3jfyk\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0, 'memory_usage_mb': 1331.609375}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.82876968383789, 'lr': 1.0638297872340426e-05, 'memory_usage_mb': 1398.11328125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 37.09670639038086, 'lr': 2.1276595744680852e-05, 'memory_usage_mb': 1399.05078125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 40.39081573486328, 'lr': 3.191489361702128e-05, 'memory_usage_mb': 1399.67578125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 39.13740921020508, 'lr': 4.2553191489361704e-05, 'memory_usage_mb': 1399.98828125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 40.92796325683594, 'lr': 5.319148936170213e-05, 'memory_usage_mb': 1399.98828125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 39.930137634277344, 'lr': 6.382978723404256e-05, 'memory_usage_mb': 1399.98828125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 38.52591788768768, 'rougeL': 0.3453898327097608, 'epoch_time_seconds': 59.64350414276123, 'memory_usage_mb': 1536.52734375}\n",
      "New best accuracy: 0.3454\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 36.093502044677734, 'lr': 7.446808510638298e-05, 'memory_usage_mb': 1536.52734375}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 34.236328125, 'lr': 8.510638297872341e-05, 'memory_usage_mb': 1536.52734375}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 29.684707641601562, 'lr': 9.574468085106384e-05, 'memory_usage_mb': 1536.52734375}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 26.795713424682617, 'lr': 9.929494712103408e-05, 'memory_usage_mb': 1536.52734375}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 24.499008178710938, 'lr': 9.81198589894242e-05, 'memory_usage_mb': 1536.52734375}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 23.165515899658203, 'lr': 9.694477085781433e-05, 'memory_usage_mb': 1536.52734375}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 19.767067193984985, 'rougeL': 0.20420564370878935, 'epoch_time_seconds': 59.23121428489685, 'memory_usage_mb': 1537.15234375}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 20.16053581237793, 'lr': 9.576968272620447e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 18.16990852355957, 'lr': 9.45945945945946e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 16.00408172607422, 'lr': 9.341950646298473e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 12.779500007629395, 'lr': 9.224441833137486e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 10.041377067565918, 'lr': 9.1069330199765e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 9.083600044250488, 'lr': 8.989424206815512e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 7.598087593913078, 'rougeL': 0.10912938329863114, 'epoch_time_seconds': 24.263953924179077, 'memory_usage_mb': 1537.15234375}\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 9.18605899810791, 'lr': 8.871915393654523e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 8.045638084411621, 'lr': 8.754406580493537e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 7.837947845458984, 'lr': 8.63689776733255e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 7.224713325500488, 'lr': 8.519388954171563e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 7.073348045349121, 'lr': 8.401880141010577e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 6.543755054473877, 'lr': 8.28437132784959e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 5.998758316040039, 'lr': 8.166862514688603e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 5.08975176513195, 'rougeL': 0.19223281068898876, 'epoch_time_seconds': 48.36639738082886, 'memory_usage_mb': 1537.15234375}\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 5.7645087242126465, 'lr': 8.049353701527615e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 5.37064790725708, 'lr': 7.931844888366627e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 5.358338356018066, 'lr': 7.81433607520564e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 5.299792289733887, 'lr': 7.696827262044653e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 4.929295539855957, 'lr': 7.579318448883667e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 4.958721160888672, 'lr': 7.46180963572268e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 4.5195134580135345, 'rougeL': 0.21905142713500164, 'epoch_time_seconds': 30.995487928390503, 'memory_usage_mb': 1537.15234375}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 4.775906562805176, 'lr': 7.344300822561693e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 4.805356025695801, 'lr': 7.226792009400705e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 4.7789411544799805, 'lr': 7.109283196239718e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 4.740774631500244, 'lr': 6.99177438307873e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 4.6101975440979, 'lr': 6.874265569917744e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 4.62898588180542, 'lr': 6.756756756756757e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 4.335681647062302, 'rougeL': 0.2461396129670816, 'epoch_time_seconds': 42.42322373390198, 'memory_usage_mb': 1537.15234375}\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 4.616003513336182, 'lr': 6.63924794359577e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 4.685981273651123, 'lr': 6.521739130434783e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 4.421697616577148, 'lr': 6.404230317273797e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 4.42081880569458, 'lr': 6.286721504112809e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 4.357646465301514, 'lr': 6.169212690951822e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 4.446883678436279, 'lr': 6.0517038777908344e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 4.44827938079834, 'lr': 5.934195064629847e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 4.180600017309189, 'rougeL': 0.25067212721472476, 'epoch_time_seconds': 46.347570180892944, 'memory_usage_mb': 1537.15234375}\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 4.423025608062744, 'lr': 5.81668625146886e-05, 'memory_usage_mb': 1537.15234375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 4.384660720825195, 'lr': 5.6991774383078735e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 4.397501468658447, 'lr': 5.581668625146886e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 4.252716541290283, 'lr': 5.4641598119858994e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 4.2519378662109375, 'lr': 5.346650998824913e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 4.288766384124756, 'lr': 5.229142185663925e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 4.111056052148342, 'rougeL': 0.25817644556403174, 'epoch_time_seconds': 48.13539242744446, 'memory_usage_mb': 1537.15234375}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 4.3049774169921875, 'lr': 5.111633372502937e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 4.268069267272949, 'lr': 4.994124559341951e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 4.272369861602783, 'lr': 4.876615746180964e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 4.263186931610107, 'lr': 4.759106933019977e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 4.197707176208496, 'lr': 4.6415981198589895e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 4.2671966552734375, 'lr': 4.524089306698003e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 4.05973993241787, 'rougeL': 0.267238472303429, 'epoch_time_seconds': 54.057212829589844, 'memory_usage_mb': 1537.15234375}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 4.1904616355896, 'lr': 4.4065804935370154e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 4.23789644241333, 'lr': 4.289071680376029e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 4.271538734436035, 'lr': 4.171562867215041e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 4.217520236968994, 'lr': 4.0540540540540545e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 4.106788158416748, 'lr': 3.936545240893067e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 4.106539726257324, 'lr': 3.81903642773208e-05, 'memory_usage_mb': 1537.15234375}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 4.015036970376968, 'rougeL': 0.2669321588261054, 'epoch_time_seconds': 58.20592904090881, 'memory_usage_mb': 1537.46484375}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 4.156611442565918, 'lr': 3.701527614571093e-05, 'memory_usage_mb': 1537.46484375}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 4.215607166290283, 'lr': 3.584018801410106e-05, 'memory_usage_mb': 1537.46484375}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 4.161373138427734, 'lr': 3.466509988249119e-05, 'memory_usage_mb': 1537.46484375}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 4.125048637390137, 'lr': 3.3490011750881314e-05, 'memory_usage_mb': 1537.46484375}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 4.122390270233154, 'lr': 3.231492361927145e-05, 'memory_usage_mb': 1537.46484375}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 4.170472145080566, 'lr': 3.113983548766158e-05, 'memory_usage_mb': 1537.46484375}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 4.010736465454102, 'lr': 2.9964747356051702e-05, 'memory_usage_mb': 1537.46484375}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 3.977345421910286, 'rougeL': 0.2709214106634202, 'epoch_time_seconds': 62.341893434524536, 'memory_usage_mb': 1551.21484375}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 4.211092472076416, 'lr': 2.8789659224441835e-05, 'memory_usage_mb': 1551.21484375}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 4.111722469329834, 'lr': 2.7614571092831964e-05, 'memory_usage_mb': 1551.21484375}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 4.0570068359375, 'lr': 2.6439482961222096e-05, 'memory_usage_mb': 1557.15234375}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 4.146020412445068, 'lr': 2.526439482961222e-05, 'memory_usage_mb': 1557.15234375}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 4.159116268157959, 'lr': 2.408930669800235e-05, 'memory_usage_mb': 1557.15234375}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 4.118874549865723, 'lr': 2.291421856639248e-05, 'memory_usage_mb': 1557.15234375}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 3.946027010679245, 'rougeL': 0.27054776154528043, 'epoch_time_seconds': 65.16037106513977, 'memory_usage_mb': 1587.15234375}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 4.208311080932617, 'lr': 2.173913043478261e-05, 'memory_usage_mb': 1587.15234375}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 4.144192695617676, 'lr': 2.056404230317274e-05, 'memory_usage_mb': 1596.83984375}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 4.121732711791992, 'lr': 1.938895417156287e-05, 'memory_usage_mb': 1596.83984375}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 4.067028045654297, 'lr': 1.8213866039952998e-05, 'memory_usage_mb': 1596.83984375}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 4.101320743560791, 'lr': 1.7038777908343127e-05, 'memory_usage_mb': 1596.83984375}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 4.154778957366943, 'lr': 1.5863689776733257e-05, 'memory_usage_mb': 1607.15234375}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 3.923316814005375, 'rougeL': 0.27282689631222184, 'epoch_time_seconds': 65.33779883384705, 'memory_usage_mb': 1621.83984375}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 4.157018661499023, 'lr': 1.4688601645123384e-05, 'memory_usage_mb': 1621.83984375}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 4.112420082092285, 'lr': 1.3513513513513515e-05, 'memory_usage_mb': 1621.83984375}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 4.1587042808532715, 'lr': 1.2338425381903643e-05, 'memory_usage_mb': 1621.83984375}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 4.1302289962768555, 'lr': 1.1163337250293772e-05, 'memory_usage_mb': 1621.83984375}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 3.905064105987549, 'lr': 9.988249118683901e-06, 'memory_usage_mb': 1626.83984375}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 3.9582417011260986, 'lr': 8.81316098707403e-06, 'memory_usage_mb': 1626.83984375}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 4.093257427215576, 'lr': 7.63807285546416e-06, 'memory_usage_mb': 1626.83984375}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 3.9094073548913, 'rougeL': 0.2738025096282408, 'epoch_time_seconds': 63.09809446334839, 'memory_usage_mb': 1661.52734375}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 4.01718807220459, 'lr': 6.462984723854289e-06, 'memory_usage_mb': 1661.52734375}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 4.035137176513672, 'lr': 5.287896592244418e-06, 'memory_usage_mb': 1661.52734375}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 4.102112293243408, 'lr': 4.1128084606345476e-06, 'memory_usage_mb': 1669.65234375}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 4.148045539855957, 'lr': 2.937720329024677e-06, 'memory_usage_mb': 1669.65234375}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 4.08534574508667, 'lr': 1.7626321974148063e-06, 'memory_usage_mb': 1669.65234375}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 4.120194911956787, 'lr': 5.875440658049354e-07, 'memory_usage_mb': 1669.65234375}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 3.9045123234391212, 'rougeL': 0.2759842064331537, 'epoch_time_seconds': 63.197479009628296, 'memory_usage_mb': 1699.02734375}\n",
      "Step 945: {'total_training_time_seconds': 790.8280980587006, 'avg_epoch_time_seconds': 52.720368178685504, 'best_accuracy': 0.3453898327097608, 'convergence_step': 'Not converged', 'memory_usage_mb': 1699.02734375}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñá‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 52.72037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.34539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 63.19748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 3.90451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1699.02734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.27598\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 790.8281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 4.12019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.0001_wd0.01_samsum_20250425_202335\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/abu3jfyk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_202335-abu3jfyk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8906f8",
   "metadata": {},
   "source": [
    "### Higher LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312ae686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_203705-97kgedde\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.001_wd0.01_samsum_20250425_203705\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/97kgedde\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0, 'memory_usage_mb': 1329.46484375}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.547149658203125, 'lr': 0.00010638297872340425, 'memory_usage_mb': 1395.98046875}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 35.28411102294922, 'lr': 0.0002127659574468085, 'memory_usage_mb': 1396.91796875}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 33.30088424682617, 'lr': 0.0003191489361702128, 'memory_usage_mb': 1397.54296875}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 24.83870506286621, 'lr': 0.000425531914893617, 'memory_usage_mb': 1397.85546875}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 17.332439422607422, 'lr': 0.0005319148936170213, 'memory_usage_mb': 1397.85546875}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 9.689188003540039, 'lr': 0.0006382978723404256, 'memory_usage_mb': 1397.85546875}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 7.307808414101601, 'rougeL': 0.10077963964082293, 'epoch_time_seconds': 30.482124090194702, 'memory_usage_mb': 1533.53515625}\n",
      "New best accuracy: 0.1008\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 7.162008285522461, 'lr': 0.0007446808510638298, 'memory_usage_mb': 1533.84765625}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 5.250532150268555, 'lr': 0.000851063829787234, 'memory_usage_mb': 1533.84765625}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 4.783263206481934, 'lr': 0.0009574468085106384, 'memory_usage_mb': 1533.84765625}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 4.57621955871582, 'lr': 0.0009929494712103408, 'memory_usage_mb': 1533.84765625}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 4.359030723571777, 'lr': 0.0009811985898942421, 'memory_usage_mb': 1533.84765625}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 4.323891639709473, 'lr': 0.0009694477085781433, 'memory_usage_mb': 1533.84765625}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 4.076440505683422, 'rougeL': 0.26324699690696945, 'epoch_time_seconds': 65.91197538375854, 'memory_usage_mb': 1534.78515625}\n",
      "New best accuracy: 0.2632\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 4.224111557006836, 'lr': 0.0009576968272620446, 'memory_usage_mb': 1535.09765625}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.1317138671875, 'lr': 0.0009459459459459459, 'memory_usage_mb': 1535.09765625}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.048201560974121, 'lr': 0.0009341950646298472, 'memory_usage_mb': 1535.09765625}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 3.9249753952026367, 'lr': 0.0009224441833137485, 'memory_usage_mb': 1535.09765625}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 3.8404297828674316, 'lr': 0.0009106933019976498, 'memory_usage_mb': 1535.09765625}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 3.6878952980041504, 'lr': 0.0008989424206815512, 'memory_usage_mb': 1535.09765625}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 3.3989836797118187, 'rougeL': 0.27753497748389605, 'epoch_time_seconds': 50.51408290863037, 'memory_usage_mb': 1535.09765625}\n",
      "New best accuracy: 0.2775\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 3.6746280193328857, 'lr': 0.0008871915393654524, 'memory_usage_mb': 1535.09765625}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 3.535310983657837, 'lr': 0.0008754406580493537, 'memory_usage_mb': 1535.09765625}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 3.4625558853149414, 'lr': 0.000863689776733255, 'memory_usage_mb': 1535.09765625}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 3.35404896736145, 'lr': 0.0008519388954171563, 'memory_usage_mb': 1535.09765625}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 3.2698912620544434, 'lr': 0.0008401880141010576, 'memory_usage_mb': 1535.09765625}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 3.330042839050293, 'lr': 0.0008284371327849589, 'memory_usage_mb': 1538.22265625}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 3.232581853866577, 'lr': 0.0008166862514688602, 'memory_usage_mb': 1538.22265625}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 2.8999063447117805, 'rougeL': 0.2866492214663491, 'epoch_time_seconds': 58.24949789047241, 'memory_usage_mb': 1559.16015625}\n",
      "New best accuracy: 0.2866\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 3.168236255645752, 'lr': 0.0008049353701527615, 'memory_usage_mb': 1559.16015625}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.0727663040161133, 'lr': 0.0007931844888366627, 'memory_usage_mb': 1561.66015625}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.0857787132263184, 'lr': 0.000781433607520564, 'memory_usage_mb': 1561.66015625}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.0437614917755127, 'lr': 0.0007696827262044653, 'memory_usage_mb': 1561.66015625}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 2.984318733215332, 'lr': 0.0007579318448883666, 'memory_usage_mb': 1561.66015625}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 2.9639174938201904, 'lr': 0.0007461809635722679, 'memory_usage_mb': 1561.66015625}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 2.561664432287216, 'rougeL': 0.28714781909000553, 'epoch_time_seconds': 70.90001702308655, 'memory_usage_mb': 1604.47265625}\n",
      "New best accuracy: 0.2871\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 2.8023204803466797, 'lr': 0.0007344300822561692, 'memory_usage_mb': 1604.47265625}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 2.8612515926361084, 'lr': 0.0007226792009400705, 'memory_usage_mb': 1604.47265625}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 2.839416265487671, 'lr': 0.0007109283196239718, 'memory_usage_mb': 1611.97265625}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 2.8336095809936523, 'lr': 0.000699177438307873, 'memory_usage_mb': 1611.97265625}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 2.6962714195251465, 'lr': 0.0006874265569917744, 'memory_usage_mb': 1611.97265625}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 2.690298318862915, 'lr': 0.0006756756756756757, 'memory_usage_mb': 1611.97265625}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 2.373624235391617, 'rougeL': 0.29196239368804305, 'epoch_time_seconds': 52.556177377700806, 'memory_usage_mb': 1621.97265625}\n",
      "New best accuracy: 0.2920\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 2.6858818531036377, 'lr': 0.000663924794359577, 'memory_usage_mb': 1621.97265625}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 2.6376030445098877, 'lr': 0.0006521739130434783, 'memory_usage_mb': 1626.66015625}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 2.572964668273926, 'lr': 0.0006404230317273796, 'memory_usage_mb': 1626.66015625}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 2.630281925201416, 'lr': 0.0006286721504112809, 'memory_usage_mb': 1626.66015625}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 2.506788492202759, 'lr': 0.0006169212690951822, 'memory_usage_mb': 1626.66015625}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 2.600731134414673, 'lr': 0.0006051703877790834, 'memory_usage_mb': 1626.66015625}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 2.5597493648529053, 'lr': 0.0005934195064629847, 'memory_usage_mb': 1631.34765625}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 2.2579208612442017, 'rougeL': 0.29040184916215006, 'epoch_time_seconds': 50.4780011177063, 'memory_usage_mb': 1652.91015625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 2.5843610763549805, 'lr': 0.000581668625146886, 'memory_usage_mb': 1652.91015625}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 2.5531318187713623, 'lr': 0.0005699177438307873, 'memory_usage_mb': 1661.34765625}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 2.5036962032318115, 'lr': 0.0005581668625146886, 'memory_usage_mb': 1661.34765625}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 2.4360997676849365, 'lr': 0.0005464159811985899, 'memory_usage_mb': 1661.34765625}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 2.460423707962036, 'lr': 0.0005346650998824912, 'memory_usage_mb': 1661.34765625}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 2.4905428886413574, 'lr': 0.0005229142185663925, 'memory_usage_mb': 1669.78515625}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.177605666220188, 'rougeL': 0.29091201472705297, 'epoch_time_seconds': 55.28680777549744, 'memory_usage_mb': 1691.03515625}\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 2.414933204650879, 'lr': 0.0005111633372502937, 'memory_usage_mb': 1691.03515625}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 2.418461799621582, 'lr': 0.0004994124559341951, 'memory_usage_mb': 1691.03515625}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 2.4156253337860107, 'lr': 0.00048766157461809634, 'memory_usage_mb': 1691.03515625}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.4212474822998047, 'lr': 0.00047591069330199764, 'memory_usage_mb': 1691.03515625}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.4460196495056152, 'lr': 0.00046415981198589894, 'memory_usage_mb': 1699.16015625}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.3816521167755127, 'lr': 0.00045240893066980024, 'memory_usage_mb': 1699.16015625}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.137606106698513, 'rougeL': 0.2925947880785357, 'epoch_time_seconds': 50.2253041267395, 'memory_usage_mb': 1721.34765625}\n",
      "New best accuracy: 0.2926\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.3779709339141846, 'lr': 0.00044065804935370154, 'memory_usage_mb': 1721.34765625}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.3722944259643555, 'lr': 0.00042890716803760284, 'memory_usage_mb': 1721.34765625}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.4487147331237793, 'lr': 0.00041715628672150414, 'memory_usage_mb': 1721.34765625}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.348607301712036, 'lr': 0.00040540540540540544, 'memory_usage_mb': 1721.34765625}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.3475184440612793, 'lr': 0.0003936545240893067, 'memory_usage_mb': 1721.34765625}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.2981228828430176, 'lr': 0.000381903642773208, 'memory_usage_mb': 1727.28515625}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.08799709379673, 'rougeL': 0.2973562435870673, 'epoch_time_seconds': 46.56357550621033, 'memory_usage_mb': 1740.09765625}\n",
      "New best accuracy: 0.2974\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.335254430770874, 'lr': 0.0003701527614571093, 'memory_usage_mb': 1740.09765625}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.3582186698913574, 'lr': 0.0003584018801410106, 'memory_usage_mb': 1740.09765625}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.377333402633667, 'lr': 0.0003466509988249119, 'memory_usage_mb': 1740.09765625}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.3853588104248047, 'lr': 0.0003349001175088132, 'memory_usage_mb': 1740.09765625}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.3383893966674805, 'lr': 0.0003231492361927145, 'memory_usage_mb': 1740.09765625}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.288447856903076, 'lr': 0.0003113983548766158, 'memory_usage_mb': 1740.09765625}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.2842278480529785, 'lr': 0.000299647473560517, 'memory_usage_mb': 1740.09765625}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.061618961393833, 'rougeL': 0.2958901064797806, 'epoch_time_seconds': 48.190265417099, 'memory_usage_mb': 1745.09765625}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.374842643737793, 'lr': 0.0002878965922444183, 'memory_usage_mb': 1745.09765625}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.309617757797241, 'lr': 0.0002761457109283196, 'memory_usage_mb': 1745.09765625}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.2924516201019287, 'lr': 0.0002643948296122209, 'memory_usage_mb': 1745.09765625}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.3016395568847656, 'lr': 0.00025264394829612217, 'memory_usage_mb': 1745.09765625}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.3307301998138428, 'lr': 0.0002408930669800235, 'memory_usage_mb': 1745.09765625}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.267143726348877, 'lr': 0.0002291421856639248, 'memory_usage_mb': 1745.09765625}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.042443308979273, 'rougeL': 0.29781055976489224, 'epoch_time_seconds': 47.52189755439758, 'memory_usage_mb': 1745.09765625}\n",
      "New best accuracy: 0.2978\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.3115527629852295, 'lr': 0.0002173913043478261, 'memory_usage_mb': 1745.09765625}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.327528953552246, 'lr': 0.0002056404230317274, 'memory_usage_mb': 1745.09765625}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.3023483753204346, 'lr': 0.00019388954171562867, 'memory_usage_mb': 1745.09765625}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.286083936691284, 'lr': 0.00018213866039952997, 'memory_usage_mb': 1745.09765625}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.3614110946655273, 'lr': 0.00017038777908343127, 'memory_usage_mb': 1745.09765625}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.3334884643554688, 'lr': 0.00015863689776733257, 'memory_usage_mb': 1745.09765625}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.028402153402567, 'rougeL': 0.2984401523639369, 'epoch_time_seconds': 47.36110782623291, 'memory_usage_mb': 1745.72265625}\n",
      "New best accuracy: 0.2984\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.273495674133301, 'lr': 0.00014688601645123384, 'memory_usage_mb': 1745.72265625}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.324939727783203, 'lr': 0.00013513513513513514, 'memory_usage_mb': 1745.72265625}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.3111183643341064, 'lr': 0.00012338425381903644, 'memory_usage_mb': 1745.72265625}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.3531837463378906, 'lr': 0.00011163337250293772, 'memory_usage_mb': 1745.72265625}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.3746485710144043, 'lr': 9.988249118683901e-05, 'memory_usage_mb': 1745.72265625}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.2403225898742676, 'lr': 8.81316098707403e-05, 'memory_usage_mb': 1745.72265625}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.2757468223571777, 'lr': 7.63807285546416e-05, 'memory_usage_mb': 1745.72265625}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.0188485831022263, 'rougeL': 0.30284051211329116, 'epoch_time_seconds': 47.45024037361145, 'memory_usage_mb': 1748.22265625}\n",
      "New best accuracy: 0.3028\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.254305124282837, 'lr': 6.462984723854288e-05, 'memory_usage_mb': 1748.22265625}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.2905311584472656, 'lr': 5.2878965922444186e-05, 'memory_usage_mb': 1748.22265625}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.297899007797241, 'lr': 4.112808460634548e-05, 'memory_usage_mb': 1748.22265625}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.3540713787078857, 'lr': 2.9377203290246768e-05, 'memory_usage_mb': 1748.53515625}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.253910779953003, 'lr': 1.762632197414806e-05, 'memory_usage_mb': 1748.53515625}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.300067663192749, 'lr': 5.875440658049354e-06, 'memory_usage_mb': 1748.53515625}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.019354410469532, 'rougeL': 0.30094021908414104, 'epoch_time_seconds': 46.590495586395264, 'memory_usage_mb': 1748.87890625}\n",
      "Step 945: {'total_training_time_seconds': 768.3040261268616, 'avg_epoch_time_seconds': 51.218771330515544, 'best_accuracy': 0.30284051211329116, 'convergence_step': 'Not converged', 'memory_usage_mb': 1748.87890625}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÅ‚ñá‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 51.21877\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.30284\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 46.5905\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.01935\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 1e-05\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1748.87891\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.30094\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 768.30403\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 2.30007\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.001_wd0.01_samsum_20250425_203705\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/97kgedde\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_203705-97kgedde/logs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a90b2a",
   "metadata": {},
   "source": [
    "### Higher Weight Decay (0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786923e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_210413-flzwp2gk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.1_samsum_20250425_210413\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/flzwp2gk\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0, 'memory_usage_mb': 1312.0859375}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.77180862426758, 'lr': 3.1914893617021275e-05, 'memory_usage_mb': 1377.3359375}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 36.80939865112305, 'lr': 6.382978723404255e-05, 'memory_usage_mb': 1378.2734375}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 39.39719009399414, 'lr': 9.574468085106382e-05, 'memory_usage_mb': 1378.5859375}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 36.497528076171875, 'lr': 0.0001276595744680851, 'memory_usage_mb': 1379.2109375}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 34.9708251953125, 'lr': 0.00015957446808510637, 'memory_usage_mb': 1379.2109375}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 30.888463973999023, 'lr': 0.00019148936170212765, 'memory_usage_mb': 1379.2109375}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 26.727157711982727, 'rougeL': 0.27012582845792693, 'epoch_time_seconds': 66.18755960464478, 'memory_usage_mb': 1514.3671875}\n",
      "New best accuracy: 0.2701\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 23.987680435180664, 'lr': 0.0002234042553191489, 'memory_usage_mb': 1514.3671875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 19.48454475402832, 'lr': 0.0002553191489361702, 'memory_usage_mb': 1514.3671875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 13.955326080322266, 'lr': 0.0002872340425531915, 'memory_usage_mb': 1514.3671875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 8.664811134338379, 'lr': 0.00029788484136310223, 'memory_usage_mb': 1514.3671875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 7.546847343444824, 'lr': 0.0002943595769682726, 'memory_usage_mb': 1514.3671875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 6.464262962341309, 'lr': 0.000290834312573443, 'memory_usage_mb': 1514.3671875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 5.015513002872467, 'rougeL': 0.16782025274465773, 'epoch_time_seconds': 54.13812804222107, 'memory_usage_mb': 1514.9921875}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 5.617321968078613, 'lr': 0.00028730904817861336, 'memory_usage_mb': 1514.9921875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 5.087122917175293, 'lr': 0.00028378378378378377, 'memory_usage_mb': 1514.9921875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.903637409210205, 'lr': 0.0002802585193889541, 'memory_usage_mb': 1514.9921875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 4.703579902648926, 'lr': 0.00027673325499412454, 'memory_usage_mb': 1514.9921875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 4.622314453125, 'lr': 0.00027320799059929495, 'memory_usage_mb': 1514.9921875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.478093147277832, 'lr': 0.0002696827262044653, 'memory_usage_mb': 1514.9921875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.332442834973335, 'rougeL': 0.25108480362053087, 'epoch_time_seconds': 57.82681632041931, 'memory_usage_mb': 1514.9921875}\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.591794967651367, 'lr': 0.00026615746180963566, 'memory_usage_mb': 1514.9921875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.42284631729126, 'lr': 0.0002626321974148061, 'memory_usage_mb': 1514.9921875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.413042068481445, 'lr': 0.0002591069330199765, 'memory_usage_mb': 1514.9921875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.361201763153076, 'lr': 0.00025558166862514684, 'memory_usage_mb': 1514.9921875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 4.312107086181641, 'lr': 0.00025205640423031726, 'memory_usage_mb': 1514.9921875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.360044479370117, 'lr': 0.00024853113983548767, 'memory_usage_mb': 1514.9921875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 4.244270324707031, 'lr': 0.000245005875440658, 'memory_usage_mb': 1514.9921875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 4.0518215000629425, 'rougeL': 0.25695302148923294, 'epoch_time_seconds': 66.94011545181274, 'memory_usage_mb': 1514.9921875}\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.2435407638549805, 'lr': 0.00024148061104582844, 'memory_usage_mb': 1514.9921875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 4.088951587677002, 'lr': 0.0002379553466509988, 'memory_usage_mb': 1514.9921875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 4.218912124633789, 'lr': 0.00023443008225616918, 'memory_usage_mb': 1514.9921875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 4.199029445648193, 'lr': 0.00023090481786133956, 'memory_usage_mb': 1514.9921875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 4.101064205169678, 'lr': 0.00022737955346650997, 'memory_usage_mb': 1514.9921875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 4.091133117675781, 'lr': 0.00022385428907168036, 'memory_usage_mb': 1514.9921875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.8160102292895317, 'rougeL': 0.2615036762287932, 'epoch_time_seconds': 66.5096025466919, 'memory_usage_mb': 1514.9921875}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.93319034576416, 'lr': 0.00022032902467685074, 'memory_usage_mb': 1514.9921875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 4.012039661407471, 'lr': 0.00021680376028202115, 'memory_usage_mb': 1514.9921875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.97617769241333, 'lr': 0.00021327849588719154, 'memory_usage_mb': 1514.9921875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.9867584705352783, 'lr': 0.0002097532314923619, 'memory_usage_mb': 1514.9921875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.8293004035949707, 'lr': 0.00020622796709753228, 'memory_usage_mb': 1514.9921875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.79892635345459, 'lr': 0.0002027027027027027, 'memory_usage_mb': 1514.9921875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.599958024919033, 'rougeL': 0.28058915284234937, 'epoch_time_seconds': 57.39402222633362, 'memory_usage_mb': 1514.9921875}\n",
      "New best accuracy: 0.2806\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.8358147144317627, 'lr': 0.00019917743830787308, 'memory_usage_mb': 1514.9921875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.823892593383789, 'lr': 0.00019565217391304346, 'memory_usage_mb': 1514.9921875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.676978349685669, 'lr': 0.00019212690951821385, 'memory_usage_mb': 1514.9921875}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.6575300693511963, 'lr': 0.00018860164512338426, 'memory_usage_mb': 1514.9921875}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.5729668140411377, 'lr': 0.00018507638072855464, 'memory_usage_mb': 1514.9921875}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.6773135662078857, 'lr': 0.000181551116333725, 'memory_usage_mb': 1514.9921875}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.6355655193328857, 'lr': 0.00017802585193889538, 'memory_usage_mb': 1514.9921875}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 3.3715603351593018, 'rougeL': 0.2855444382801081, 'epoch_time_seconds': 42.96586561203003, 'memory_usage_mb': 1514.9921875}\n",
      "New best accuracy: 0.2855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.594088554382324, 'lr': 0.0001745005875440658, 'memory_usage_mb': 1514.9921875}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.5554397106170654, 'lr': 0.00017097532314923618, 'memory_usage_mb': 1514.9921875}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.5569941997528076, 'lr': 0.00016745005875440656, 'memory_usage_mb': 1514.9921875}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.4331164360046387, 'lr': 0.00016392479435957695, 'memory_usage_mb': 1514.9921875}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.4103469848632812, 'lr': 0.00016039952996474736, 'memory_usage_mb': 1514.9921875}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.4851062297821045, 'lr': 0.00015687426556991774, 'memory_usage_mb': 1514.9921875}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 3.1765957847237587, 'rougeL': 0.28905541232109094, 'epoch_time_seconds': 43.83108377456665, 'memory_usage_mb': 1514.9921875}\n",
      "New best accuracy: 0.2891\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.4077541828155518, 'lr': 0.0001533490011750881, 'memory_usage_mb': 1514.9921875}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.384666681289673, 'lr': 0.0001498237367802585, 'memory_usage_mb': 1514.9921875}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.3990166187286377, 'lr': 0.0001462984723854289, 'memory_usage_mb': 1514.9921875}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 3.353498935699463, 'lr': 0.00014277320799059928, 'memory_usage_mb': 1514.9921875}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 3.3293492794036865, 'lr': 0.00013924794359576967, 'memory_usage_mb': 1514.9921875}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 3.330150604248047, 'lr': 0.00013572267920094008, 'memory_usage_mb': 1514.9921875}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 3.0316983312368393, 'rougeL': 0.28850363840474547, 'epoch_time_seconds': 39.98355841636658, 'memory_usage_mb': 1514.9921875}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 3.3057026863098145, 'lr': 0.00013219741480611043, 'memory_usage_mb': 1514.9921875}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 3.2571661472320557, 'lr': 0.00012867215041128085, 'memory_usage_mb': 1514.9921875}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 3.329469680786133, 'lr': 0.00012514688601645123, 'memory_usage_mb': 1514.9921875}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 3.2274105548858643, 'lr': 0.00012162162162162162, 'memory_usage_mb': 1514.9921875}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 3.1839115619659424, 'lr': 0.000118096357226792, 'memory_usage_mb': 1514.9921875}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 3.1481494903564453, 'lr': 0.00011457109283196238, 'memory_usage_mb': 1514.9921875}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.8934742137789726, 'rougeL': 0.29299667453172484, 'epoch_time_seconds': 42.807446002960205, 'memory_usage_mb': 1514.9921875}\n",
      "New best accuracy: 0.2930\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 3.1925971508026123, 'lr': 0.00011104582843713278, 'memory_usage_mb': 1514.9921875}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 3.225101947784424, 'lr': 0.00010752056404230317, 'memory_usage_mb': 1515.3046875}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 3.183438539505005, 'lr': 0.00010399529964747355, 'memory_usage_mb': 1515.3046875}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 3.216212749481201, 'lr': 0.00010047003525264394, 'memory_usage_mb': 1515.3046875}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 3.1605148315429688, 'lr': 9.694477085781433e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 3.1215760707855225, 'lr': 9.341950646298472e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 3.040696620941162, 'lr': 8.98942420681551e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.7982502952218056, 'rougeL': 0.29321111055188287, 'epoch_time_seconds': 40.636879444122314, 'memory_usage_mb': 1515.3046875}\n",
      "New best accuracy: 0.2932\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 3.1540753841400146, 'lr': 8.636897767332549e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 3.106407403945923, 'lr': 8.284371327849588e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 3.0635478496551514, 'lr': 7.931844888366627e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 3.085930824279785, 'lr': 7.579318448883665e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 3.139735460281372, 'lr': 7.226792009400704e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 3.032989978790283, 'lr': 6.874265569917744e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.7460665181279182, 'rougeL': 0.2923506963943987, 'epoch_time_seconds': 39.21382141113281, 'memory_usage_mb': 1515.3046875}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 3.1099774837493896, 'lr': 6.521739130434782e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 3.095987319946289, 'lr': 6.16921269095182e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 3.0757627487182617, 'lr': 5.8166862514688596e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 3.025125503540039, 'lr': 5.464159811985899e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 3.09745717048645, 'lr': 5.111633372502937e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 3.0733535289764404, 'lr': 4.759106933019976e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.70176949352026, 'rougeL': 0.29061225391507534, 'epoch_time_seconds': 39.429006576538086, 'memory_usage_mb': 1515.3046875}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 3.084902286529541, 'lr': 4.406580493537015e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 3.048413038253784, 'lr': 4.054054054054054e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 3.089371681213379, 'lr': 3.701527614571092e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 3.0886855125427246, 'lr': 3.3490011750881314e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 3.022613763809204, 'lr': 2.99647473560517e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.9536123275756836, 'lr': 2.6439482961222086e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 3.0081632137298584, 'lr': 2.2914218566392474e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.6754414066672325, 'rougeL': 0.2897478793500915, 'epoch_time_seconds': 41.030750036239624, 'memory_usage_mb': 1515.3046875}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.9563279151916504, 'lr': 1.9388954171562862e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.9984467029571533, 'lr': 1.5863689776733253e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 3.024177074432373, 'lr': 1.2338425381903641e-05, 'memory_usage_mb': 1515.3046875}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 3.099590539932251, 'lr': 8.81316098707403e-06, 'memory_usage_mb': 1515.3046875}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.9988183975219727, 'lr': 5.287896592244418e-06, 'memory_usage_mb': 1515.3046875}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 3.028932571411133, 'lr': 1.7626321974148059e-06, 'memory_usage_mb': 1515.3046875}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.6635877043008804, 'rougeL': 0.28955901113584054, 'epoch_time_seconds': 41.67018008232117, 'memory_usage_mb': 1515.3046875}\n",
      "Step 945: {'total_training_time_seconds': 740.5994696617126, 'avg_epoch_time_seconds': 49.37098903656006, 'best_accuracy': 0.29321111055188287, 'convergence_step': 'Not converged', 'memory_usage_mb': 1515.3046875}\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñà‚ñÖ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñá‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 49.37099\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.29321\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 41.67018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.66359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1515.30469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.28956\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 740.59947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 3.02893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.1_samsum_20250425_210413\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/flzwp2gk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_210413-flzwp2gk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bf468",
   "metadata": {},
   "source": [
    "### Lower Weight Decay (0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4192c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_211658-34hn6ymw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.001_samsum_20250425_211658\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/34hn6ymw\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0, 'memory_usage_mb': 1321.8125}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 40.77180862426758, 'lr': 3.1914893617021275e-05, 'memory_usage_mb': 1387.6953125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 36.809364318847656, 'lr': 6.382978723404255e-05, 'memory_usage_mb': 1388.6328125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 39.39696502685547, 'lr': 9.574468085106382e-05, 'memory_usage_mb': 1388.9453125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 36.49665069580078, 'lr': 0.0001276595744680851, 'memory_usage_mb': 1389.2578125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 34.96834182739258, 'lr': 0.00015957446808510637, 'memory_usage_mb': 1389.2578125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 30.88409996032715, 'lr': 0.00019148936170212765, 'memory_usage_mb': 1389.2578125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 26.722037971019745, 'rougeL': 0.2701288314609299, 'epoch_time_seconds': 64.50087022781372, 'memory_usage_mb': 1525.421875}\n",
      "New best accuracy: 0.2701\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 23.982542037963867, 'lr': 0.0002234042553191489, 'memory_usage_mb': 1525.421875}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 19.478845596313477, 'lr': 0.0002553191489361702, 'memory_usage_mb': 1525.421875}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 13.943014144897461, 'lr': 0.0002872340425531915, 'memory_usage_mb': 1525.421875}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 8.664689064025879, 'lr': 0.00029788484136310223, 'memory_usage_mb': 1525.421875}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 7.548124313354492, 'lr': 0.0002943595769682726, 'memory_usage_mb': 1525.421875}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 6.48200798034668, 'lr': 0.000290834312573443, 'memory_usage_mb': 1525.421875}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 5.023220598697662, 'rougeL': 0.16953229766108813, 'epoch_time_seconds': 54.280590295791626, 'memory_usage_mb': 1526.046875}\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 5.631324768066406, 'lr': 0.00028730904817861336, 'memory_usage_mb': 1526.046875}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 5.090958118438721, 'lr': 0.00028378378378378377, 'memory_usage_mb': 1526.046875}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.906186580657959, 'lr': 0.0002802585193889541, 'memory_usage_mb': 1526.046875}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 4.7067646980285645, 'lr': 0.00027673325499412454, 'memory_usage_mb': 1526.046875}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 4.622771739959717, 'lr': 0.00027320799059929495, 'memory_usage_mb': 1526.046875}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.477352142333984, 'lr': 0.0002696827262044653, 'memory_usage_mb': 1526.046875}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.332526311278343, 'rougeL': 0.25106147333885825, 'epoch_time_seconds': 59.33000946044922, 'memory_usage_mb': 1526.046875}\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.592360973358154, 'lr': 0.00026615746180963566, 'memory_usage_mb': 1526.046875}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.4233479499816895, 'lr': 0.0002626321974148061, 'memory_usage_mb': 1526.046875}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.4134674072265625, 'lr': 0.0002591069330199765, 'memory_usage_mb': 1526.046875}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.360225677490234, 'lr': 0.00025558166862514684, 'memory_usage_mb': 1526.046875}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 4.313267707824707, 'lr': 0.00025205640423031726, 'memory_usage_mb': 1526.046875}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.361202239990234, 'lr': 0.00024853113983548767, 'memory_usage_mb': 1526.046875}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 4.245112895965576, 'lr': 0.000245005875440658, 'memory_usage_mb': 1526.046875}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 4.052713371813297, 'rougeL': 0.25575959820347854, 'epoch_time_seconds': 71.93296194076538, 'memory_usage_mb': 1526.046875}\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.244570732116699, 'lr': 0.00024148061104582844, 'memory_usage_mb': 1526.046875}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 4.091076850891113, 'lr': 0.0002379553466509988, 'memory_usage_mb': 1526.046875}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 4.221526622772217, 'lr': 0.00023443008225616918, 'memory_usage_mb': 1526.046875}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 4.20313835144043, 'lr': 0.00023090481786133956, 'memory_usage_mb': 1526.046875}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 4.090458393096924, 'lr': 0.00022737955346650997, 'memory_usage_mb': 1526.046875}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 4.093888282775879, 'lr': 0.00022385428907168036, 'memory_usage_mb': 1526.046875}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.8234176114201546, 'rougeL': 0.2633569906747828, 'epoch_time_seconds': 62.35700964927673, 'memory_usage_mb': 1526.046875}\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.9372544288635254, 'lr': 0.00022032902467685074, 'memory_usage_mb': 1526.046875}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 4.0131049156188965, 'lr': 0.00021680376028202115, 'memory_usage_mb': 1526.046875}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.9787790775299072, 'lr': 0.00021327849588719154, 'memory_usage_mb': 1526.046875}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.989993095397949, 'lr': 0.0002097532314923619, 'memory_usage_mb': 1526.046875}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.831594467163086, 'lr': 0.00020622796709753228, 'memory_usage_mb': 1526.046875}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.804077386856079, 'lr': 0.0002027027027027027, 'memory_usage_mb': 1526.046875}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.6027150228619576, 'rougeL': 0.27834096384125573, 'epoch_time_seconds': 57.79770302772522, 'memory_usage_mb': 1526.046875}\n",
      "New best accuracy: 0.2783\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.837731122970581, 'lr': 0.00019917743830787308, 'memory_usage_mb': 1526.046875}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.826195001602173, 'lr': 0.00019565217391304346, 'memory_usage_mb': 1526.046875}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.6767396926879883, 'lr': 0.00019212690951821385, 'memory_usage_mb': 1526.359375}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.660163402557373, 'lr': 0.00018860164512338426, 'memory_usage_mb': 1526.359375}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.571823835372925, 'lr': 0.00018507638072855464, 'memory_usage_mb': 1526.359375}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.68174147605896, 'lr': 0.000181551116333725, 'memory_usage_mb': 1526.359375}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.6399903297424316, 'lr': 0.00017802585193889538, 'memory_usage_mb': 1526.359375}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 3.37569260597229, 'rougeL': 0.28254446247698706, 'epoch_time_seconds': 46.99628448486328, 'memory_usage_mb': 1526.359375}\n",
      "New best accuracy: 0.2825\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.5963118076324463, 'lr': 0.0001745005875440658, 'memory_usage_mb': 1526.359375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.556622266769409, 'lr': 0.00017097532314923618, 'memory_usage_mb': 1526.359375}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.557731866836548, 'lr': 0.00016745005875440656, 'memory_usage_mb': 1526.359375}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.436661958694458, 'lr': 0.00016392479435957695, 'memory_usage_mb': 1526.359375}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.4133472442626953, 'lr': 0.00016039952996474736, 'memory_usage_mb': 1526.359375}\n",
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.4895436763763428, 'lr': 0.00015687426556991774, 'memory_usage_mb': 1526.359375}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 3.181338347494602, 'rougeL': 0.2894389278104296, 'epoch_time_seconds': 49.06790232658386, 'memory_usage_mb': 1526.359375}\n",
      "New best accuracy: 0.2894\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.409315586090088, 'lr': 0.0001533490011750881, 'memory_usage_mb': 1526.359375}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.3895463943481445, 'lr': 0.0001498237367802585, 'memory_usage_mb': 1526.359375}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.4030752182006836, 'lr': 0.0001462984723854289, 'memory_usage_mb': 1526.359375}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 3.3574366569519043, 'lr': 0.00014277320799059928, 'memory_usage_mb': 1526.359375}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 3.3333327770233154, 'lr': 0.00013924794359576967, 'memory_usage_mb': 1526.359375}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 3.3361098766326904, 'lr': 0.00013572267920094008, 'memory_usage_mb': 1526.359375}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 3.03879763931036, 'rougeL': 0.28973818050488737, 'epoch_time_seconds': 37.70288419723511, 'memory_usage_mb': 1526.359375}\n",
      "New best accuracy: 0.2897\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 3.3066489696502686, 'lr': 0.00013219741480611043, 'memory_usage_mb': 1526.359375}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 3.261345148086548, 'lr': 0.00012867215041128085, 'memory_usage_mb': 1526.359375}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 3.3389501571655273, 'lr': 0.00012514688601645123, 'memory_usage_mb': 1526.359375}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 3.231844902038574, 'lr': 0.00012162162162162162, 'memory_usage_mb': 1526.359375}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 3.188131809234619, 'lr': 0.000118096357226792, 'memory_usage_mb': 1526.359375}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 3.1485676765441895, 'lr': 0.00011457109283196238, 'memory_usage_mb': 1526.359375}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.9005055353045464, 'rougeL': 0.29003782999623956, 'epoch_time_seconds': 40.041715145111084, 'memory_usage_mb': 1526.359375}\n",
      "New best accuracy: 0.2900\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 3.1950814723968506, 'lr': 0.00011104582843713278, 'memory_usage_mb': 1526.359375}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 3.229370355606079, 'lr': 0.00010752056404230317, 'memory_usage_mb': 1526.359375}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 3.188124179840088, 'lr': 0.00010399529964747355, 'memory_usage_mb': 1526.359375}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 3.2195444107055664, 'lr': 0.00010047003525264394, 'memory_usage_mb': 1526.359375}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 3.165152072906494, 'lr': 9.694477085781433e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 3.125136375427246, 'lr': 9.341950646298472e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 3.041645050048828, 'lr': 8.98942420681551e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.8044750317931175, 'rougeL': 0.2911351006054748, 'epoch_time_seconds': 40.615623235702515, 'memory_usage_mb': 1526.359375}\n",
      "New best accuracy: 0.2911\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 3.1591453552246094, 'lr': 8.636897767332549e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 3.110093116760254, 'lr': 8.284371327849588e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 3.0682249069213867, 'lr': 7.931844888366627e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 3.0896060466766357, 'lr': 7.579318448883665e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 3.141615152359009, 'lr': 7.226792009400704e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 3.036557674407959, 'lr': 6.874265569917744e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.7522519156336784, 'rougeL': 0.2905124004656166, 'epoch_time_seconds': 41.12854719161987, 'memory_usage_mb': 1526.359375}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 3.116124153137207, 'lr': 6.521739130434782e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 3.0987627506256104, 'lr': 6.16921269095182e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 3.0793938636779785, 'lr': 5.8166862514688596e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 3.030573606491089, 'lr': 5.464159811985899e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 3.102606773376465, 'lr': 5.111633372502937e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 3.078953742980957, 'lr': 4.759106933019976e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.708983413875103, 'rougeL': 0.28823896248153724, 'epoch_time_seconds': 50.11196732521057, 'memory_usage_mb': 1526.359375}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 3.0895073413848877, 'lr': 4.406580493537015e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 3.052225112915039, 'lr': 4.054054054054054e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 3.098101854324341, 'lr': 3.701527614571092e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 3.096046209335327, 'lr': 3.3490011750881314e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 3.030949115753174, 'lr': 2.99647473560517e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.957897424697876, 'lr': 2.6439482961222086e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 3.013791561126709, 'lr': 2.2914218566392474e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.683919481933117, 'rougeL': 0.28803776403622594, 'epoch_time_seconds': 41.406771421432495, 'memory_usage_mb': 1526.359375}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.9606411457061768, 'lr': 1.9388954171562862e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 3.003966808319092, 'lr': 1.5863689776733253e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 3.0329434871673584, 'lr': 1.2338425381903641e-05, 'memory_usage_mb': 1526.359375}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 3.1079063415527344, 'lr': 8.81316098707403e-06, 'memory_usage_mb': 1526.359375}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 3.0069305896759033, 'lr': 5.287896592244418e-06, 'memory_usage_mb': 1526.359375}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 3.0385663509368896, 'lr': 1.7626321974148059e-06, 'memory_usage_mb': 1526.359375}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.672276958823204, 'rougeL': 0.28721768044873, 'epoch_time_seconds': 41.47193145751953, 'memory_usage_mb': 1526.359375}\n",
      "Step 945: {'total_training_time_seconds': 758.7710695266724, 'avg_epoch_time_seconds': 50.58285142580668, 'best_accuracy': 0.2911351006054748, 'convergence_step': 'Not converged', 'memory_usage_mb': 1526.359375}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÜ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñá‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 50.58285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.29114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 41.47193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.67228\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1526.35938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.28722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 758.77107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 3.03857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_warmup_lr0.0003_wd0.001_samsum_20250425_211658\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/34hn6ymw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_211658-34hn6ymw/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.001 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312a8ef",
   "metadata": {},
   "source": [
    "### Plateau with F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285e5822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpgarg76\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/9/6/pgarg76/dl/peft-convergence/wandb/run-20250425_212956-9jn0xo4o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_samsum_20250425_212956\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/9jn0xo4o\u001b[0m\n",
      "Loading model: google/flan-t5-small\n",
      "Preparing dataset: samsum\n",
      "Applying PEFT method: lora\n",
      "Total parameters: 77305216\n",
      "Trainable parameters: 344064 (0.45%)\n",
      "Starting training...\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 37.840938568115234, 'lr': 0.0003, 'memory_usage_mb': 1321.9375}\n",
      "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 37.64632797241211, 'lr': 0.0003, 'memory_usage_mb': 1387.8203125}\n",
      "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 28.870807647705078, 'lr': 0.0003, 'memory_usage_mb': 1388.7578125}\n",
      "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 25.61958885192871, 'lr': 0.0003, 'memory_usage_mb': 1389.0703125}\n",
      "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 20.60686683654785, 'lr': 0.0003, 'memory_usage_mb': 1389.3828125}\n",
      "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 16.18609619140625, 'lr': 0.0003, 'memory_usage_mb': 1389.6953125}\n",
      "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 11.457185745239258, 'lr': 0.0003, 'memory_usage_mb': 1389.6953125}\n",
      "Step 63: {'epoch': 0, 'eval_loss': 7.486599639058113, 'rougeL': 0.10852145413992983, 'epoch_time_seconds': 25.214215755462646, 'memory_usage_mb': 1524.6953125}\n",
      "New best accuracy: 0.1085\n",
      "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 8.625401496887207, 'lr': 0.0003, 'memory_usage_mb': 1525.0078125}\n",
      "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 7.54392147064209, 'lr': 0.0003, 'memory_usage_mb': 1525.0078125}\n",
      "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 6.413055896759033, 'lr': 0.0003, 'memory_usage_mb': 1525.0078125}\n",
      "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 5.427874565124512, 'lr': 0.0003, 'memory_usage_mb': 1525.0078125}\n",
      "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 4.956932544708252, 'lr': 0.0003, 'memory_usage_mb': 1525.0078125}\n",
      "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 4.836333274841309, 'lr': 0.0003, 'memory_usage_mb': 1525.0078125}\n",
      "Step 126: {'epoch': 1, 'eval_loss': 4.471572354435921, 'rougeL': 0.2324372897155914, 'epoch_time_seconds': 47.08476233482361, 'memory_usage_mb': 1525.6328125}\n",
      "New best accuracy: 0.2324\n",
      "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 4.742351531982422, 'lr': 0.0003, 'memory_usage_mb': 1525.6328125}\n",
      "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 4.63507080078125, 'lr': 0.0003, 'memory_usage_mb': 1525.6328125}\n",
      "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 4.595230579376221, 'lr': 0.0003, 'memory_usage_mb': 1525.6328125}\n",
      "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 4.510801315307617, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 4.4730544090271, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 4.3418049812316895, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 189: {'epoch': 2, 'eval_loss': 4.1639465391635895, 'rougeL': 0.24763484259013666, 'epoch_time_seconds': 68.1813735961914, 'memory_usage_mb': 1525.9453125}\n",
      "New best accuracy: 0.2476\n",
      "Step 190: {'epoch': 3, 'step': 190, 'train_loss': 4.446924209594727, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 200: {'epoch': 3, 'step': 200, 'train_loss': 4.246571063995361, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 210: {'epoch': 3, 'step': 210, 'train_loss': 4.237246513366699, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 220: {'epoch': 3, 'step': 220, 'train_loss': 4.1782612800598145, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 230: {'epoch': 3, 'step': 230, 'train_loss': 4.125941753387451, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 240: {'epoch': 3, 'step': 240, 'train_loss': 4.164710521697998, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 250: {'epoch': 3, 'step': 250, 'train_loss': 4.06135368347168, 'lr': 0.0003, 'memory_usage_mb': 1525.9453125}\n",
      "Step 252: {'epoch': 3, 'eval_loss': 3.8677202239632607, 'rougeL': 0.25625775639564474, 'epoch_time_seconds': 72.4609649181366, 'memory_usage_mb': 1526.2578125}\n",
      "New best accuracy: 0.2563\n",
      "Step 260: {'epoch': 4, 'step': 260, 'train_loss': 4.043960094451904, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 270: {'epoch': 4, 'step': 270, 'train_loss': 3.8931634426116943, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 280: {'epoch': 4, 'step': 280, 'train_loss': 3.9922709465026855, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 290: {'epoch': 4, 'step': 290, 'train_loss': 3.948561429977417, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 300: {'epoch': 4, 'step': 300, 'train_loss': 3.8664119243621826, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 310: {'epoch': 4, 'step': 310, 'train_loss': 3.8350343704223633, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 315: {'epoch': 4, 'eval_loss': 3.5313548371195793, 'rougeL': 0.2731208697170299, 'epoch_time_seconds': 53.59229636192322, 'memory_usage_mb': 1526.2578125}\n",
      "New best accuracy: 0.2731\n",
      "Step 320: {'epoch': 5, 'step': 320, 'train_loss': 3.6791090965270996, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 330: {'epoch': 5, 'step': 330, 'train_loss': 3.7128453254699707, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 340: {'epoch': 5, 'step': 340, 'train_loss': 3.6799001693725586, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 350: {'epoch': 5, 'step': 350, 'train_loss': 3.6851255893707275, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 360: {'epoch': 5, 'step': 360, 'train_loss': 3.5286591053009033, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 370: {'epoch': 5, 'step': 370, 'train_loss': 3.5068371295928955, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 378: {'epoch': 5, 'eval_loss': 3.2501608952879906, 'rougeL': 0.277661282798527, 'epoch_time_seconds': 46.8518853187561, 'memory_usage_mb': 1526.2578125}\n",
      "New best accuracy: 0.2777\n",
      "Step 380: {'epoch': 6, 'step': 380, 'train_loss': 3.5173518657684326, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 390: {'epoch': 6, 'step': 390, 'train_loss': 3.482165575027466, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 400: {'epoch': 6, 'step': 400, 'train_loss': 3.3491673469543457, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 410: {'epoch': 6, 'step': 410, 'train_loss': 3.353917360305786, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 420: {'epoch': 6, 'step': 420, 'train_loss': 3.249485492706299, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 430: {'epoch': 6, 'step': 430, 'train_loss': 3.3345422744750977, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 440: {'epoch': 6, 'step': 440, 'train_loss': 3.318725347518921, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 441: {'epoch': 6, 'eval_loss': 3.010710656642914, 'rougeL': 0.2908982836720264, 'epoch_time_seconds': 40.49704194068909, 'memory_usage_mb': 1526.2578125}\n",
      "New best accuracy: 0.2909\n",
      "Step 450: {'epoch': 7, 'step': 450, 'train_loss': 3.2751007080078125, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 460: {'epoch': 7, 'step': 460, 'train_loss': 3.2293541431427, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 470: {'epoch': 7, 'step': 470, 'train_loss': 3.194256544113159, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 480: {'epoch': 7, 'step': 480, 'train_loss': 3.094001054763794, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 490: {'epoch': 7, 'step': 490, 'train_loss': 3.0731868743896484, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: {'epoch': 7, 'step': 500, 'train_loss': 3.128371477127075, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 504: {'epoch': 7, 'eval_loss': 2.7620585039258003, 'rougeL': 0.2937801651586734, 'epoch_time_seconds': 54.329166650772095, 'memory_usage_mb': 1526.2578125}\n",
      "New best accuracy: 0.2938\n",
      "Step 510: {'epoch': 8, 'step': 510, 'train_loss': 3.0276377201080322, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 520: {'epoch': 8, 'step': 520, 'train_loss': 3.0204708576202393, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 530: {'epoch': 8, 'step': 530, 'train_loss': 3.032094717025757, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 540: {'epoch': 8, 'step': 540, 'train_loss': 2.984502077102661, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 550: {'epoch': 8, 'step': 550, 'train_loss': 2.9850823879241943, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 560: {'epoch': 8, 'step': 560, 'train_loss': 2.942432403564453, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 567: {'epoch': 8, 'eval_loss': 2.5903896391391754, 'rougeL': 0.2897302688885677, 'epoch_time_seconds': 42.6948618888855, 'memory_usage_mb': 1526.2578125}\n",
      "Step 570: {'epoch': 9, 'step': 570, 'train_loss': 2.904409170150757, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 580: {'epoch': 9, 'step': 580, 'train_loss': 2.879695177078247, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 590: {'epoch': 9, 'step': 590, 'train_loss': 2.9508657455444336, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 600: {'epoch': 9, 'step': 600, 'train_loss': 2.8516664505004883, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 610: {'epoch': 9, 'step': 610, 'train_loss': 2.7939882278442383, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 620: {'epoch': 9, 'step': 620, 'train_loss': 2.7505569458007812, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 630: {'epoch': 9, 'eval_loss': 2.461208552122116, 'rougeL': 0.2896400015155912, 'epoch_time_seconds': 45.88244295120239, 'memory_usage_mb': 1526.2578125}\n",
      "Step 630: {'epoch': 10, 'step': 630, 'train_loss': 2.7921698093414307, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 640: {'epoch': 10, 'step': 640, 'train_loss': 2.822604179382324, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 650: {'epoch': 10, 'step': 650, 'train_loss': 2.7891666889190674, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 660: {'epoch': 10, 'step': 660, 'train_loss': 2.797031879425049, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 670: {'epoch': 10, 'step': 670, 'train_loss': 2.7323460578918457, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 680: {'epoch': 10, 'step': 680, 'train_loss': 2.676342010498047, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 690: {'epoch': 10, 'step': 690, 'train_loss': 2.6337578296661377, 'lr': 0.0003, 'memory_usage_mb': 1526.2578125}\n",
      "Step 693: {'epoch': 10, 'eval_loss': 2.3660067170858383, 'rougeL': 0.28560257900733454, 'epoch_time_seconds': 46.24147987365723, 'memory_usage_mb': 1526.5703125}\n",
      "Step 700: {'epoch': 11, 'step': 700, 'train_loss': 2.736621856689453, 'lr': 0.00015, 'memory_usage_mb': 1526.5703125}\n",
      "Step 710: {'epoch': 11, 'step': 710, 'train_loss': 2.6642613410949707, 'lr': 0.00015, 'memory_usage_mb': 1526.5703125}\n",
      "Step 720: {'epoch': 11, 'step': 720, 'train_loss': 2.64775013923645, 'lr': 0.00015, 'memory_usage_mb': 1526.5703125}\n",
      "Step 730: {'epoch': 11, 'step': 730, 'train_loss': 2.6680397987365723, 'lr': 0.00015, 'memory_usage_mb': 1526.5703125}\n",
      "Step 740: {'epoch': 11, 'step': 740, 'train_loss': 2.7023799419403076, 'lr': 0.00015, 'memory_usage_mb': 1526.5703125}\n",
      "Step 750: {'epoch': 11, 'step': 750, 'train_loss': 2.609682321548462, 'lr': 0.00015, 'memory_usage_mb': 1526.5703125}\n",
      "Step 756: {'epoch': 11, 'eval_loss': 2.314431220293045, 'rougeL': 0.2863935624880315, 'epoch_time_seconds': 44.046265602111816, 'memory_usage_mb': 1529.3828125}\n",
      "Step 760: {'epoch': 12, 'step': 760, 'train_loss': 2.6701014041900635, 'lr': 0.00015, 'memory_usage_mb': 1529.3828125}\n",
      "Step 770: {'epoch': 12, 'step': 770, 'train_loss': 2.6695258617401123, 'lr': 0.00015, 'memory_usage_mb': 1529.3828125}\n",
      "Step 780: {'epoch': 12, 'step': 780, 'train_loss': 2.648206949234009, 'lr': 0.00015, 'memory_usage_mb': 1529.3828125}\n",
      "Step 790: {'epoch': 12, 'step': 790, 'train_loss': 2.6030454635620117, 'lr': 0.00015, 'memory_usage_mb': 1529.3828125}\n",
      "Step 800: {'epoch': 12, 'step': 800, 'train_loss': 2.685669422149658, 'lr': 0.00015, 'memory_usage_mb': 1529.3828125}\n",
      "Step 810: {'epoch': 12, 'step': 810, 'train_loss': 2.645556688308716, 'lr': 0.00015, 'memory_usage_mb': 1529.3828125}\n",
      "Step 819: {'epoch': 12, 'eval_loss': 2.282509133219719, 'rougeL': 0.287075961615741, 'epoch_time_seconds': 45.22779726982117, 'memory_usage_mb': 1544.3828125}\n",
      "Step 820: {'epoch': 13, 'step': 820, 'train_loss': 2.638697624206543, 'lr': 0.00015, 'memory_usage_mb': 1544.3828125}\n",
      "Step 830: {'epoch': 13, 'step': 830, 'train_loss': 2.62919545173645, 'lr': 0.00015, 'memory_usage_mb': 1545.3203125}\n",
      "Step 840: {'epoch': 13, 'step': 840, 'train_loss': 2.640045642852783, 'lr': 0.00015, 'memory_usage_mb': 1545.3203125}\n",
      "Step 850: {'epoch': 13, 'step': 850, 'train_loss': 2.6689743995666504, 'lr': 0.00015, 'memory_usage_mb': 1545.3203125}\n",
      "Step 860: {'epoch': 13, 'step': 860, 'train_loss': 2.652708053588867, 'lr': 0.00015, 'memory_usage_mb': 1545.3203125}\n",
      "Step 870: {'epoch': 13, 'step': 870, 'train_loss': 2.532201051712036, 'lr': 0.00015, 'memory_usage_mb': 1549.6953125}\n",
      "Step 880: {'epoch': 13, 'step': 880, 'train_loss': 2.5667011737823486, 'lr': 0.00015, 'memory_usage_mb': 1549.6953125}\n",
      "Step 882: {'epoch': 13, 'eval_loss': 2.2335950434207916, 'rougeL': 0.28841782637476066, 'epoch_time_seconds': 44.80592489242554, 'memory_usage_mb': 1578.7578125}\n",
      "Step 890: {'epoch': 14, 'step': 890, 'train_loss': 2.5461266040802, 'lr': 7.5e-05, 'memory_usage_mb': 1578.7578125}\n",
      "Step 900: {'epoch': 14, 'step': 900, 'train_loss': 2.5811381340026855, 'lr': 7.5e-05, 'memory_usage_mb': 1578.7578125}\n",
      "Step 910: {'epoch': 14, 'step': 910, 'train_loss': 2.5783512592315674, 'lr': 7.5e-05, 'memory_usage_mb': 1578.7578125}\n",
      "Step 920: {'epoch': 14, 'step': 920, 'train_loss': 2.654680013656616, 'lr': 7.5e-05, 'memory_usage_mb': 1588.4453125}\n",
      "Step 930: {'epoch': 14, 'step': 930, 'train_loss': 2.544262647628784, 'lr': 7.5e-05, 'memory_usage_mb': 1588.4453125}\n",
      "Step 940: {'epoch': 14, 'step': 940, 'train_loss': 2.5722267627716064, 'lr': 7.5e-05, 'memory_usage_mb': 1588.4453125}\n",
      "Step 945: {'epoch': 14, 'eval_loss': 2.2335100919008255, 'rougeL': 0.28817312230597025, 'epoch_time_seconds': 43.28533172607422, 'memory_usage_mb': 1598.1328125}\n",
      "Step 945: {'total_training_time_seconds': 720.4240508079529, 'avg_epoch_time_seconds': 48.02638740539551, 'best_accuracy': 0.2937801651586734, 'convergence_step': 'Not converged', 'memory_usage_mb': 1598.1328125}\n",
      "Training complete!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      avg_epoch_time_seconds 48.02639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               best_accuracy 0.29378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            convergence_step Not converged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch_time_seconds 43.28533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval_loss 2.23351\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             memory_usage_mb 1598.13281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      rougeL 0.28817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        step 940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time_seconds 720.42405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train_loss 2.57223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mlora_adamw_plateau_lr0.0003_wd0.01_samsum_20250425_212956\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum/runs/9jn0xo4o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pgarg76/peft-convergence-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_212956-9jn0xo4o/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_training_samsum.py \\\n",
    "    --model_name google/flan-t5-small \\\n",
    "    --dataset_name samsum \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule plateau \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 15 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7884d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
