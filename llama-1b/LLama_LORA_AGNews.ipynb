{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c051d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be05df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir_path = \"/home/hice1/tkaple3/scratch/hf-cache\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/hice1/tkaple3/scratch/hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765cf52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c6977",
   "metadata": {},
   "source": [
    "### Full Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c026c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_230541-bfqu4ilh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_warmup_20250425_230541\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/bfqu4ilh\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1914.06 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 5547.64 examples/s]\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 1.9108280254777072e-07, 'memory_mb': 1926.7109375}\n",
      "Step 10: {'train_loss': 6.693925380706787, 'lr': 2.1019108280254775e-06, 'memory_mb': 1929.7109375}\n",
      "Step 20: {'train_loss': 6.066961288452148, 'lr': 4.012738853503185e-06, 'memory_mb': 1930.2109375}\n",
      "Step 30: {'train_loss': 5.793533802032471, 'lr': 5.923566878980892e-06, 'memory_mb': 1930.2109375}\n",
      "Step 40: {'train_loss': 4.948215961456299, 'lr': 7.834394904458599e-06, 'memory_mb': 1930.2109375}\n",
      "Step 50: {'train_loss': 3.153064012527466, 'lr': 9.745222929936305e-06, 'memory_mb': 1930.2109375}\n",
      "Step 60: {'train_loss': 1.989108920097351, 'lr': 1.1656050955414015e-05, 'memory_mb': 1930.2109375}\n",
      "Step 63: {'eval_loss': 1.8835424706339836, 'memory_mb': 1932.2109375}\n",
      "Step 70: {'train_loss': 1.8341706991195679, 'lr': 1.356687898089172e-05, 'memory_mb': 1932.2109375}\n",
      "Step 80: {'train_loss': 1.9354618787765503, 'lr': 1.547770700636943e-05, 'memory_mb': 1932.2109375}\n",
      "Step 90: {'train_loss': 1.6404917240142822, 'lr': 1.7388535031847135e-05, 'memory_mb': 1932.2109375}\n",
      "Step 100: {'train_loss': 1.4722901582717896, 'lr': 1.929936305732484e-05, 'memory_mb': 1932.2109375}\n",
      "Step 110: {'train_loss': 1.5090386867523193, 'lr': 2.1210191082802547e-05, 'memory_mb': 1932.2109375}\n",
      "Step 120: {'train_loss': 1.741908073425293, 'lr': 2.3121019108280257e-05, 'memory_mb': 1932.2109375}\n",
      "Step 126: {'eval_loss': 1.6428379341959953, 'memory_mb': 1932.2109375}\n",
      "Step 130: {'train_loss': 1.642562747001648, 'lr': 2.5031847133757963e-05, 'memory_mb': 1932.2109375}\n",
      "Step 140: {'train_loss': 1.4815462827682495, 'lr': 2.694267515923567e-05, 'memory_mb': 1932.2109375}\n",
      "Step 150: {'train_loss': 1.6299182176589966, 'lr': 2.885350318471338e-05, 'memory_mb': 1932.2109375}\n",
      "Step 160: {'train_loss': 1.4058427810668945, 'lr': 2.991537376586742e-05, 'memory_mb': 1932.2109375}\n",
      "Step 170: {'train_loss': 1.6267489194869995, 'lr': 2.9703808180535966e-05, 'memory_mb': 1932.2109375}\n",
      "Step 180: {'train_loss': 1.5178552865982056, 'lr': 2.9492242595204516e-05, 'memory_mb': 1932.2109375}\n",
      "Step 189: {'eval_loss': 1.5387238189578056, 'memory_mb': 1932.2109375}\n",
      "Step 190: {'train_loss': 1.7129065990447998, 'lr': 2.9280677009873063e-05, 'memory_mb': 1932.2109375}\n",
      "Step 200: {'train_loss': 1.5894722938537598, 'lr': 2.906911142454161e-05, 'memory_mb': 1932.2109375}\n",
      "Step 210: {'train_loss': 1.3201160430908203, 'lr': 2.8857545839210153e-05, 'memory_mb': 1932.2109375}\n",
      "Step 220: {'train_loss': 1.4011669158935547, 'lr': 2.86459802538787e-05, 'memory_mb': 1932.2109375}\n",
      "Step 230: {'train_loss': 1.3303184509277344, 'lr': 2.843441466854725e-05, 'memory_mb': 1932.2109375}\n",
      "Step 240: {'train_loss': 1.4726710319519043, 'lr': 2.8222849083215797e-05, 'memory_mb': 1932.2109375}\n",
      "Step 250: {'train_loss': 1.2316782474517822, 'lr': 2.8011283497884344e-05, 'memory_mb': 1932.2109375}\n",
      "Step 252: {'eval_loss': 1.4057729505002499, 'memory_mb': 1932.2109375}\n",
      "Step 260: {'train_loss': 1.2213026285171509, 'lr': 2.779971791255289e-05, 'memory_mb': 1932.2109375}\n",
      "Step 270: {'train_loss': 1.7372957468032837, 'lr': 2.7588152327221438e-05, 'memory_mb': 1932.2109375}\n",
      "Step 280: {'train_loss': 1.5806647539138794, 'lr': 2.737658674188999e-05, 'memory_mb': 1932.2109375}\n",
      "Step 290: {'train_loss': 1.2411633729934692, 'lr': 2.7165021156558535e-05, 'memory_mb': 1932.2109375}\n",
      "Step 300: {'train_loss': 1.1052193641662598, 'lr': 2.6953455571227082e-05, 'memory_mb': 1932.2109375}\n",
      "Step 310: {'train_loss': 1.4185172319412231, 'lr': 2.6741889985895626e-05, 'memory_mb': 1932.2109375}\n",
      "Step 315: {'eval_loss': 1.3167022205889225, 'memory_mb': 1932.2109375}\n",
      "Step 320: {'train_loss': 1.1582015752792358, 'lr': 2.6530324400564176e-05, 'memory_mb': 1932.2109375}\n",
      "Step 330: {'train_loss': 1.2729072570800781, 'lr': 2.6318758815232723e-05, 'memory_mb': 1932.2109375}\n",
      "Step 340: {'train_loss': 1.2790855169296265, 'lr': 2.610719322990127e-05, 'memory_mb': 1932.2109375}\n",
      "Step 350: {'train_loss': 1.3149404525756836, 'lr': 2.5895627644569817e-05, 'memory_mb': 1932.2109375}\n",
      "Step 360: {'train_loss': 1.188058614730835, 'lr': 2.5684062059238364e-05, 'memory_mb': 1932.2109375}\n",
      "Step 370: {'train_loss': 1.233466625213623, 'lr': 2.5472496473906914e-05, 'memory_mb': 1932.2109375}\n",
      "Step 378: {'eval_loss': 1.292384773492813, 'memory_mb': 1932.2109375}\n",
      "Step 380: {'train_loss': 1.3385019302368164, 'lr': 2.526093088857546e-05, 'memory_mb': 1932.2109375}\n",
      "Step 390: {'train_loss': 1.3340812921524048, 'lr': 2.5049365303244008e-05, 'memory_mb': 1932.2109375}\n",
      "Step 400: {'train_loss': 1.2885733842849731, 'lr': 2.4837799717912554e-05, 'memory_mb': 1932.2109375}\n",
      "Step 410: {'train_loss': 1.107496738433838, 'lr': 2.4626234132581098e-05, 'memory_mb': 1932.2109375}\n",
      "Step 420: {'train_loss': 1.2210811376571655, 'lr': 2.4414668547249648e-05, 'memory_mb': 1932.2109375}\n",
      "Step 430: {'train_loss': 1.3571120500564575, 'lr': 2.4203102961918195e-05, 'memory_mb': 1932.2109375}\n",
      "Step 440: {'train_loss': 1.28019118309021, 'lr': 2.3991537376586742e-05, 'memory_mb': 1932.2109375}\n",
      "Step 441: {'eval_loss': 1.2826189994812012, 'memory_mb': 1932.2109375}\n",
      "Step 450: {'train_loss': 1.1089674234390259, 'lr': 2.377997179125529e-05, 'memory_mb': 1932.2109375}\n",
      "Step 460: {'train_loss': 1.395235300064087, 'lr': 2.3568406205923836e-05, 'memory_mb': 1932.2109375}\n",
      "Step 470: {'train_loss': 1.160008430480957, 'lr': 2.3356840620592386e-05, 'memory_mb': 1932.2109375}\n",
      "Step 480: {'train_loss': 1.3432872295379639, 'lr': 2.3145275035260933e-05, 'memory_mb': 1932.2109375}\n",
      "Step 490: {'train_loss': 1.3452411890029907, 'lr': 2.293370944992948e-05, 'memory_mb': 1932.2109375}\n",
      "Step 500: {'train_loss': 1.5415629148483276, 'lr': 2.2722143864598027e-05, 'memory_mb': 1932.2109375}\n",
      "Step 504: {'eval_loss': 1.2763000763952732, 'memory_mb': 1932.2109375}\n",
      "Step 510: {'train_loss': 1.2650896310806274, 'lr': 2.2510578279266574e-05, 'memory_mb': 1932.2109375}\n",
      "Step 520: {'train_loss': 1.2759510278701782, 'lr': 2.229901269393512e-05, 'memory_mb': 1932.2109375}\n",
      "Step 530: {'train_loss': 1.0346125364303589, 'lr': 2.2087447108603668e-05, 'memory_mb': 1932.2109375}\n",
      "Step 540: {'train_loss': 1.071410059928894, 'lr': 2.1875881523272214e-05, 'memory_mb': 1932.2109375}\n",
      "Step 550: {'train_loss': 1.1702009439468384, 'lr': 2.166431593794076e-05, 'memory_mb': 1932.2109375}\n",
      "Step 560: {'train_loss': 1.263436198234558, 'lr': 2.1452750352609308e-05, 'memory_mb': 1932.2109375}\n",
      "Step 567: {'eval_loss': 1.269500270485878, 'memory_mb': 1932.2109375}\n",
      "Model converged at step 567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 1.2695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1932.21094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 1.26344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_warmup_20250425_230541\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/bfqu4ilh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_230541-bfqu4ilh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --full_ft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8999c",
   "metadata": {},
   "source": [
    "### Default LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687a148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_230938-iyekm8mu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_warmup_20250425_230938\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/iyekm8mu\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 6963.10 examples/s]\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 1.910828025477707e-06, 'memory_mb': 1919.9609375}\n",
      "Step 10: {'train_loss': 5.763973236083984, 'lr': 2.1019108280254774e-05, 'memory_mb': 1922.9609375}\n",
      "Step 20: {'train_loss': 2.1714437007904053, 'lr': 4.012738853503184e-05, 'memory_mb': 1923.4609375}\n",
      "Step 30: {'train_loss': 1.571169376373291, 'lr': 5.923566878980892e-05, 'memory_mb': 1923.4609375}\n",
      "Step 40: {'train_loss': 1.651560664176941, 'lr': 7.834394904458598e-05, 'memory_mb': 1923.4609375}\n",
      "Step 50: {'train_loss': 1.4756532907485962, 'lr': 9.745222929936304e-05, 'memory_mb': 1923.4609375}\n",
      "Step 60: {'train_loss': 1.521204948425293, 'lr': 0.00011656050955414013, 'memory_mb': 1923.4609375}\n",
      "Step 63: {'eval_loss': 1.5298958159983158, 'memory_mb': 1925.4609375}\n",
      "Step 70: {'train_loss': 1.5514768362045288, 'lr': 0.0001356687898089172, 'memory_mb': 1925.9609375}\n",
      "Step 80: {'train_loss': 1.6301926374435425, 'lr': 0.00015477707006369425, 'memory_mb': 1925.9609375}\n",
      "Step 90: {'train_loss': 1.271825909614563, 'lr': 0.00017388535031847131, 'memory_mb': 1925.9609375}\n",
      "Step 100: {'train_loss': 1.0974262952804565, 'lr': 0.00019299363057324838, 'memory_mb': 1925.9609375}\n",
      "Step 110: {'train_loss': 1.1210205554962158, 'lr': 0.00021210191082802544, 'memory_mb': 1925.9609375}\n",
      "Step 120: {'train_loss': 1.3542817831039429, 'lr': 0.00023121019108280255, 'memory_mb': 1925.9609375}\n",
      "Step 126: {'eval_loss': 1.2722922451794147, 'memory_mb': 1925.9609375}\n",
      "Step 130: {'train_loss': 1.238128662109375, 'lr': 0.0002503184713375796, 'memory_mb': 1925.9609375}\n",
      "Step 140: {'train_loss': 1.0632641315460205, 'lr': 0.0002694267515923567, 'memory_mb': 1925.9609375}\n",
      "Step 150: {'train_loss': 1.2629445791244507, 'lr': 0.00028853503184713374, 'memory_mb': 1925.9609375}\n",
      "Step 160: {'train_loss': 1.0018690824508667, 'lr': 0.0002991537376586742, 'memory_mb': 1925.9609375}\n",
      "Step 170: {'train_loss': 1.3016855716705322, 'lr': 0.00029703808180535964, 'memory_mb': 1925.9609375}\n",
      "Step 180: {'train_loss': 1.2094979286193848, 'lr': 0.0002949224259520451, 'memory_mb': 1925.9609375}\n",
      "Step 189: {'eval_loss': 1.2504314668476582, 'memory_mb': 1925.9609375}\n",
      "Step 190: {'train_loss': 1.3578013181686401, 'lr': 0.0002928067700987306, 'memory_mb': 1925.9609375}\n",
      "Step 200: {'train_loss': 1.1937857866287231, 'lr': 0.00029069111424541606, 'memory_mb': 1925.9609375}\n",
      "Step 210: {'train_loss': 0.998962938785553, 'lr': 0.0002885754583921015, 'memory_mb': 1925.9609375}\n",
      "Step 220: {'train_loss': 1.0615793466567993, 'lr': 0.000286459802538787, 'memory_mb': 1925.9609375}\n",
      "Step 230: {'train_loss': 1.031130313873291, 'lr': 0.0002843441466854725, 'memory_mb': 1925.9609375}\n",
      "Step 240: {'train_loss': 1.265354871749878, 'lr': 0.00028222849083215794, 'memory_mb': 1925.9609375}\n",
      "Step 250: {'train_loss': 1.0588356256484985, 'lr': 0.0002801128349788434, 'memory_mb': 1925.9609375}\n",
      "Step 252: {'eval_loss': 1.2560312412679195, 'memory_mb': 1925.9609375}\n",
      "Step 260: {'train_loss': 0.9387083053588867, 'lr': 0.0002779971791255289, 'memory_mb': 1925.9609375}\n",
      "Step 270: {'train_loss': 1.4291609525680542, 'lr': 0.00027588152327221436, 'memory_mb': 1925.9609375}\n",
      "Step 280: {'train_loss': 1.2793831825256348, 'lr': 0.0002737658674188998, 'memory_mb': 1925.9609375}\n",
      "Step 290: {'train_loss': 1.028131365776062, 'lr': 0.0002716502115655853, 'memory_mb': 1925.9609375}\n",
      "Step 300: {'train_loss': 0.9331284165382385, 'lr': 0.0002695345557122708, 'memory_mb': 1925.9609375}\n",
      "Step 310: {'train_loss': 1.169629454612732, 'lr': 0.00026741889985895624, 'memory_mb': 1925.9609375}\n",
      "Step 315: {'eval_loss': 1.2706949561834335, 'memory_mb': 1925.9609375}\n",
      "Step 320: {'train_loss': 0.8692542314529419, 'lr': 0.0002653032440056417, 'memory_mb': 1925.9609375}\n",
      "Step 330: {'train_loss': 1.0288076400756836, 'lr': 0.0002631875881523272, 'memory_mb': 1925.9609375}\n",
      "Step 340: {'train_loss': 0.9833772778511047, 'lr': 0.00026107193229901266, 'memory_mb': 1925.9609375}\n",
      "Step 350: {'train_loss': 1.046581506729126, 'lr': 0.00025895627644569817, 'memory_mb': 1925.9609375}\n",
      "Step 360: {'train_loss': 0.942162036895752, 'lr': 0.0002568406205923836, 'memory_mb': 1925.9609375}\n",
      "Step 370: {'train_loss': 0.9910576939582825, 'lr': 0.0002547249647390691, 'memory_mb': 1925.9609375}\n",
      "Step 378: {'eval_loss': 1.3079698346555233, 'memory_mb': 1925.9609375}\n",
      "Step 380: {'train_loss': 0.8894038796424866, 'lr': 0.0002526093088857546, 'memory_mb': 1925.9609375}\n",
      "Step 390: {'train_loss': 0.940487265586853, 'lr': 0.00025049365303244004, 'memory_mb': 1925.9609375}\n",
      "Step 400: {'train_loss': 0.8640848994255066, 'lr': 0.0002483779971791255, 'memory_mb': 1925.9609375}\n",
      "Step 410: {'train_loss': 0.8201399445533752, 'lr': 0.00024626234132581095, 'memory_mb': 1925.9609375}\n",
      "Step 420: {'train_loss': 0.888618528842926, 'lr': 0.00024414668547249646, 'memory_mb': 1925.9609375}\n",
      "Step 430: {'train_loss': 0.955543041229248, 'lr': 0.00024203102961918192, 'memory_mb': 1925.9609375}\n",
      "Step 440: {'train_loss': 0.9635226130485535, 'lr': 0.0002399153737658674, 'memory_mb': 1925.9609375}\n",
      "Step 441: {'eval_loss': 1.3451477363705635, 'memory_mb': 1925.9609375}\n",
      "Step 450: {'train_loss': 0.7354468703269958, 'lr': 0.00023779971791255286, 'memory_mb': 1925.9609375}\n",
      "Step 460: {'train_loss': 0.8998845815658569, 'lr': 0.00023568406205923834, 'memory_mb': 1925.9609375}\n",
      "Step 470: {'train_loss': 0.8278604745864868, 'lr': 0.00023356840620592382, 'memory_mb': 1925.9609375}\n",
      "Step 480: {'train_loss': 0.9594752192497253, 'lr': 0.0002314527503526093, 'memory_mb': 1925.9609375}\n",
      "Step 490: {'train_loss': 0.8607874512672424, 'lr': 0.00022933709449929476, 'memory_mb': 1925.9609375}\n",
      "Step 500: {'train_loss': 1.0665183067321777, 'lr': 0.00022722143864598024, 'memory_mb': 1925.9609375}\n",
      "Step 504: {'eval_loss': 1.3936623595654964, 'memory_mb': 1925.9609375}\n",
      "Step 510: {'train_loss': 0.7605859041213989, 'lr': 0.00022510578279266572, 'memory_mb': 1925.9609375}\n",
      "Step 520: {'train_loss': 0.7460939884185791, 'lr': 0.00022299012693935118, 'memory_mb': 1925.9609375}\n",
      "Step 530: {'train_loss': 0.691889762878418, 'lr': 0.00022087447108603663, 'memory_mb': 1925.9609375}\n",
      "Step 540: {'train_loss': 0.6790350675582886, 'lr': 0.00021875881523272212, 'memory_mb': 1925.9609375}\n",
      "Step 550: {'train_loss': 0.7383130788803101, 'lr': 0.0002166431593794076, 'memory_mb': 1925.9609375}\n",
      "Step 560: {'train_loss': 0.8175129294395447, 'lr': 0.00021452750352609308, 'memory_mb': 1925.9609375}\n",
      "Step 567: {'eval_loss': 1.4642434194684029, 'memory_mb': 1925.9609375}\n",
      "Step 570: {'train_loss': 0.6015049815177917, 'lr': 0.00021241184767277854, 'memory_mb': 1925.9609375}\n",
      "Step 580: {'train_loss': 0.6400661468505859, 'lr': 0.00021029619181946402, 'memory_mb': 1925.9609375}\n",
      "Step 590: {'train_loss': 0.6032886505126953, 'lr': 0.0002081805359661495, 'memory_mb': 1925.9609375}\n",
      "Step 600: {'train_loss': 0.6658974885940552, 'lr': 0.00020606488011283499, 'memory_mb': 1925.9609375}\n",
      "Step 610: {'train_loss': 0.6654263734817505, 'lr': 0.00020394922425952044, 'memory_mb': 1925.9609375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 620: {'train_loss': 0.7442575693130493, 'lr': 0.0002018335684062059, 'memory_mb': 1925.9609375}\n",
      "Step 630: {'eval_loss': 1.4920559860765934, 'memory_mb': 1925.9609375}\n",
      "Step 630: {'train_loss': 0.7674885392189026, 'lr': 0.00019971791255289138, 'memory_mb': 1925.9609375}\n",
      "Step 640: {'train_loss': 0.6077069640159607, 'lr': 0.00019760225669957683, 'memory_mb': 1925.9609375}\n",
      "Step 650: {'train_loss': 0.7439389824867249, 'lr': 0.00019548660084626232, 'memory_mb': 1925.9609375}\n",
      "Step 660: {'train_loss': 0.6667447090148926, 'lr': 0.0001933709449929478, 'memory_mb': 1925.9609375}\n",
      "Step 670: {'train_loss': 0.5299290418624878, 'lr': 0.00019125528913963328, 'memory_mb': 1925.9609375}\n",
      "Step 680: {'train_loss': 0.7051672339439392, 'lr': 0.00018913963328631874, 'memory_mb': 1925.9609375}\n",
      "Step 690: {'train_loss': 0.5717016458511353, 'lr': 0.00018702397743300422, 'memory_mb': 1925.9609375}\n",
      "Step 693: {'eval_loss': 1.5515686944127083, 'memory_mb': 1925.9609375}\n",
      "Step 700: {'train_loss': 0.5231149196624756, 'lr': 0.0001849083215796897, 'memory_mb': 1925.9609375}\n",
      "Step 710: {'train_loss': 0.5368736982345581, 'lr': 0.00018279266572637518, 'memory_mb': 1925.9609375}\n",
      "Step 720: {'train_loss': 0.6085618138313293, 'lr': 0.0001806770098730606, 'memory_mb': 1925.9609375}\n",
      "Step 730: {'train_loss': 0.6303466558456421, 'lr': 0.0001785613540197461, 'memory_mb': 1925.9609375}\n",
      "Step 740: {'train_loss': 0.5160287618637085, 'lr': 0.00017644569816643158, 'memory_mb': 1925.9609375}\n",
      "Step 750: {'train_loss': 0.4651196002960205, 'lr': 0.00017433004231311706, 'memory_mb': 1925.9609375}\n",
      "Step 756: {'eval_loss': 1.5880375020205975, 'memory_mb': 1925.9609375}\n",
      "Step 760: {'train_loss': 0.4628816843032837, 'lr': 0.00017221438645980252, 'memory_mb': 1925.9609375}\n",
      "Step 770: {'train_loss': 0.4007032811641693, 'lr': 0.000170098730606488, 'memory_mb': 1925.9609375}\n",
      "Step 780: {'train_loss': 0.404011607170105, 'lr': 0.00016798307475317348, 'memory_mb': 1925.9609375}\n",
      "Step 790: {'train_loss': 0.45427292585372925, 'lr': 0.00016586741889985896, 'memory_mb': 1925.9609375}\n",
      "Step 800: {'train_loss': 0.5167211294174194, 'lr': 0.00016375176304654442, 'memory_mb': 1925.9609375}\n",
      "Step 810: {'train_loss': 0.5267351865768433, 'lr': 0.0001616361071932299, 'memory_mb': 1925.9609375}\n",
      "Step 819: {'eval_loss': 1.6393628977239132, 'memory_mb': 1925.9609375}\n",
      "Step 820: {'train_loss': 0.4809999167919159, 'lr': 0.00015952045133991536, 'memory_mb': 1925.9609375}\n",
      "Step 830: {'train_loss': 0.35635581612586975, 'lr': 0.0001574047954866008, 'memory_mb': 1925.9609375}\n",
      "Step 840: {'train_loss': 0.455806165933609, 'lr': 0.0001552891396332863, 'memory_mb': 1925.9609375}\n",
      "Step 850: {'train_loss': 0.4136482775211334, 'lr': 0.00015317348377997178, 'memory_mb': 1925.9609375}\n",
      "Step 860: {'train_loss': 0.35930541157722473, 'lr': 0.00015105782792665726, 'memory_mb': 1925.9609375}\n",
      "Step 870: {'train_loss': 0.49653756618499756, 'lr': 0.00014894217207334271, 'memory_mb': 1925.9609375}\n",
      "Step 880: {'train_loss': 0.42729511857032776, 'lr': 0.0001468265162200282, 'memory_mb': 1925.9609375}\n",
      "Step 882: {'eval_loss': 1.727712295949459, 'memory_mb': 1925.9609375}\n",
      "Step 890: {'train_loss': 0.44464996457099915, 'lr': 0.00014471086036671368, 'memory_mb': 1925.9609375}\n",
      "Step 900: {'train_loss': 0.3535679578781128, 'lr': 0.00014259520451339913, 'memory_mb': 1925.9609375}\n",
      "Step 910: {'train_loss': 0.4868026077747345, 'lr': 0.00014047954866008462, 'memory_mb': 1925.9609375}\n",
      "Step 920: {'train_loss': 0.35156914591789246, 'lr': 0.0001383638928067701, 'memory_mb': 1925.9609375}\n",
      "Step 930: {'train_loss': 0.4183633327484131, 'lr': 0.00013624823695345556, 'memory_mb': 1925.9609375}\n",
      "Step 940: {'train_loss': 0.3688618838787079, 'lr': 0.00013413258110014104, 'memory_mb': 1925.9609375}\n",
      "Step 945: {'eval_loss': 1.790421724319458, 'memory_mb': 1925.9609375}\n",
      "Step 950: {'train_loss': 0.32271718978881836, 'lr': 0.0001320169252468265, 'memory_mb': 1925.9609375}\n",
      "Step 960: {'train_loss': 0.3975195288658142, 'lr': 0.00012990126939351198, 'memory_mb': 1925.9609375}\n",
      "Step 970: {'train_loss': 0.3845232129096985, 'lr': 0.00012778561354019746, 'memory_mb': 1925.9609375}\n",
      "Step 980: {'train_loss': 0.33040758967399597, 'lr': 0.00012566995768688291, 'memory_mb': 1925.9609375}\n",
      "Step 990: {'train_loss': 0.3474535048007965, 'lr': 0.0001235543018335684, 'memory_mb': 1925.9609375}\n",
      "Step 1000: {'train_loss': 0.414598286151886, 'lr': 0.00012143864598025387, 'memory_mb': 1925.9609375}\n",
      "Step 1008: {'eval_loss': 1.8894339203834534, 'memory_mb': 1925.9609375}\n",
      "Step 1010: {'train_loss': 0.27986276149749756, 'lr': 0.00011932299012693933, 'memory_mb': 1925.9609375}\n",
      "Step 1020: {'train_loss': 0.28143471479415894, 'lr': 0.00011720733427362482, 'memory_mb': 1925.9609375}\n",
      "Step 1030: {'train_loss': 0.26589617133140564, 'lr': 0.00011509167842031029, 'memory_mb': 1925.9609375}\n",
      "Step 1040: {'train_loss': 0.36606988310813904, 'lr': 0.00011297602256699577, 'memory_mb': 1925.9609375}\n",
      "Step 1050: {'train_loss': 0.2881600856781006, 'lr': 0.00011086036671368122, 'memory_mb': 1925.9609375}\n",
      "Step 1060: {'train_loss': 0.4089137315750122, 'lr': 0.0001087447108603667, 'memory_mb': 1925.9609375}\n",
      "Step 1070: {'train_loss': 0.2862825095653534, 'lr': 0.00010662905500705217, 'memory_mb': 1925.9609375}\n",
      "Step 1071: {'eval_loss': 1.885521788150072, 'memory_mb': 1925.9609375}\n",
      "Step 1080: {'train_loss': 0.24885712563991547, 'lr': 0.00010451339915373766, 'memory_mb': 1925.9609375}\n",
      "Step 1090: {'train_loss': 0.2809503674507141, 'lr': 0.00010239774330042313, 'memory_mb': 1925.9609375}\n",
      "Step 1100: {'train_loss': 0.2402976006269455, 'lr': 0.0001002820874471086, 'memory_mb': 1925.9609375}\n",
      "Step 1110: {'train_loss': 0.31323161721229553, 'lr': 9.816643159379406e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1120: {'train_loss': 0.3406546413898468, 'lr': 9.605077574047955e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1130: {'train_loss': 0.35375720262527466, 'lr': 9.393511988716502e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1134: {'eval_loss': 1.9510527588427067, 'memory_mb': 1925.9609375}\n",
      "Step 1140: {'train_loss': 0.24377308785915375, 'lr': 9.18194640338505e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1150: {'train_loss': 0.27880600094795227, 'lr': 8.970380818053595e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1160: {'train_loss': 0.2812533378601074, 'lr': 8.758815232722142e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1170: {'train_loss': 0.32361528277397156, 'lr': 8.54724964739069e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1180: {'train_loss': 0.22514380514621735, 'lr': 8.335684062059237e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1190: {'train_loss': 0.26808616518974304, 'lr': 8.124118476727786e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1197: {'eval_loss': 2.0183958411216736, 'memory_mb': 1925.9609375}\n",
      "Step 1200: {'train_loss': 0.20912611484527588, 'lr': 7.912552891396331e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1210: {'train_loss': 0.2380668818950653, 'lr': 7.70098730606488e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1220: {'train_loss': 0.2719453275203705, 'lr': 7.489421720733426e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1230: {'train_loss': 0.20964404940605164, 'lr': 7.277856135401975e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1240: {'train_loss': 0.23244476318359375, 'lr': 7.066290550070521e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1250: {'train_loss': 0.24903611838817596, 'lr': 6.854724964739068e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1260: {'eval_loss': 2.069428738206625, 'memory_mb': 1925.9609375}\n",
      "Step 1260: {'train_loss': 0.24344585835933685, 'lr': 6.643159379407615e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1270: {'train_loss': 0.20874597132205963, 'lr': 6.431593794076164e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1280: {'train_loss': 0.21832072734832764, 'lr': 6.22002820874471e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1290: {'train_loss': 0.18594956398010254, 'lr': 6.008462623413257e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1300: {'train_loss': 0.22513259947299957, 'lr': 5.796897038081805e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1310: {'train_loss': 0.22441807389259338, 'lr': 5.585331452750352e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1320: {'train_loss': 0.2770097255706787, 'lr': 5.3737658674188994e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1323: {'eval_loss': 2.1204216852784157, 'memory_mb': 1925.9609375}\n",
      "Step 1330: {'train_loss': 0.24191173911094666, 'lr': 5.162200282087447e-05, 'memory_mb': 1925.9609375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1340: {'train_loss': 0.21054458618164062, 'lr': 4.950634696755994e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1350: {'train_loss': 0.17733171582221985, 'lr': 4.7390691114245414e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1360: {'train_loss': 0.2013244777917862, 'lr': 4.527503526093088e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1370: {'train_loss': 0.2492770254611969, 'lr': 4.315937940761636e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1380: {'train_loss': 0.20711469650268555, 'lr': 4.1043723554301835e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1386: {'eval_loss': 2.1343016140162945, 'memory_mb': 1925.9609375}\n",
      "Step 1390: {'train_loss': 0.1799113005399704, 'lr': 3.8928067700987303e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1400: {'train_loss': 0.16764283180236816, 'lr': 3.681241184767278e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1410: {'train_loss': 0.23294661939144135, 'lr': 3.469675599435825e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1420: {'train_loss': 0.16415534913539886, 'lr': 3.258110014104372e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1430: {'train_loss': 0.19022223353385925, 'lr': 3.0465444287729193e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1440: {'train_loss': 0.18067710101604462, 'lr': 2.834978843441467e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1449: {'eval_loss': 2.1654319651424885, 'memory_mb': 1925.9609375}\n",
      "Step 1450: {'train_loss': 0.16888999938964844, 'lr': 2.623413258110014e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1460: {'train_loss': 0.21226109564304352, 'lr': 2.411847672778561e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1470: {'train_loss': 0.1997634917497635, 'lr': 2.2002820874471082e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1480: {'train_loss': 0.20179201662540436, 'lr': 1.9887165021156555e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1490: {'train_loss': 0.21341155469417572, 'lr': 1.777150916784203e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1500: {'train_loss': 0.1961541622877121, 'lr': 1.5655853314527503e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1510: {'train_loss': 0.18310171365737915, 'lr': 1.3540197461212975e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1512: {'eval_loss': 2.1993900537490845, 'memory_mb': 1925.9609375}\n",
      "Step 1520: {'train_loss': 0.18547984957695007, 'lr': 1.1424541607898449e-05, 'memory_mb': 1925.9609375}\n",
      "Step 1530: {'train_loss': 0.16534090042114258, 'lr': 9.30888575458392e-06, 'memory_mb': 1925.9609375}\n",
      "Step 1540: {'train_loss': 0.1980300098657608, 'lr': 7.193229901269393e-06, 'memory_mb': 1925.9609375}\n",
      "Step 1550: {'train_loss': 0.2622081935405731, 'lr': 5.077574047954866e-06, 'memory_mb': 1925.9609375}\n",
      "Step 1560: {'train_loss': 0.22274170815944672, 'lr': 2.9619181946403384e-06, 'memory_mb': 1925.9609375}\n",
      "Step 1570: {'train_loss': 0.20701345801353455, 'lr': 8.462623413258109e-07, 'memory_mb': 1925.9609375}\n",
      "Step 1575: {'eval_loss': 2.2138357125222683, 'memory_mb': 1925.9609375}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.21384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1925.96094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.20701\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_warmup_20250425_230938\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/iyekm8mu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_230938-iyekm8mu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4f173",
   "metadata": {},
   "source": [
    "### AdaFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b7f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c21533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_231202-ioo0qlqw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adafactor_warmup_20250425_231202\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/ioo0qlqw\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 1.910828025477707e-06, 'memory_mb': 1894.484375}\n",
      "Step 10: {'train_loss': 5.864461421966553, 'lr': 2.1019108280254774e-05, 'memory_mb': 1897.484375}\n",
      "Step 20: {'train_loss': 2.141554355621338, 'lr': 4.012738853503184e-05, 'memory_mb': 1897.484375}\n",
      "Step 30: {'train_loss': 1.5617411136627197, 'lr': 5.923566878980892e-05, 'memory_mb': 1897.984375}\n",
      "Step 40: {'train_loss': 1.6335724592208862, 'lr': 7.834394904458598e-05, 'memory_mb': 1897.984375}\n",
      "Step 50: {'train_loss': 1.4347587823867798, 'lr': 9.745222929936304e-05, 'memory_mb': 1897.984375}\n",
      "Step 60: {'train_loss': 1.40921151638031, 'lr': 0.00011656050955414013, 'memory_mb': 1897.984375}\n",
      "Step 63: {'eval_loss': 1.4056437276303768, 'memory_mb': 1899.984375}\n",
      "Step 70: {'train_loss': 1.4062975645065308, 'lr': 0.0001356687898089172, 'memory_mb': 1900.484375}\n",
      "Step 80: {'train_loss': 1.5274022817611694, 'lr': 0.00015477707006369425, 'memory_mb': 1900.484375}\n",
      "Step 90: {'train_loss': 1.2466450929641724, 'lr': 0.00017388535031847131, 'memory_mb': 1900.484375}\n",
      "Step 100: {'train_loss': 1.080156922340393, 'lr': 0.00019299363057324838, 'memory_mb': 1900.484375}\n",
      "Step 110: {'train_loss': 1.1077446937561035, 'lr': 0.00021210191082802544, 'memory_mb': 1900.484375}\n",
      "Step 120: {'train_loss': 1.3353103399276733, 'lr': 0.00023121019108280255, 'memory_mb': 1900.484375}\n",
      "Step 126: {'eval_loss': 1.2632965818047523, 'memory_mb': 1900.484375}\n",
      "Step 130: {'train_loss': 1.2353026866912842, 'lr': 0.0002503184713375796, 'memory_mb': 1900.484375}\n",
      "Step 140: {'train_loss': 1.0526645183563232, 'lr': 0.0002694267515923567, 'memory_mb': 1900.484375}\n",
      "Step 150: {'train_loss': 1.2653878927230835, 'lr': 0.00028853503184713374, 'memory_mb': 1900.484375}\n",
      "Step 160: {'train_loss': 0.9889137148857117, 'lr': 0.0002991537376586742, 'memory_mb': 1900.484375}\n",
      "Step 170: {'train_loss': 1.3022089004516602, 'lr': 0.00029703808180535964, 'memory_mb': 1900.484375}\n",
      "Step 180: {'train_loss': 1.224352478981018, 'lr': 0.0002949224259520451, 'memory_mb': 1900.484375}\n",
      "Step 189: {'eval_loss': 1.2452905848622322, 'memory_mb': 1900.484375}\n",
      "Step 190: {'train_loss': 1.321227788925171, 'lr': 0.0002928067700987306, 'memory_mb': 1900.484375}\n",
      "Step 200: {'train_loss': 1.1669801473617554, 'lr': 0.00029069111424541606, 'memory_mb': 1900.484375}\n",
      "Step 210: {'train_loss': 0.9788072109222412, 'lr': 0.0002885754583921015, 'memory_mb': 1900.484375}\n",
      "Step 220: {'train_loss': 1.0115394592285156, 'lr': 0.000286459802538787, 'memory_mb': 1900.484375}\n",
      "Step 230: {'train_loss': 1.015604019165039, 'lr': 0.0002843441466854725, 'memory_mb': 1900.484375}\n",
      "Step 240: {'train_loss': 1.2372801303863525, 'lr': 0.00028222849083215794, 'memory_mb': 1900.484375}\n",
      "Step 250: {'train_loss': 1.0386042594909668, 'lr': 0.0002801128349788434, 'memory_mb': 1900.484375}\n",
      "Step 252: {'eval_loss': 1.2486250549554825, 'memory_mb': 1900.484375}\n",
      "Step 260: {'train_loss': 0.9034035801887512, 'lr': 0.0002779971791255289, 'memory_mb': 1900.484375}\n",
      "Step 270: {'train_loss': 1.383389949798584, 'lr': 0.00027588152327221436, 'memory_mb': 1900.484375}\n",
      "Step 280: {'train_loss': 1.2206814289093018, 'lr': 0.0002737658674188998, 'memory_mb': 1900.484375}\n",
      "Step 290: {'train_loss': 0.990480363368988, 'lr': 0.0002716502115655853, 'memory_mb': 1900.484375}\n",
      "Step 300: {'train_loss': 0.9123401641845703, 'lr': 0.0002695345557122708, 'memory_mb': 1900.484375}\n",
      "Step 310: {'train_loss': 1.1155602931976318, 'lr': 0.00026741889985895624, 'memory_mb': 1900.484375}\n",
      "Step 315: {'eval_loss': 1.2758971322327852, 'memory_mb': 1900.484375}\n",
      "Step 320: {'train_loss': 0.7980073094367981, 'lr': 0.0002653032440056417, 'memory_mb': 1900.484375}\n",
      "Step 330: {'train_loss': 0.9709855914115906, 'lr': 0.0002631875881523272, 'memory_mb': 1900.484375}\n",
      "Step 340: {'train_loss': 0.9150084853172302, 'lr': 0.00026107193229901266, 'memory_mb': 1900.484375}\n",
      "Step 350: {'train_loss': 0.9837941527366638, 'lr': 0.00025895627644569817, 'memory_mb': 1900.484375}\n",
      "Step 360: {'train_loss': 0.8802359700202942, 'lr': 0.0002568406205923836, 'memory_mb': 1900.484375}\n",
      "Step 370: {'train_loss': 0.9259052276611328, 'lr': 0.0002547249647390691, 'memory_mb': 1900.484375}\n",
      "Step 378: {'eval_loss': 1.3181858658790588, 'memory_mb': 1900.484375}\n",
      "Step 380: {'train_loss': 0.791117250919342, 'lr': 0.0002526093088857546, 'memory_mb': 1900.484375}\n",
      "Step 390: {'train_loss': 0.8389186859130859, 'lr': 0.00025049365303244004, 'memory_mb': 1900.484375}\n",
      "Step 400: {'train_loss': 0.7577562928199768, 'lr': 0.0002483779971791255, 'memory_mb': 1900.484375}\n",
      "Step 410: {'train_loss': 0.7275086641311646, 'lr': 0.00024626234132581095, 'memory_mb': 1900.484375}\n",
      "Step 420: {'train_loss': 0.7958507537841797, 'lr': 0.00024414668547249646, 'memory_mb': 1900.484375}\n",
      "Step 430: {'train_loss': 0.8410547971725464, 'lr': 0.00024203102961918192, 'memory_mb': 1900.484375}\n",
      "Step 440: {'train_loss': 0.8349927067756653, 'lr': 0.0002399153737658674, 'memory_mb': 1900.484375}\n",
      "Step 441: {'eval_loss': 1.366280984133482, 'memory_mb': 1900.484375}\n",
      "Step 450: {'train_loss': 0.6344716548919678, 'lr': 0.00023779971791255286, 'memory_mb': 1900.484375}\n",
      "Step 460: {'train_loss': 0.7781540155410767, 'lr': 0.00023568406205923834, 'memory_mb': 1900.484375}\n",
      "Step 470: {'train_loss': 0.7024422883987427, 'lr': 0.00023356840620592382, 'memory_mb': 1900.484375}\n",
      "Step 480: {'train_loss': 0.8119259476661682, 'lr': 0.0002314527503526093, 'memory_mb': 1900.484375}\n",
      "Step 490: {'train_loss': 0.7320166230201721, 'lr': 0.00022933709449929476, 'memory_mb': 1900.484375}\n",
      "Step 500: {'train_loss': 0.9003109931945801, 'lr': 0.00022722143864598024, 'memory_mb': 1900.484375}\n",
      "Step 504: {'eval_loss': 1.37960284948349, 'memory_mb': 1900.484375}\n",
      "Step 510: {'train_loss': 0.6040736436843872, 'lr': 0.00022510578279266572, 'memory_mb': 1900.484375}\n",
      "Step 520: {'train_loss': 0.5950070023536682, 'lr': 0.00022299012693935118, 'memory_mb': 1900.484375}\n",
      "Step 530: {'train_loss': 0.5602172017097473, 'lr': 0.00022087447108603663, 'memory_mb': 1900.484375}\n",
      "Step 540: {'train_loss': 0.5494680404663086, 'lr': 0.00021875881523272212, 'memory_mb': 1900.484375}\n",
      "Step 550: {'train_loss': 0.5878658294677734, 'lr': 0.0002166431593794076, 'memory_mb': 1900.484375}\n",
      "Step 560: {'train_loss': 0.6534217596054077, 'lr': 0.00021452750352609308, 'memory_mb': 1900.484375}\n",
      "Step 567: {'eval_loss': 1.4808790981769562, 'memory_mb': 1900.484375}\n",
      "Step 570: {'train_loss': 0.4779457747936249, 'lr': 0.00021241184767277854, 'memory_mb': 1900.484375}\n",
      "Step 580: {'train_loss': 0.48845231533050537, 'lr': 0.00021029619181946402, 'memory_mb': 1900.484375}\n",
      "Step 590: {'train_loss': 0.470582515001297, 'lr': 0.0002081805359661495, 'memory_mb': 1900.484375}\n",
      "Step 600: {'train_loss': 0.5219550132751465, 'lr': 0.00020606488011283499, 'memory_mb': 1900.484375}\n",
      "Step 610: {'train_loss': 0.5199611783027649, 'lr': 0.00020394922425952044, 'memory_mb': 1900.484375}\n",
      "Step 620: {'train_loss': 0.5806573033332825, 'lr': 0.0002018335684062059, 'memory_mb': 1900.484375}\n",
      "Step 630: {'eval_loss': 1.5663153007626534, 'memory_mb': 1900.484375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 630: {'train_loss': 0.5934141874313354, 'lr': 0.00019971791255289138, 'memory_mb': 1900.484375}\n",
      "Step 640: {'train_loss': 0.48304465413093567, 'lr': 0.00019760225669957683, 'memory_mb': 1900.484375}\n",
      "Step 650: {'train_loss': 0.5887879729270935, 'lr': 0.00019548660084626232, 'memory_mb': 1900.484375}\n",
      "Step 660: {'train_loss': 0.49997690320014954, 'lr': 0.0001933709449929478, 'memory_mb': 1900.484375}\n",
      "Step 670: {'train_loss': 0.387896865606308, 'lr': 0.00019125528913963328, 'memory_mb': 1900.484375}\n",
      "Step 680: {'train_loss': 0.5576250553131104, 'lr': 0.00018913963328631874, 'memory_mb': 1900.484375}\n",
      "Step 690: {'train_loss': 0.43313875794410706, 'lr': 0.00018702397743300422, 'memory_mb': 1900.484375}\n",
      "Step 693: {'eval_loss': 1.6103229373693466, 'memory_mb': 1900.484375}\n",
      "Step 700: {'train_loss': 0.4077020287513733, 'lr': 0.0001849083215796897, 'memory_mb': 1900.484375}\n",
      "Step 710: {'train_loss': 0.40049979090690613, 'lr': 0.00018279266572637518, 'memory_mb': 1900.484375}\n",
      "Step 720: {'train_loss': 0.5108124613761902, 'lr': 0.0001806770098730606, 'memory_mb': 1900.484375}\n",
      "Step 730: {'train_loss': 0.49010568857192993, 'lr': 0.0001785613540197461, 'memory_mb': 1900.484375}\n",
      "Step 740: {'train_loss': 0.4125206768512726, 'lr': 0.00017644569816643158, 'memory_mb': 1900.484375}\n",
      "Step 750: {'train_loss': 0.383923202753067, 'lr': 0.00017433004231311706, 'memory_mb': 1900.484375}\n",
      "Step 756: {'eval_loss': 1.694899719208479, 'memory_mb': 1900.484375}\n",
      "Step 760: {'train_loss': 0.36536869406700134, 'lr': 0.00017221438645980252, 'memory_mb': 1900.484375}\n",
      "Step 770: {'train_loss': 0.3139825165271759, 'lr': 0.000170098730606488, 'memory_mb': 1900.484375}\n",
      "Step 780: {'train_loss': 0.3142494559288025, 'lr': 0.00016798307475317348, 'memory_mb': 1900.484375}\n",
      "Step 790: {'train_loss': 0.3539499044418335, 'lr': 0.00016586741889985896, 'memory_mb': 1900.484375}\n",
      "Step 800: {'train_loss': 0.408812552690506, 'lr': 0.00016375176304654442, 'memory_mb': 1900.484375}\n",
      "Step 810: {'train_loss': 0.4239409565925598, 'lr': 0.0001616361071932299, 'memory_mb': 1900.484375}\n",
      "Step 819: {'eval_loss': 1.7878648042678833, 'memory_mb': 1900.484375}\n",
      "Step 820: {'train_loss': 0.37065085768699646, 'lr': 0.00015952045133991536, 'memory_mb': 1900.484375}\n",
      "Step 830: {'train_loss': 0.28176864981651306, 'lr': 0.0001574047954866008, 'memory_mb': 1900.484375}\n",
      "Step 840: {'train_loss': 0.3799794018268585, 'lr': 0.0001552891396332863, 'memory_mb': 1900.484375}\n",
      "Step 850: {'train_loss': 0.33536216616630554, 'lr': 0.00015317348377997178, 'memory_mb': 1900.484375}\n",
      "Step 860: {'train_loss': 0.2723320722579956, 'lr': 0.00015105782792665726, 'memory_mb': 1900.484375}\n",
      "Step 870: {'train_loss': 0.3912706673145294, 'lr': 0.00014894217207334271, 'memory_mb': 1900.484375}\n",
      "Step 880: {'train_loss': 0.3575925827026367, 'lr': 0.0001468265162200282, 'memory_mb': 1900.484375}\n",
      "Step 882: {'eval_loss': 1.8223964385688305, 'memory_mb': 1900.484375}\n",
      "Step 890: {'train_loss': 0.3357698917388916, 'lr': 0.00014471086036671368, 'memory_mb': 1900.484375}\n",
      "Step 900: {'train_loss': 0.28877145051956177, 'lr': 0.00014259520451339913, 'memory_mb': 1900.484375}\n",
      "Step 910: {'train_loss': 0.3842203915119171, 'lr': 0.00014047954866008462, 'memory_mb': 1900.484375}\n",
      "Step 920: {'train_loss': 0.28765952587127686, 'lr': 0.0001383638928067701, 'memory_mb': 1900.484375}\n",
      "Step 930: {'train_loss': 0.3356771171092987, 'lr': 0.00013624823695345556, 'memory_mb': 1900.484375}\n",
      "Step 940: {'train_loss': 0.28658294677734375, 'lr': 0.00013413258110014104, 'memory_mb': 1900.484375}\n",
      "Step 945: {'eval_loss': 1.8962909951806068, 'memory_mb': 1900.484375}\n",
      "Step 950: {'train_loss': 0.2655118405818939, 'lr': 0.0001320169252468265, 'memory_mb': 1900.484375}\n",
      "Step 960: {'train_loss': 0.29986512660980225, 'lr': 0.00012990126939351198, 'memory_mb': 1900.484375}\n",
      "Step 970: {'train_loss': 0.31489500403404236, 'lr': 0.00012778561354019746, 'memory_mb': 1900.484375}\n",
      "Step 980: {'train_loss': 0.27205565571784973, 'lr': 0.00012566995768688291, 'memory_mb': 1900.484375}\n",
      "Step 990: {'train_loss': 0.26152345538139343, 'lr': 0.0001235543018335684, 'memory_mb': 1900.484375}\n",
      "Step 1000: {'train_loss': 0.3146788477897644, 'lr': 0.00012143864598025387, 'memory_mb': 1900.484375}\n",
      "Step 1008: {'eval_loss': 1.959705475717783, 'memory_mb': 1900.484375}\n",
      "Step 1010: {'train_loss': 0.22915710508823395, 'lr': 0.00011932299012693933, 'memory_mb': 1900.484375}\n",
      "Step 1020: {'train_loss': 0.2095339298248291, 'lr': 0.00011720733427362482, 'memory_mb': 1900.484375}\n",
      "Step 1030: {'train_loss': 0.21413224935531616, 'lr': 0.00011509167842031029, 'memory_mb': 1900.484375}\n",
      "Step 1040: {'train_loss': 0.2803170382976532, 'lr': 0.00011297602256699577, 'memory_mb': 1900.484375}\n",
      "Step 1050: {'train_loss': 0.2323058843612671, 'lr': 0.00011086036671368122, 'memory_mb': 1900.484375}\n",
      "Step 1060: {'train_loss': 0.32454973459243774, 'lr': 0.0001087447108603667, 'memory_mb': 1900.484375}\n",
      "Step 1070: {'train_loss': 0.2393537163734436, 'lr': 0.00010662905500705217, 'memory_mb': 1900.484375}\n",
      "Step 1071: {'eval_loss': 2.0164313800632954, 'memory_mb': 1900.484375}\n",
      "Step 1080: {'train_loss': 0.18942047655582428, 'lr': 0.00010451339915373766, 'memory_mb': 1900.484375}\n",
      "Step 1090: {'train_loss': 0.21880891919136047, 'lr': 0.00010239774330042313, 'memory_mb': 1900.484375}\n",
      "Step 1100: {'train_loss': 0.19415389001369476, 'lr': 0.0001002820874471086, 'memory_mb': 1900.484375}\n",
      "Step 1110: {'train_loss': 0.23332056403160095, 'lr': 9.816643159379406e-05, 'memory_mb': 1900.484375}\n",
      "Step 1120: {'train_loss': 0.2740306556224823, 'lr': 9.605077574047955e-05, 'memory_mb': 1900.484375}\n",
      "Step 1130: {'train_loss': 0.25975120067596436, 'lr': 9.393511988716502e-05, 'memory_mb': 1900.484375}\n",
      "Step 1134: {'eval_loss': 2.1185930557549, 'memory_mb': 1900.484375}\n",
      "Step 1140: {'train_loss': 0.18826624751091003, 'lr': 9.18194640338505e-05, 'memory_mb': 1900.484375}\n",
      "Step 1150: {'train_loss': 0.20275099575519562, 'lr': 8.970380818053595e-05, 'memory_mb': 1900.484375}\n",
      "Step 1160: {'train_loss': 0.22801125049591064, 'lr': 8.758815232722142e-05, 'memory_mb': 1900.484375}\n",
      "Step 1170: {'train_loss': 0.23348483443260193, 'lr': 8.54724964739069e-05, 'memory_mb': 1900.484375}\n",
      "Step 1180: {'train_loss': 0.16826941072940826, 'lr': 8.335684062059237e-05, 'memory_mb': 1900.484375}\n",
      "Step 1190: {'train_loss': 0.20202986896038055, 'lr': 8.124118476727786e-05, 'memory_mb': 1900.484375}\n",
      "Step 1197: {'eval_loss': 2.1568007580935955, 'memory_mb': 1900.484375}\n",
      "Step 1200: {'train_loss': 0.15901920199394226, 'lr': 7.912552891396331e-05, 'memory_mb': 1900.484375}\n",
      "Step 1210: {'train_loss': 0.19301152229309082, 'lr': 7.70098730606488e-05, 'memory_mb': 1900.484375}\n",
      "Step 1220: {'train_loss': 0.21396800875663757, 'lr': 7.489421720733426e-05, 'memory_mb': 1900.484375}\n",
      "Step 1230: {'train_loss': 0.16078774631023407, 'lr': 7.277856135401975e-05, 'memory_mb': 1900.484375}\n",
      "Step 1240: {'train_loss': 0.20125727355480194, 'lr': 7.066290550070521e-05, 'memory_mb': 1900.484375}\n",
      "Step 1250: {'train_loss': 0.19450265169143677, 'lr': 6.854724964739068e-05, 'memory_mb': 1900.484375}\n",
      "Step 1260: {'eval_loss': 2.224408693611622, 'memory_mb': 1900.484375}\n",
      "Step 1260: {'train_loss': 0.1898832768201828, 'lr': 6.643159379407615e-05, 'memory_mb': 1900.484375}\n",
      "Step 1270: {'train_loss': 0.16357572376728058, 'lr': 6.431593794076164e-05, 'memory_mb': 1900.484375}\n",
      "Step 1280: {'train_loss': 0.15092429518699646, 'lr': 6.22002820874471e-05, 'memory_mb': 1900.484375}\n",
      "Step 1290: {'train_loss': 0.15341036021709442, 'lr': 6.008462623413257e-05, 'memory_mb': 1900.484375}\n",
      "Step 1300: {'train_loss': 0.17448262870311737, 'lr': 5.796897038081805e-05, 'memory_mb': 1900.484375}\n",
      "Step 1310: {'train_loss': 0.16644613444805145, 'lr': 5.585331452750352e-05, 'memory_mb': 1900.484375}\n",
      "Step 1320: {'train_loss': 0.21385124325752258, 'lr': 5.3737658674188994e-05, 'memory_mb': 1900.484375}\n",
      "Step 1323: {'eval_loss': 2.2429928332567215, 'memory_mb': 1900.484375}\n",
      "Step 1330: {'train_loss': 0.17955224215984344, 'lr': 5.162200282087447e-05, 'memory_mb': 1900.484375}\n",
      "Step 1340: {'train_loss': 0.1609681099653244, 'lr': 4.950634696755994e-05, 'memory_mb': 1900.484375}\n",
      "Step 1350: {'train_loss': 0.1469791829586029, 'lr': 4.7390691114245414e-05, 'memory_mb': 1900.484375}\n",
      "Step 1360: {'train_loss': 0.15462170541286469, 'lr': 4.527503526093088e-05, 'memory_mb': 1900.484375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1370: {'train_loss': 0.17563344538211823, 'lr': 4.315937940761636e-05, 'memory_mb': 1900.484375}\n",
      "Step 1380: {'train_loss': 0.15855532884597778, 'lr': 4.1043723554301835e-05, 'memory_mb': 1900.484375}\n",
      "Step 1386: {'eval_loss': 2.2796414121985435, 'memory_mb': 1900.484375}\n",
      "Step 1390: {'train_loss': 0.13408243656158447, 'lr': 3.8928067700987303e-05, 'memory_mb': 1900.484375}\n",
      "Step 1400: {'train_loss': 0.12960411608219147, 'lr': 3.681241184767278e-05, 'memory_mb': 1900.484375}\n",
      "Step 1410: {'train_loss': 0.1636231541633606, 'lr': 3.469675599435825e-05, 'memory_mb': 1900.484375}\n",
      "Step 1420: {'train_loss': 0.1409711241722107, 'lr': 3.258110014104372e-05, 'memory_mb': 1900.484375}\n",
      "Step 1430: {'train_loss': 0.145439013838768, 'lr': 3.0465444287729193e-05, 'memory_mb': 1900.484375}\n",
      "Step 1440: {'train_loss': 0.14280185103416443, 'lr': 2.834978843441467e-05, 'memory_mb': 1900.484375}\n",
      "Step 1449: {'eval_loss': 2.316496428102255, 'memory_mb': 1900.484375}\n",
      "Step 1450: {'train_loss': 0.14141133427619934, 'lr': 2.623413258110014e-05, 'memory_mb': 1900.484375}\n",
      "Step 1460: {'train_loss': 0.15017199516296387, 'lr': 2.411847672778561e-05, 'memory_mb': 1900.484375}\n",
      "Step 1470: {'train_loss': 0.1541443169116974, 'lr': 2.2002820874471082e-05, 'memory_mb': 1900.484375}\n",
      "Step 1480: {'train_loss': 0.15332935750484467, 'lr': 1.9887165021156555e-05, 'memory_mb': 1900.484375}\n",
      "Step 1490: {'train_loss': 0.1598522961139679, 'lr': 1.777150916784203e-05, 'memory_mb': 1900.484375}\n",
      "Step 1500: {'train_loss': 0.1434251070022583, 'lr': 1.5655853314527503e-05, 'memory_mb': 1900.484375}\n",
      "Step 1510: {'train_loss': 0.1310914158821106, 'lr': 1.3540197461212975e-05, 'memory_mb': 1900.484375}\n",
      "Step 1512: {'eval_loss': 2.3448946699500084, 'memory_mb': 1900.484375}\n",
      "Step 1520: {'train_loss': 0.13575716316699982, 'lr': 1.1424541607898449e-05, 'memory_mb': 1900.484375}\n",
      "Step 1530: {'train_loss': 0.12255629897117615, 'lr': 9.30888575458392e-06, 'memory_mb': 1900.484375}\n",
      "Step 1540: {'train_loss': 0.14281751215457916, 'lr': 7.193229901269393e-06, 'memory_mb': 1900.484375}\n",
      "Step 1550: {'train_loss': 0.16875170171260834, 'lr': 5.077574047954866e-06, 'memory_mb': 1900.484375}\n",
      "Step 1560: {'train_loss': 0.16921286284923553, 'lr': 2.9619181946403384e-06, 'memory_mb': 1900.484375}\n",
      "Step 1570: {'train_loss': 0.14364419877529144, 'lr': 8.462623413258109e-07, 'memory_mb': 1900.484375}\n",
      "Step 1575: {'eval_loss': 2.3579820692539215, 'memory_mb': 1900.484375}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.35798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1900.48438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.14364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adafactor_warmup_20250425_231202\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/ioo0qlqw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_231202-ioo0qlqw/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adafactor \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b956f6",
   "metadata": {},
   "source": [
    "### Lion Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbe68e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_233359-sjaz6a12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_lion_warmup_20250425_233359\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/sjaz6a12\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 1.910828025477707e-06, 'memory_mb': 1838.03515625}\n",
      "Step 10: {'train_loss': 5.733637809753418, 'lr': 2.1019108280254774e-05, 'memory_mb': 1841.03515625}\n",
      "Step 20: {'train_loss': 2.149186849594116, 'lr': 4.012738853503184e-05, 'memory_mb': 1841.03515625}\n",
      "Step 30: {'train_loss': 1.6155903339385986, 'lr': 5.923566878980892e-05, 'memory_mb': 1841.03515625}\n",
      "Step 40: {'train_loss': 1.697494387626648, 'lr': 7.834394904458598e-05, 'memory_mb': 1841.53515625}\n",
      "Step 50: {'train_loss': 1.5384424924850464, 'lr': 9.745222929936304e-05, 'memory_mb': 1841.53515625}\n",
      "Step 60: {'train_loss': 1.6902098655700684, 'lr': 0.00011656050955414013, 'memory_mb': 1841.53515625}\n",
      "Step 63: {'eval_loss': 1.6852670647203922, 'memory_mb': 1843.03515625}\n",
      "Step 70: {'train_loss': 1.6535409688949585, 'lr': 0.0001356687898089172, 'memory_mb': 1843.03515625}\n",
      "Step 80: {'train_loss': 1.821605920791626, 'lr': 0.00015477707006369425, 'memory_mb': 1843.53515625}\n",
      "Step 90: {'train_loss': 1.540727138519287, 'lr': 0.00017388535031847131, 'memory_mb': 1843.53515625}\n",
      "Step 100: {'train_loss': 1.3510594367980957, 'lr': 0.00019299363057324838, 'memory_mb': 1843.53515625}\n",
      "Step 110: {'train_loss': 1.3832452297210693, 'lr': 0.00021210191082802544, 'memory_mb': 1843.53515625}\n",
      "Step 120: {'train_loss': 1.7370119094848633, 'lr': 0.00023121019108280255, 'memory_mb': 1843.53515625}\n",
      "Step 126: {'eval_loss': 1.6225841753184795, 'memory_mb': 1843.53515625}\n",
      "Step 130: {'train_loss': 1.6671569347381592, 'lr': 0.0002503184713375796, 'memory_mb': 1843.53515625}\n",
      "Step 140: {'train_loss': 1.5404188632965088, 'lr': 0.0002694267515923567, 'memory_mb': 1843.53515625}\n",
      "Step 150: {'train_loss': 2.1852831840515137, 'lr': 0.00028853503184713374, 'memory_mb': 1843.53515625}\n",
      "Step 160: {'train_loss': 3.1353132724761963, 'lr': 0.0002991537376586742, 'memory_mb': 1843.53515625}\n",
      "Step 170: {'train_loss': 4.5763773918151855, 'lr': 0.00029703808180535964, 'memory_mb': 1843.53515625}\n",
      "Step 180: {'train_loss': 11.114727973937988, 'lr': 0.0002949224259520451, 'memory_mb': 1843.53515625}\n",
      "Step 189: {'eval_loss': 6.134319335222244, 'memory_mb': 1843.53515625}\n",
      "Step 190: {'train_loss': 6.0610504150390625, 'lr': 0.0002928067700987306, 'memory_mb': 1843.53515625}\n",
      "Step 200: {'train_loss': 5.709181785583496, 'lr': 0.00029069111424541606, 'memory_mb': 1843.53515625}\n",
      "Step 210: {'train_loss': 4.945626735687256, 'lr': 0.0002885754583921015, 'memory_mb': 1843.53515625}\n",
      "Step 220: {'train_loss': 5.164736270904541, 'lr': 0.000286459802538787, 'memory_mb': 1843.53515625}\n",
      "Step 230: {'train_loss': 4.482004642486572, 'lr': 0.0002843441466854725, 'memory_mb': 1843.53515625}\n",
      "Step 240: {'train_loss': 5.126322269439697, 'lr': 0.00028222849083215794, 'memory_mb': 1843.53515625}\n",
      "Step 250: {'train_loss': 4.621854782104492, 'lr': 0.0002801128349788434, 'memory_mb': 1843.53515625}\n",
      "Step 252: {'eval_loss': 5.046182289719582, 'memory_mb': 1843.53515625}\n",
      "Step 260: {'train_loss': 5.038348197937012, 'lr': 0.0002779971791255289, 'memory_mb': 1843.53515625}\n",
      "Step 270: {'train_loss': 6.538571834564209, 'lr': 0.00027588152327221436, 'memory_mb': 1843.53515625}\n",
      "Step 280: {'train_loss': 7.059004306793213, 'lr': 0.0002737658674188998, 'memory_mb': 1843.53515625}\n",
      "Step 290: {'train_loss': 6.940411567687988, 'lr': 0.0002716502115655853, 'memory_mb': 1843.53515625}\n",
      "Step 300: {'train_loss': 7.447687149047852, 'lr': 0.0002695345557122708, 'memory_mb': 1843.53515625}\n",
      "Step 310: {'train_loss': 8.317461013793945, 'lr': 0.00026741889985895624, 'memory_mb': 1843.53515625}\n",
      "Step 315: {'eval_loss': 8.648238271474838, 'memory_mb': 1843.53515625}\n",
      "Step 320: {'train_loss': 8.70328140258789, 'lr': 0.0002653032440056417, 'memory_mb': 1843.53515625}\n",
      "Step 330: {'train_loss': 9.460816383361816, 'lr': 0.0002631875881523272, 'memory_mb': 1843.53515625}\n",
      "Step 340: {'train_loss': 9.956376075744629, 'lr': 0.00026107193229901266, 'memory_mb': 1843.53515625}\n",
      "Step 350: {'train_loss': 10.504048347473145, 'lr': 0.00025895627644569817, 'memory_mb': 1843.53515625}\n",
      "Step 360: {'train_loss': 10.867449760437012, 'lr': 0.0002568406205923836, 'memory_mb': 1843.53515625}\n",
      "Step 370: {'train_loss': 11.216150283813477, 'lr': 0.0002547249647390691, 'memory_mb': 1843.53515625}\n",
      "Step 378: {'eval_loss': 11.591013461351395, 'memory_mb': 1843.53515625}\n",
      "Step 380: {'train_loss': 11.711969375610352, 'lr': 0.0002526093088857546, 'memory_mb': 1843.53515625}\n",
      "Step 390: {'train_loss': 11.99674129486084, 'lr': 0.00025049365303244004, 'memory_mb': 1843.53515625}\n",
      "Step 400: {'train_loss': 12.18330192565918, 'lr': 0.0002483779971791255, 'memory_mb': 1843.53515625}\n",
      "Step 410: {'train_loss': 12.301066398620605, 'lr': 0.00024626234132581095, 'memory_mb': 1843.53515625}\n",
      "Step 420: {'train_loss': nan, 'lr': 0.00024414668547249646, 'memory_mb': 1843.53515625}\n",
      "Step 430: {'train_loss': 12.420022964477539, 'lr': 0.00024203102961918192, 'memory_mb': 1843.53515625}\n",
      "Step 440: {'train_loss': 12.573171615600586, 'lr': 0.0002399153737658674, 'memory_mb': 1843.53515625}\n",
      "Step 441: {'eval_loss': 12.510140925645828, 'memory_mb': 1843.53515625}\n",
      "Step 450: {'train_loss': nan, 'lr': 0.00023779971791255286, 'memory_mb': 1843.53515625}\n",
      "Step 460: {'train_loss': nan, 'lr': 0.00023568406205923834, 'memory_mb': 1843.53515625}\n",
      "Step 470: {'train_loss': nan, 'lr': 0.00023356840620592382, 'memory_mb': 1843.53515625}\n",
      "Step 480: {'train_loss': 12.372885704040527, 'lr': 0.0002314527503526093, 'memory_mb': 1843.53515625}\n",
      "Step 490: {'train_loss': 12.533585548400879, 'lr': 0.00022933709449929476, 'memory_mb': 1843.53515625}\n",
      "Step 500: {'train_loss': 12.584762573242188, 'lr': 0.00022722143864598024, 'memory_mb': 1843.53515625}\n",
      "Step 504: {'eval_loss': 12.51065680384636, 'memory_mb': 1843.53515625}\n",
      "Step 510: {'train_loss': 12.414994239807129, 'lr': 0.00022510578279266572, 'memory_mb': 1843.53515625}\n",
      "Step 520: {'train_loss': 12.536117553710938, 'lr': 0.00022299012693935118, 'memory_mb': 1843.53515625}\n",
      "Step 530: {'train_loss': 12.50011157989502, 'lr': 0.00022087447108603663, 'memory_mb': 1843.53515625}\n",
      "Step 540: {'train_loss': 12.602237701416016, 'lr': 0.00021875881523272212, 'memory_mb': 1843.53515625}\n",
      "Step 550: {'train_loss': nan, 'lr': 0.0002166431593794076, 'memory_mb': 1843.53515625}\n",
      "Step 560: {'train_loss': 12.439166069030762, 'lr': 0.00021452750352609308, 'memory_mb': 1843.53515625}\n",
      "Step 567: {'eval_loss': 12.51467677950859, 'memory_mb': 1843.53515625}\n",
      "Step 570: {'train_loss': 12.253807067871094, 'lr': 0.00021241184767277854, 'memory_mb': 1843.53515625}\n",
      "Step 580: {'train_loss': 12.456741333007812, 'lr': 0.00021029619181946402, 'memory_mb': 1843.53515625}\n",
      "Step 590: {'train_loss': 12.420241355895996, 'lr': 0.0002081805359661495, 'memory_mb': 1843.53515625}\n",
      "Step 600: {'train_loss': 12.502800941467285, 'lr': 0.00020606488011283499, 'memory_mb': 1843.53515625}\n",
      "Step 610: {'train_loss': 12.346318244934082, 'lr': 0.00020394922425952044, 'memory_mb': 1843.53515625}\n",
      "Step 620: {'train_loss': 12.501660346984863, 'lr': 0.0002018335684062059, 'memory_mb': 1843.53515625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 630: {'eval_loss': 12.513765454292297, 'memory_mb': 1843.53515625}\n",
      "Step 630: {'train_loss': 12.534062385559082, 'lr': 0.00019971791255289138, 'memory_mb': 1843.53515625}\n",
      "Step 640: {'train_loss': 12.428526878356934, 'lr': 0.00019760225669957683, 'memory_mb': 1843.53515625}\n",
      "Step 650: {'train_loss': nan, 'lr': 0.00019548660084626232, 'memory_mb': 1843.53515625}\n",
      "Step 660: {'train_loss': 12.523898124694824, 'lr': 0.0001933709449929478, 'memory_mb': 1843.53515625}\n",
      "Step 670: {'train_loss': nan, 'lr': 0.00019125528913963328, 'memory_mb': 1843.53515625}\n",
      "Step 680: {'train_loss': 12.61072063446045, 'lr': 0.00018913963328631874, 'memory_mb': 1843.53515625}\n",
      "Step 690: {'train_loss': 12.448634147644043, 'lr': 0.00018702397743300422, 'memory_mb': 1843.53515625}\n",
      "Step 693: {'eval_loss': 12.511861354112625, 'memory_mb': 1843.53515625}\n",
      "Step 700: {'train_loss': 12.641640663146973, 'lr': 0.0001849083215796897, 'memory_mb': 1843.53515625}\n",
      "Step 710: {'train_loss': 12.574226379394531, 'lr': 0.00018279266572637518, 'memory_mb': 1843.53515625}\n",
      "Step 720: {'train_loss': 12.49355697631836, 'lr': 0.0001806770098730606, 'memory_mb': 1843.53515625}\n",
      "Step 730: {'train_loss': 12.537474632263184, 'lr': 0.0001785613540197461, 'memory_mb': 1843.53515625}\n",
      "Step 740: {'train_loss': 12.505058288574219, 'lr': 0.00017644569816643158, 'memory_mb': 1843.53515625}\n",
      "Step 750: {'train_loss': 12.436006546020508, 'lr': 0.00017433004231311706, 'memory_mb': 1843.53515625}\n",
      "Step 756: {'eval_loss': 12.513458162546158, 'memory_mb': 1843.53515625}\n",
      "Step 760: {'train_loss': 12.632925033569336, 'lr': 0.00017221438645980252, 'memory_mb': 1843.53515625}\n",
      "Step 770: {'train_loss': 12.656673431396484, 'lr': 0.000170098730606488, 'memory_mb': 1843.53515625}\n",
      "Step 780: {'train_loss': 12.44336986541748, 'lr': 0.00016798307475317348, 'memory_mb': 1843.53515625}\n",
      "Step 790: {'train_loss': 12.420703887939453, 'lr': 0.00016586741889985896, 'memory_mb': 1843.53515625}\n",
      "Step 800: {'train_loss': nan, 'lr': 0.00016375176304654442, 'memory_mb': 1843.53515625}\n",
      "Step 810: {'train_loss': 12.536924362182617, 'lr': 0.0001616361071932299, 'memory_mb': 1843.53515625}\n",
      "Step 819: {'eval_loss': 12.516781955957413, 'memory_mb': 1843.53515625}\n",
      "Step 820: {'train_loss': 12.637726783752441, 'lr': 0.00015952045133991536, 'memory_mb': 1843.53515625}\n",
      "Step 830: {'train_loss': 12.50306224822998, 'lr': 0.0001574047954866008, 'memory_mb': 1843.53515625}\n",
      "Step 840: {'train_loss': 12.642155647277832, 'lr': 0.0001552891396332863, 'memory_mb': 1843.53515625}\n",
      "Step 850: {'train_loss': nan, 'lr': 0.00015317348377997178, 'memory_mb': 1843.53515625}\n",
      "Step 860: {'train_loss': 12.69499397277832, 'lr': 0.00015105782792665726, 'memory_mb': 1843.53515625}\n",
      "Step 870: {'train_loss': 12.457173347473145, 'lr': 0.00014894217207334271, 'memory_mb': 1843.53515625}\n",
      "Step 880: {'train_loss': 12.539850234985352, 'lr': 0.0001468265162200282, 'memory_mb': 1843.53515625}\n",
      "Step 882: {'eval_loss': 12.513638228178024, 'memory_mb': 1843.53515625}\n",
      "Step 890: {'train_loss': nan, 'lr': 0.00014471086036671368, 'memory_mb': 1843.53515625}\n",
      "Step 900: {'train_loss': 12.278756141662598, 'lr': 0.00014259520451339913, 'memory_mb': 1843.53515625}\n",
      "Step 910: {'train_loss': 12.565910339355469, 'lr': 0.00014047954866008462, 'memory_mb': 1843.53515625}\n",
      "Step 920: {'train_loss': 12.5555419921875, 'lr': 0.0001383638928067701, 'memory_mb': 1843.53515625}\n",
      "Step 930: {'train_loss': 12.524531364440918, 'lr': 0.00013624823695345556, 'memory_mb': 1843.53515625}\n",
      "Step 940: {'train_loss': nan, 'lr': 0.00013413258110014104, 'memory_mb': 1843.53515625}\n",
      "Step 945: {'eval_loss': 12.514163881540298, 'memory_mb': 1843.53515625}\n",
      "Step 950: {'train_loss': 12.484928131103516, 'lr': 0.0001320169252468265, 'memory_mb': 1843.53515625}\n",
      "Step 960: {'train_loss': 12.570466995239258, 'lr': 0.00012990126939351198, 'memory_mb': 1843.53515625}\n",
      "Step 970: {'train_loss': 12.670683860778809, 'lr': 0.00012778561354019746, 'memory_mb': 1843.53515625}\n",
      "Step 980: {'train_loss': 12.490499496459961, 'lr': 0.00012566995768688291, 'memory_mb': 1843.53515625}\n",
      "Step 990: {'train_loss': 12.406110763549805, 'lr': 0.0001235543018335684, 'memory_mb': 1843.53515625}\n",
      "Step 1000: {'train_loss': 12.630427360534668, 'lr': 0.00012143864598025387, 'memory_mb': 1843.53515625}\n",
      "Step 1008: {'eval_loss': 12.513449668884277, 'memory_mb': 1843.53515625}\n",
      "Step 1010: {'train_loss': 12.320938110351562, 'lr': 0.00011932299012693933, 'memory_mb': 1843.53515625}\n",
      "Step 1020: {'train_loss': 12.514711380004883, 'lr': 0.00011720733427362482, 'memory_mb': 1843.53515625}\n",
      "Step 1030: {'train_loss': 12.476094245910645, 'lr': 0.00011509167842031029, 'memory_mb': 1843.53515625}\n",
      "Step 1040: {'train_loss': 12.675568580627441, 'lr': 0.00011297602256699577, 'memory_mb': 1843.53515625}\n",
      "Step 1050: {'train_loss': 12.4358491897583, 'lr': 0.00011086036671368122, 'memory_mb': 1843.53515625}\n",
      "Step 1060: {'train_loss': 12.478801727294922, 'lr': 0.0001087447108603667, 'memory_mb': 1843.53515625}\n",
      "Step 1070: {'train_loss': 12.513968467712402, 'lr': 0.00010662905500705217, 'memory_mb': 1843.53515625}\n",
      "Step 1071: {'eval_loss': 12.516233921051025, 'memory_mb': 1843.53515625}\n",
      "Step 1080: {'train_loss': 12.400077819824219, 'lr': 0.00010451339915373766, 'memory_mb': 1843.53515625}\n",
      "Step 1090: {'train_loss': 12.521183967590332, 'lr': 0.00010239774330042313, 'memory_mb': 1843.53515625}\n",
      "Step 1100: {'train_loss': 12.427544593811035, 'lr': 0.0001002820874471086, 'memory_mb': 1843.53515625}\n",
      "Step 1110: {'train_loss': 12.628812789916992, 'lr': 9.816643159379406e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1120: {'train_loss': 12.643645286560059, 'lr': 9.605077574047955e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1130: {'train_loss': 12.610854148864746, 'lr': 9.393511988716502e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1134: {'eval_loss': 12.513113170862198, 'memory_mb': 1843.53515625}\n",
      "Step 1140: {'train_loss': 12.491889953613281, 'lr': 9.18194640338505e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1150: {'train_loss': 12.654918670654297, 'lr': 8.970380818053595e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1160: {'train_loss': 12.498737335205078, 'lr': 8.758815232722142e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1170: {'train_loss': 12.570026397705078, 'lr': 8.54724964739069e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1180: {'train_loss': 12.391798973083496, 'lr': 8.335684062059237e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1190: {'train_loss': 12.512126922607422, 'lr': 8.124118476727786e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1197: {'eval_loss': 12.516900837421417, 'memory_mb': 1843.53515625}\n",
      "Step 1200: {'train_loss': 12.626836776733398, 'lr': 7.912552891396331e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1210: {'train_loss': 12.621801376342773, 'lr': 7.70098730606488e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1220: {'train_loss': 12.59134292602539, 'lr': 7.489421720733426e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1230: {'train_loss': 12.36992359161377, 'lr': 7.277856135401975e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1240: {'train_loss': 12.678548812866211, 'lr': 7.066290550070521e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1250: {'train_loss': 12.606422424316406, 'lr': 6.854724964739068e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1260: {'eval_loss': 12.514712244272232, 'memory_mb': 1843.53515625}\n",
      "Step 1260: {'train_loss': 12.530393600463867, 'lr': 6.643159379407615e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1270: {'train_loss': 12.470699310302734, 'lr': 6.431593794076164e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1280: {'train_loss': 12.483342170715332, 'lr': 6.22002820874471e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1290: {'train_loss': 12.44648551940918, 'lr': 6.008462623413257e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1300: {'train_loss': 12.39907169342041, 'lr': 5.796897038081805e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1310: {'train_loss': 12.567639350891113, 'lr': 5.585331452750352e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1320: {'train_loss': 12.557661056518555, 'lr': 5.3737658674188994e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1323: {'eval_loss': 12.515674948692322, 'memory_mb': 1843.53515625}\n",
      "Step 1330: {'train_loss': 12.505584716796875, 'lr': 5.162200282087447e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1340: {'train_loss': 12.371163368225098, 'lr': 4.950634696755994e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1350: {'train_loss': 12.376498222351074, 'lr': 4.7390691114245414e-05, 'memory_mb': 1843.53515625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1360: {'train_loss': 12.490527153015137, 'lr': 4.527503526093088e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1370: {'train_loss': 12.551071166992188, 'lr': 4.315937940761636e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1380: {'train_loss': 12.571438789367676, 'lr': 4.1043723554301835e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1386: {'eval_loss': 12.515569180250168, 'memory_mb': 1843.53515625}\n",
      "Step 1390: {'train_loss': 12.55479621887207, 'lr': 3.8928067700987303e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1400: {'train_loss': 12.35531234741211, 'lr': 3.681241184767278e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1410: {'train_loss': 12.593299865722656, 'lr': 3.469675599435825e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1420: {'train_loss': 12.443942070007324, 'lr': 3.258110014104372e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1430: {'train_loss': 12.543289184570312, 'lr': 3.0465444287729193e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1440: {'train_loss': nan, 'lr': 2.834978843441467e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1449: {'eval_loss': 12.5158169567585, 'memory_mb': 1843.53515625}\n",
      "Step 1450: {'train_loss': 12.673176765441895, 'lr': 2.623413258110014e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1460: {'train_loss': 12.496259689331055, 'lr': 2.411847672778561e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1470: {'train_loss': 12.513909339904785, 'lr': 2.2002820874471082e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1480: {'train_loss': 12.596720695495605, 'lr': 1.9887165021156555e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1490: {'train_loss': 12.600136756896973, 'lr': 1.777150916784203e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1500: {'train_loss': 12.535536766052246, 'lr': 1.5655853314527503e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1510: {'train_loss': 12.616707801818848, 'lr': 1.3540197461212975e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1512: {'eval_loss': 12.515869200229645, 'memory_mb': 1843.53515625}\n",
      "Step 1520: {'train_loss': nan, 'lr': 1.1424541607898449e-05, 'memory_mb': 1843.53515625}\n",
      "Step 1530: {'train_loss': 12.658019065856934, 'lr': 9.30888575458392e-06, 'memory_mb': 1843.53515625}\n",
      "Step 1540: {'train_loss': 12.432085037231445, 'lr': 7.193229901269393e-06, 'memory_mb': 1843.53515625}\n",
      "Step 1550: {'train_loss': 12.534499168395996, 'lr': 5.077574047954866e-06, 'memory_mb': 1843.53515625}\n",
      "Step 1560: {'train_loss': 12.448269844055176, 'lr': 2.9619181946403384e-06, 'memory_mb': 1843.53515625}\n",
      "Step 1570: {'train_loss': nan, 'lr': 8.462623413258109e-07, 'memory_mb': 1843.53515625}\n",
      "Step 1575: {'eval_loss': 12.51607921719551, 'memory_mb': 1843.53515625}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñá‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà ‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 12.51608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1843.53516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss nan\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_lion_warmup_20250425_233359\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/sjaz6a12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_233359-sjaz6a12/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer lion \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c329f83",
   "metadata": {},
   "source": [
    "### CosineAnnealing LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91db027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_231951-qg9m70ou\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_cosine_20250425_231951\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/qg9m70ou\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 0.0002999997015993612, 'memory_mb': 1898.109375}\n",
      "Step 10: {'train_loss': 1.695207118988037, 'lr': 0.0002999638949592407, 'memory_mb': 1901.109375}\n",
      "Step 20: {'train_loss': 1.4419267177581787, 'lr': 0.00029986842451482874, 'memory_mb': 1902.109375}\n",
      "Step 30: {'train_loss': 1.1626416444778442, 'lr': 0.0002997133282494673, 'memory_mb': 1902.109375}\n",
      "Step 40: {'train_loss': 1.2526087760925293, 'lr': 0.00029949866786889705, 'memory_mb': 1902.109375}\n",
      "Step 50: {'train_loss': 1.1675046682357788, 'lr': 0.0002992245287767077, 'memory_mb': 1902.109375}\n",
      "Step 60: {'train_loss': 1.2393745183944702, 'lr': 0.00029889102004035937, 'memory_mb': 1902.109375}\n",
      "Step 63: {'eval_loss': 1.265814758837223, 'memory_mb': 1904.109375}\n",
      "Step 70: {'train_loss': 1.2837313413619995, 'lr': 0.0002984982743477899, 'memory_mb': 1904.109375}\n",
      "Step 80: {'train_loss': 1.3620730638504028, 'lr': 0.00029804644795462435, 'memory_mb': 1904.109375}\n",
      "Step 90: {'train_loss': 1.1883715391159058, 'lr': 0.0002975357206220079, 'memory_mb': 1904.109375}\n",
      "Step 100: {'train_loss': 1.0395368337631226, 'lr': 0.00029696629554508706, 'memory_mb': 1904.109375}\n",
      "Step 110: {'train_loss': 1.05805504322052, 'lr': 0.0002963383992721679, 'memory_mb': 1904.109375}\n",
      "Step 120: {'train_loss': 1.2717976570129395, 'lr': 0.0002956522816145822, 'memory_mb': 1904.109375}\n",
      "Step 126: {'eval_loss': 1.247655376791954, 'memory_mb': 1904.109375}\n",
      "Step 130: {'train_loss': 1.1240681409835815, 'lr': 0.0002949082155472995, 'memory_mb': 1904.109375}\n",
      "Step 140: {'train_loss': 0.9606665372848511, 'lr': 0.00029410649710032236, 'memory_mb': 1904.109375}\n",
      "Step 150: {'train_loss': 1.1750657558441162, 'lr': 0.0002932474452409098, 'memory_mb': 1904.109375}\n",
      "Step 160: {'train_loss': 0.9110651612281799, 'lr': 0.00029233140174667445, 'memory_mb': 1904.109375}\n",
      "Step 170: {'train_loss': 1.2192552089691162, 'lr': 0.0002913587310696052, 'memory_mb': 1904.109375}\n",
      "Step 180: {'train_loss': 1.1315500736236572, 'lr': 0.000290329820191068, 'memory_mb': 1904.109375}\n",
      "Step 189: {'eval_loss': 1.2610814664512873, 'memory_mb': 1904.109375}\n",
      "Step 190: {'train_loss': 1.2240303754806519, 'lr': 0.0002892450784678436, 'memory_mb': 1904.109375}\n",
      "Step 200: {'train_loss': 1.0355861186981201, 'lr': 0.0002881049374692636, 'memory_mb': 1904.109375}\n",
      "Step 210: {'train_loss': 0.8808300495147705, 'lr': 0.000286909850805508, 'memory_mb': 1904.109375}\n",
      "Step 220: {'train_loss': 0.914503812789917, 'lr': 0.00028566029394713483, 'memory_mb': 1904.109375}\n",
      "Step 230: {'train_loss': 0.9031033515930176, 'lr': 0.0002843567640359119, 'memory_mb': 1904.109375}\n",
      "Step 240: {'train_loss': 1.1437952518463135, 'lr': 0.00028299977968702683, 'memory_mb': 1904.109375}\n",
      "Step 250: {'train_loss': 0.9534108638763428, 'lr': 0.0002815898807827535, 'memory_mb': 1904.109375}\n",
      "Step 252: {'eval_loss': 1.3005765043199062, 'memory_mb': 1904.109375}\n",
      "Step 260: {'train_loss': 0.7886708378791809, 'lr': 0.0002801276282576576, 'memory_mb': 1904.109375}\n",
      "Step 270: {'train_loss': 1.2158324718475342, 'lr': 0.0002786136038754264, 'memory_mb': 1904.109375}\n",
      "Step 280: {'train_loss': 1.0999689102172852, 'lr': 0.00027704840999741146, 'memory_mb': 1904.109375}\n",
      "Step 290: {'train_loss': 0.8541898727416992, 'lr': 0.00027543266934297605, 'memory_mb': 1904.109375}\n",
      "Step 300: {'train_loss': 0.8072702884674072, 'lr': 0.00027376702474174425, 'memory_mb': 1904.109375}\n",
      "Step 310: {'train_loss': 0.9949843287467957, 'lr': 0.00027205213887784754, 'memory_mb': 1904.109375}\n",
      "Step 315: {'eval_loss': 1.324414448812604, 'memory_mb': 1904.109375}\n",
      "Step 320: {'train_loss': 0.6541774868965149, 'lr': 0.00027028869402627356, 'memory_mb': 1904.109375}\n",
      "Step 330: {'train_loss': 0.8386363387107849, 'lr': 0.0002684773917814196, 'memory_mb': 1904.109375}\n",
      "Step 340: {'train_loss': 0.7776066660881042, 'lr': 0.00026661895277796025, 'memory_mb': 1904.109375}\n",
      "Step 350: {'train_loss': 0.8158940672874451, 'lr': 0.00026471411640413973, 'memory_mb': 1904.109375}\n",
      "Step 360: {'train_loss': 0.7377415895462036, 'lr': 0.000262763640507603, 'memory_mb': 1904.109375}\n",
      "Step 370: {'train_loss': 0.7893613576889038, 'lr': 0.00026076830109388255, 'memory_mb': 1904.109375}\n",
      "Step 378: {'eval_loss': 1.3452425058931112, 'memory_mb': 1904.109375}\n",
      "Step 380: {'train_loss': 0.6235345005989075, 'lr': 0.00025872889201766124, 'memory_mb': 1904.109375}\n",
      "Step 390: {'train_loss': 0.6807482838630676, 'lr': 0.00025664622466693373, 'memory_mb': 1904.109375}\n",
      "Step 400: {'train_loss': 0.6350759267807007, 'lr': 0.0002545211276401919, 'memory_mb': 1904.109375}\n",
      "Step 410: {'train_loss': 0.5915186405181885, 'lr': 0.0002523544464167637, 'memory_mb': 1904.109375}\n",
      "Step 420: {'train_loss': 0.6504191160202026, 'lr': 0.00025014704302043486, 'memory_mb': 1904.109375}\n",
      "Step 430: {'train_loss': 0.679956316947937, 'lr': 0.00024789979567648937, 'memory_mb': 1904.109375}\n",
      "Step 440: {'train_loss': 0.690863311290741, 'lr': 0.0002456135984623034, 'memory_mb': 1904.109375}\n",
      "Step 441: {'eval_loss': 1.413277193903923, 'memory_mb': 1904.109375}\n",
      "Step 450: {'train_loss': 0.522487223148346, 'lr': 0.00024328936095163273, 'memory_mb': 1904.109375}\n",
      "Step 460: {'train_loss': 0.6282026767730713, 'lr': 0.00024092800785273453, 'memory_mb': 1904.109375}\n",
      "Step 470: {'train_loss': 0.5839874148368835, 'lr': 0.00023853047864046841, 'memory_mb': 1904.109375}\n",
      "Step 480: {'train_loss': 0.6706454753875732, 'lr': 0.00023609772718252193, 'memory_mb': 1904.109375}\n",
      "Step 490: {'train_loss': 0.5970062613487244, 'lr': 0.00023363072135991017, 'memory_mb': 1904.109375}\n",
      "Step 500: {'train_loss': 0.7625090479850769, 'lr': 0.00023113044268189994, 'memory_mb': 1904.109375}\n",
      "Step 504: {'eval_loss': 1.483937069773674, 'memory_mb': 1904.109375}\n",
      "Step 510: {'train_loss': 0.4803909659385681, 'lr': 0.00022859788589551188, 'memory_mb': 1904.109375}\n",
      "Step 520: {'train_loss': 0.48802220821380615, 'lr': 0.00022603405858975597, 'memory_mb': 1904.109375}\n",
      "Step 530: {'train_loss': 0.4664035439491272, 'lr': 0.00022343998079475786, 'memory_mb': 1904.109375}\n",
      "Step 540: {'train_loss': 0.46757984161376953, 'lr': 0.0002208166845759351, 'memory_mb': 1904.109375}\n",
      "Step 550: {'train_loss': 0.5085008144378662, 'lr': 0.0002181652136233856, 'memory_mb': 1904.109375}\n",
      "Step 560: {'train_loss': 0.5308470726013184, 'lr': 0.0002154866228366505, 'memory_mb': 1904.109375}\n",
      "Step 567: {'eval_loss': 1.5588619224727154, 'memory_mb': 1904.109375}\n",
      "Step 570: {'train_loss': 0.3876580595970154, 'lr': 0.00021278197790501805, 'memory_mb': 1904.109375}\n",
      "Step 580: {'train_loss': 0.38728469610214233, 'lr': 0.00021005235488353428, 'memory_mb': 1904.109375}\n",
      "Step 590: {'train_loss': 0.37340572476387024, 'lr': 0.00020729883976488935, 'memory_mb': 1904.109375}\n",
      "Step 600: {'train_loss': 0.4160812795162201, 'lr': 0.00020452252804735124, 'memory_mb': 1904.109375}\n",
      "Step 610: {'train_loss': 0.42699897289276123, 'lr': 0.000201724524298916, 'memory_mb': 1904.109375}\n",
      "Step 620: {'train_loss': 0.492538720369339, 'lr': 0.00019890594171785125, 'memory_mb': 1904.109375}\n",
      "Step 630: {'eval_loss': 1.6470262594521046, 'memory_mb': 1904.109375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 630: {'train_loss': 0.48896872997283936, 'lr': 0.00019606790168980476, 'memory_mb': 1904.109375}\n",
      "Step 640: {'train_loss': 0.3734007477760315, 'lr': 0.0001932115333416564, 'memory_mb': 1904.109375}\n",
      "Step 650: {'train_loss': 0.461438924074173, 'lr': 0.00019033797309228983, 'memory_mb': 1904.109375}\n",
      "Step 660: {'train_loss': 0.4091918170452118, 'lr': 0.0001874483642004634, 'memory_mb': 1904.109375}\n",
      "Step 670: {'train_loss': 0.32454681396484375, 'lr': 0.00018454385630995926, 'memory_mb': 1904.109375}\n",
      "Step 680: {'train_loss': 0.4494182765483856, 'lr': 0.00018162560499219283, 'memory_mb': 1904.109375}\n",
      "Step 690: {'train_loss': 0.36269283294677734, 'lr': 0.00017869477128646316, 'memory_mb': 1904.109375}\n",
      "Step 693: {'eval_loss': 1.7563013583421707, 'memory_mb': 1904.109375}\n",
      "Step 700: {'train_loss': 0.3206930458545685, 'lr': 0.00017575252123802854, 'memory_mb': 1904.109375}\n",
      "Step 710: {'train_loss': 0.3302036225795746, 'lr': 0.0001728000254341901, 'memory_mb': 1904.109375}\n",
      "Step 720: {'train_loss': 0.3649424612522125, 'lr': 0.00016983845853856837, 'memory_mb': 1904.109375}\n",
      "Step 730: {'train_loss': 0.37559258937835693, 'lr': 0.00016686899882375835, 'memory_mb': 1904.109375}\n",
      "Step 740: {'train_loss': 0.3486781418323517, 'lr': 0.0001638928277025482, 'memory_mb': 1904.109375}\n",
      "Step 750: {'train_loss': 0.28690195083618164, 'lr': 0.00016091112925788916, 'memory_mb': 1904.109375}\n",
      "Step 756: {'eval_loss': 1.8139797784388065, 'memory_mb': 1904.109375}\n",
      "Step 760: {'train_loss': 0.27477142214775085, 'lr': 0.0001579250897718026, 'memory_mb': 1904.109375}\n",
      "Step 770: {'train_loss': 0.2543851137161255, 'lr': 0.00015493589725341277, 'memory_mb': 1904.109375}\n",
      "Step 780: {'train_loss': 0.24959108233451843, 'lr': 0.00015194474096629176, 'memory_mb': 1904.109375}\n",
      "Step 790: {'train_loss': 0.26649847626686096, 'lr': 0.00014895281095530575, 'memory_mb': 1904.109375}\n",
      "Step 800: {'train_loss': 0.3138626515865326, 'lr': 0.0001459612975731506, 'memory_mb': 1904.109375}\n",
      "Step 810: {'train_loss': 0.34019407629966736, 'lr': 0.00014297139100676433, 'memory_mb': 1904.109375}\n",
      "Step 819: {'eval_loss': 1.8805746734142303, 'memory_mb': 1904.109375}\n",
      "Step 820: {'train_loss': 0.2907172441482544, 'lr': 0.0001399842808038062, 'memory_mb': 1904.109375}\n",
      "Step 830: {'train_loss': 0.22911286354064941, 'lr': 0.00013700115539938993, 'memory_mb': 1904.109375}\n",
      "Step 840: {'train_loss': 0.2891254127025604, 'lr': 0.0001340232016432593, 'memory_mb': 1904.109375}\n",
      "Step 850: {'train_loss': 0.2598514258861542, 'lr': 0.00013105160432759464, 'memory_mb': 1904.109375}\n",
      "Step 860: {'train_loss': 0.22250200808048248, 'lr': 0.00012808754571563827, 'memory_mb': 1904.109375}\n",
      "Step 870: {'train_loss': 0.30126649141311646, 'lr': 0.00012513220507132553, 'memory_mb': 1904.109375}\n",
      "Step 880: {'train_loss': 0.26228541135787964, 'lr': 0.0001221867581901095, 'memory_mb': 1904.109375}\n",
      "Step 882: {'eval_loss': 1.9952073320746422, 'memory_mb': 1904.109375}\n",
      "Step 890: {'train_loss': 0.2565125524997711, 'lr': 0.00011925237693116544, 'memory_mb': 1904.109375}\n",
      "Step 900: {'train_loss': 0.22543247044086456, 'lr': 0.00011633022875116156, 'memory_mb': 1904.109375}\n",
      "Step 910: {'train_loss': 0.2832905352115631, 'lr': 0.00011342147623978095, 'memory_mb': 1904.109375}\n",
      "Step 920: {'train_loss': 0.24242503941059113, 'lr': 0.00011052727665718026, 'memory_mb': 1904.109375}\n",
      "Step 930: {'train_loss': 0.2502097487449646, 'lr': 0.0001076487814735685, 'memory_mb': 1904.109375}\n",
      "Step 940: {'train_loss': 0.23794874548912048, 'lr': 0.00010478713591108963, 'memory_mb': 1904.109375}\n",
      "Step 945: {'eval_loss': 2.017700955271721, 'memory_mb': 1904.109375}\n",
      "Step 950: {'train_loss': 0.20370693504810333, 'lr': 0.00010194347848819054, 'memory_mb': 1904.109375}\n",
      "Step 960: {'train_loss': 0.2201698273420334, 'lr': 9.911894056665671e-05, 'memory_mb': 1904.109375}\n",
      "Step 970: {'train_loss': 0.23935964703559875, 'lr': 9.631464590149464e-05, 'memory_mb': 1904.109375}\n",
      "Step 980: {'train_loss': 0.19446589052677155, 'lr': 9.353171019384119e-05, 'memory_mb': 1904.109375}\n",
      "Step 990: {'train_loss': 0.21633192896842957, 'lr': 9.077124064707655e-05, 'memory_mb': 1904.109375}\n",
      "Step 1000: {'train_loss': 0.2238486409187317, 'lr': 8.803433552631874e-05, 'memory_mb': 1904.109375}\n",
      "Step 1008: {'eval_loss': 2.0705789402127266, 'memory_mb': 1904.109375}\n",
      "Step 1010: {'train_loss': 0.18734627962112427, 'lr': 8.532208372147376e-05, 'memory_mb': 1904.109375}\n",
      "Step 1020: {'train_loss': 0.17122533917427063, 'lr': 8.263556431401596e-05, 'memory_mb': 1904.109375}\n",
      "Step 1030: {'train_loss': 0.17982162535190582, 'lr': 7.997584614767068e-05, 'memory_mb': 1904.109375}\n",
      "Step 1040: {'train_loss': 0.22190076112747192, 'lr': 7.734398740316998e-05, 'memory_mb': 1904.109375}\n",
      "Step 1050: {'train_loss': 0.1859067678451538, 'lr': 7.474103517725069e-05, 'memory_mb': 1904.109375}\n",
      "Step 1060: {'train_loss': 0.23302505910396576, 'lr': 7.216802506606232e-05, 'memory_mb': 1904.109375}\n",
      "Step 1070: {'train_loss': 0.17870527505874634, 'lr': 6.962598075315046e-05, 'memory_mb': 1904.109375}\n",
      "Step 1071: {'eval_loss': 2.1395243369042873, 'memory_mb': 1904.109375}\n",
      "Step 1080: {'train_loss': 0.16377902030944824, 'lr': 6.711591360218e-05, 'memory_mb': 1904.109375}\n",
      "Step 1090: {'train_loss': 0.16224895417690277, 'lr': 6.463882225455919e-05, 'memory_mb': 1904.109375}\n",
      "Step 1100: {'train_loss': 0.16159196197986603, 'lr': 6.219569223212584e-05, 'memory_mb': 1904.109375}\n",
      "Step 1110: {'train_loss': 0.19696038961410522, 'lr': 5.978749554505324e-05, 'memory_mb': 1904.109375}\n",
      "Step 1120: {'train_loss': 0.20195017755031586, 'lr': 5.7415190305131305e-05, 'memory_mb': 1904.109375}\n",
      "Step 1130: {'train_loss': 0.21021398901939392, 'lr': 5.507972034457786e-05, 'memory_mb': 1904.109375}\n",
      "Step 1134: {'eval_loss': 2.196970660239458, 'memory_mb': 1904.109375}\n",
      "Step 1140: {'train_loss': 0.15129128098487854, 'lr': 5.2782014840530366e-05, 'memory_mb': 1904.109375}\n",
      "Step 1150: {'train_loss': 0.1515110433101654, 'lr': 5.0522987945369e-05, 'memory_mb': 1904.109375}\n",
      "Step 1160: {'train_loss': 0.17728164792060852, 'lr': 4.8303538423016893e-05, 'memory_mb': 1904.109375}\n",
      "Step 1170: {'train_loss': 0.18347615003585815, 'lr': 4.6124549291362884e-05, 'memory_mb': 1904.109375}\n",
      "Step 1180: {'train_loss': 0.14511369168758392, 'lr': 4.39868874709491e-05, 'memory_mb': 1904.109375}\n",
      "Step 1190: {'train_loss': 0.156527578830719, 'lr': 4.1891403440062495e-05, 'memory_mb': 1904.109375}\n",
      "Step 1197: {'eval_loss': 2.199888601899147, 'memory_mb': 1904.109375}\n",
      "Step 1200: {'train_loss': 0.14386340975761414, 'lr': 3.983893089636841e-05, 'memory_mb': 1904.109375}\n",
      "Step 1210: {'train_loss': 0.15160204470157623, 'lr': 3.7830286425220234e-05, 'memory_mb': 1904.609375}\n",
      "Step 1220: {'train_loss': 0.16595393419265747, 'lr': 3.586626917477723e-05, 'memory_mb': 1904.609375}\n",
      "Step 1230: {'train_loss': 0.13464243710041046, 'lr': 3.394766053806002e-05, 'memory_mb': 1904.609375}\n",
      "Step 1240: {'train_loss': 0.16324767470359802, 'lr': 3.207522384206968e-05, 'memory_mb': 1904.609375}\n",
      "Step 1250: {'train_loss': 0.16177819669246674, 'lr': 3.0249704044094797e-05, 'memory_mb': 1904.609375}\n",
      "Step 1260: {'eval_loss': 2.2542431242763996, 'memory_mb': 1904.609375}\n",
      "Step 1260: {'train_loss': 0.17242559790611267, 'lr': 2.8471827435326923e-05, 'memory_mb': 1904.609375}\n",
      "Step 1270: {'train_loss': 0.13899926841259003, 'lr': 2.6742301351902263e-05, 'memory_mb': 1904.609375}\n",
      "Step 1280: {'train_loss': 0.13713446259498596, 'lr': 2.5061813893485085e-05, 'memory_mb': 1904.609375}\n",
      "Step 1290: {'train_loss': 0.12938781082630157, 'lr': 2.343103364950387e-05, 'memory_mb': 1904.609375}\n",
      "Step 1300: {'train_loss': 0.14960311353206635, 'lr': 2.1850609433150286e-05, 'memory_mb': 1904.609375}\n",
      "Step 1310: {'train_loss': 0.14436441659927368, 'lr': 2.0321170023245832e-05, 'memory_mb': 1904.609375}\n",
      "Step 1320: {'train_loss': 0.17749103903770447, 'lr': 1.88433239140794e-05, 'memory_mb': 1904.609375}\n",
      "Step 1323: {'eval_loss': 2.290684301406145, 'memory_mb': 1904.609375}\n",
      "Step 1330: {'train_loss': 0.1699315905570984, 'lr': 1.7417659073315247e-05, 'memory_mb': 1904.609375}\n",
      "Step 1340: {'train_loss': 0.14041569828987122, 'lr': 1.6044742708067205e-05, 'memory_mb': 1904.609375}\n",
      "Step 1350: {'train_loss': 0.1405918300151825, 'lr': 1.4725121039232945e-05, 'memory_mb': 1904.609375}\n",
      "Step 1360: {'train_loss': 0.14802652597427368, 'lr': 1.3459319084177572e-05, 'memory_mb': 1904.609375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1370: {'train_loss': 0.1538088321685791, 'lr': 1.2247840447852992e-05, 'memory_mb': 1904.609375}\n",
      "Step 1380: {'train_loss': 0.1382681280374527, 'lr': 1.1091167122436672e-05, 'memory_mb': 1904.609375}\n",
      "Step 1386: {'eval_loss': 2.290121618658304, 'memory_mb': 1904.609375}\n",
      "Step 1390: {'train_loss': 0.12690481543540955, 'lr': 9.98975929556866e-06, 'memory_mb': 1904.609375}\n",
      "Step 1400: {'train_loss': 0.1273026168346405, 'lr': 8.94405516726398e-06, 'memory_mb': 1904.609375}\n",
      "Step 1410: {'train_loss': 0.15279656648635864, 'lr': 7.954470775572841e-06, 'memory_mb': 1904.609375}\n",
      "Step 1420: {'train_loss': 0.12823890149593353, 'lr': 7.021399831057961e-06, 'memory_mb': 1904.609375}\n",
      "Step 1430: {'train_loss': 0.13387340307235718, 'lr': 6.1452135601551655e-06, 'memory_mb': 1904.609375}\n",
      "Step 1440: {'train_loss': 0.13139177858829498, 'lr': 5.326260557479195e-06, 'memory_mb': 1904.609375}\n",
      "Step 1449: {'eval_loss': 2.3069410510361195, 'memory_mb': 1904.609375}\n",
      "Step 1450: {'train_loss': 0.1376027911901474, 'lr': 4.564866647133847e-06, 'memory_mb': 1904.609375}\n",
      "Step 1460: {'train_loss': 0.14317548274993896, 'lr': 3.861334753081352e-06, 'memory_mb': 1904.609375}\n",
      "Step 1470: {'train_loss': 0.1432202309370041, 'lr': 3.2159447786227645e-06, 'memory_mb': 1904.609375}\n",
      "Step 1480: {'train_loss': 0.1450515240430832, 'lr': 2.628953495037239e-06, 'memory_mb': 1904.609375}\n",
      "Step 1490: {'train_loss': 0.1568605899810791, 'lr': 2.100594439424269e-06, 'memory_mb': 1904.609375}\n",
      "Step 1500: {'train_loss': 0.13712990283966064, 'lr': 1.6310778217900722e-06, 'memory_mb': 1904.609375}\n",
      "Step 1510: {'train_loss': 0.14302386343479156, 'lr': 1.2205904414145262e-06, 'memory_mb': 1904.609375}\n",
      "Step 1512: {'eval_loss': 2.3080531917512417, 'memory_mb': 1904.609375}\n",
      "Step 1520: {'train_loss': 0.1437445878982544, 'lr': 8.692956125322836e-07, 'memory_mb': 1904.609375}\n",
      "Step 1530: {'train_loss': 0.12549491226673126, 'lr': 5.773330993573799e-07, 'memory_mb': 1904.609375}\n",
      "Step 1540: {'train_loss': 0.14161424338817596, 'lr': 3.448190604775469e-07, 'memory_mb': 1904.609375}\n",
      "Step 1550: {'train_loss': 0.16987411677837372, 'lr': 1.7184600263991377e-07, 'memory_mb': 1904.609375}\n",
      "Step 1560: {'train_loss': 0.1591932624578476, 'lr': 5.8482743946847153e-08, 'memory_mb': 1904.609375}\n",
      "Step 1570: {'train_loss': 0.1510275900363922, 'lr': 4.774386476269487e-09, 'memory_mb': 1904.609375}\n",
      "Step 1575: {'eval_loss': 2.308122932910919, 'memory_mb': 1904.609375}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.30812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1904.60938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.15103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_cosine_20250425_231951\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/qg9m70ou\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_231951-qg9m70ou/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule cosine \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5ffce",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd4d9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_232159-kxsmkw0m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_plateau_20250425_232159\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/kxsmkw0m\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 0.0003, 'memory_mb': 1898.046875}\n",
      "Step 10: {'train_loss': 1.6952680349349976, 'lr': 0.0003, 'memory_mb': 1901.546875}\n",
      "Step 20: {'train_loss': 1.441868543624878, 'lr': 0.0003, 'memory_mb': 1902.046875}\n",
      "Step 30: {'train_loss': 1.1627061367034912, 'lr': 0.0003, 'memory_mb': 1902.046875}\n",
      "Step 40: {'train_loss': 1.2523077726364136, 'lr': 0.0003, 'memory_mb': 1902.046875}\n",
      "Step 50: {'train_loss': 1.1674116849899292, 'lr': 0.0003, 'memory_mb': 1902.046875}\n",
      "Step 60: {'train_loss': 1.239466667175293, 'lr': 0.0003, 'memory_mb': 1902.046875}\n",
      "Step 63: {'eval_loss': 1.2657389268279076, 'memory_mb': 1904.046875}\n",
      "Step 70: {'train_loss': 1.2837108373641968, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 80: {'train_loss': 1.3620747327804565, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 90: {'train_loss': 1.188518762588501, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 100: {'train_loss': 1.0395869016647339, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 110: {'train_loss': 1.0578974485397339, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 120: {'train_loss': 1.2720706462860107, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 126: {'eval_loss': 1.2475565671920776, 'memory_mb': 1904.046875}\n",
      "Step 130: {'train_loss': 1.1231704950332642, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 140: {'train_loss': 0.9602336287498474, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 150: {'train_loss': 1.174655795097351, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 160: {'train_loss': 0.9106057286262512, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 170: {'train_loss': 1.2193965911865234, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 180: {'train_loss': 1.1314449310302734, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 189: {'eval_loss': 1.2613228764384985, 'memory_mb': 1904.046875}\n",
      "Step 190: {'train_loss': 1.2221193313598633, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 200: {'train_loss': 1.033835530281067, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 210: {'train_loss': 0.8785492181777954, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 220: {'train_loss': 0.9134131669998169, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 230: {'train_loss': 0.9008175134658813, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 240: {'train_loss': 1.1410971879959106, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 250: {'train_loss': 0.9526962637901306, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 252: {'eval_loss': 1.3013879042118788, 'memory_mb': 1904.046875}\n",
      "Step 260: {'train_loss': 0.7860466241836548, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 270: {'train_loss': 1.2088228464126587, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 280: {'train_loss': 1.095112681388855, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 290: {'train_loss': 0.8514939546585083, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 300: {'train_loss': 0.8040570616722107, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 310: {'train_loss': 0.9927032589912415, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 315: {'eval_loss': 1.3213716447353363, 'memory_mb': 1904.046875}\n",
      "Step 320: {'train_loss': 0.6430411338806152, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 330: {'train_loss': 0.823472797870636, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 340: {'train_loss': 0.759400486946106, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 350: {'train_loss': 0.8100113272666931, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 360: {'train_loss': 0.7306981086730957, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 370: {'train_loss': 0.7854183912277222, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 378: {'eval_loss': 1.3463969584554434, 'memory_mb': 1904.046875}\n",
      "Step 380: {'train_loss': 0.6111602783203125, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 390: {'train_loss': 0.6728606224060059, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 400: {'train_loss': 0.6289180517196655, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 410: {'train_loss': 0.5843498110771179, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 420: {'train_loss': 0.6444849371910095, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 430: {'train_loss': 0.677000880241394, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 440: {'train_loss': 0.6895051002502441, 'lr': 0.0003, 'memory_mb': 1904.046875}\n",
      "Step 441: {'eval_loss': 1.415365755558014, 'memory_mb': 1904.046875}\n",
      "Step 450: {'train_loss': 0.523108184337616, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 460: {'train_loss': 0.6224876046180725, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 470: {'train_loss': 0.5795547962188721, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 480: {'train_loss': 0.6740065217018127, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 490: {'train_loss': 0.5899368524551392, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 500: {'train_loss': 0.7636972665786743, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 504: {'eval_loss': 1.4997936561703682, 'memory_mb': 1904.546875}\n",
      "Step 510: {'train_loss': 0.4694980978965759, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 520: {'train_loss': 0.4801350235939026, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 530: {'train_loss': 0.4582260251045227, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 540: {'train_loss': 0.46077004075050354, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 550: {'train_loss': 0.516086995601654, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 560: {'train_loss': 0.5293470621109009, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 567: {'eval_loss': 1.5596660412847996, 'memory_mb': 1904.546875}\n",
      "Step 570: {'train_loss': 0.3765726387500763, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 580: {'train_loss': 0.3909081816673279, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 590: {'train_loss': 0.371082603931427, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 600: {'train_loss': 0.43560758233070374, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 610: {'train_loss': 0.4481430649757385, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 620: {'train_loss': 0.5154308080673218, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 630: {'eval_loss': 1.6358608603477478, 'memory_mb': 1904.546875}\n",
      "Step 630: {'train_loss': 0.48701903223991394, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 640: {'train_loss': 0.38335996866226196, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 650: {'train_loss': 0.44482019543647766, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 660: {'train_loss': 0.4083450734615326, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 670: {'train_loss': 0.33766621351242065, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 680: {'train_loss': 0.46994656324386597, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 690: {'train_loss': 0.3874885141849518, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 693: {'eval_loss': 1.6884109191596508, 'memory_mb': 1904.546875}\n",
      "Step 700: {'train_loss': 0.32607901096343994, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 710: {'train_loss': 0.34362712502479553, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 720: {'train_loss': 0.39331990480422974, 'lr': 0.0003, 'memory_mb': 1904.546875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 730: {'train_loss': 0.40583643317222595, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 740: {'train_loss': 0.36308032274246216, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 750: {'train_loss': 0.3187917172908783, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 756: {'eval_loss': 1.7425782680511475, 'memory_mb': 1904.546875}\n",
      "Step 760: {'train_loss': 0.26501789689064026, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 770: {'train_loss': 0.2713455855846405, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 780: {'train_loss': 0.2609120309352875, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 790: {'train_loss': 0.27872899174690247, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 800: {'train_loss': 0.35063081979751587, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 810: {'train_loss': 0.3591235876083374, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 819: {'eval_loss': 1.8210347406566143, 'memory_mb': 1904.546875}\n",
      "Step 820: {'train_loss': 0.2863727807998657, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 830: {'train_loss': 0.24195127189159393, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 840: {'train_loss': 0.29500773549079895, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 850: {'train_loss': 0.2874475121498108, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 860: {'train_loss': 0.25099486112594604, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 870: {'train_loss': 0.32951751351356506, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 880: {'train_loss': 0.29608356952667236, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 882: {'eval_loss': 1.8757389672100544, 'memory_mb': 1904.546875}\n",
      "Step 890: {'train_loss': 0.2996633052825928, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 900: {'train_loss': 0.23010919988155365, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 910: {'train_loss': 0.3102838695049286, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 920: {'train_loss': 0.24140949547290802, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 930: {'train_loss': 0.26526957750320435, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 940: {'train_loss': 0.2424735277891159, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 945: {'eval_loss': 1.9359658546745777, 'memory_mb': 1904.546875}\n",
      "Step 950: {'train_loss': 0.21520739793777466, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 960: {'train_loss': 0.2586970925331116, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 970: {'train_loss': 0.2380307912826538, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 980: {'train_loss': 0.19412298500537872, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 990: {'train_loss': 0.23376528918743134, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1000: {'train_loss': 0.2612270712852478, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1008: {'eval_loss': 1.9670737348496914, 'memory_mb': 1904.546875}\n",
      "Step 1010: {'train_loss': 0.18680520355701447, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1020: {'train_loss': 0.1704561412334442, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1030: {'train_loss': 0.19093820452690125, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1040: {'train_loss': 0.23938995599746704, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1050: {'train_loss': 0.19295775890350342, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1060: {'train_loss': 0.2631259560585022, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1070: {'train_loss': 0.21523168683052063, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1071: {'eval_loss': 1.9915537275373936, 'memory_mb': 1904.546875}\n",
      "Step 1080: {'train_loss': 0.1656138151884079, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1090: {'train_loss': 0.17045825719833374, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1100: {'train_loss': 0.1712685525417328, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1110: {'train_loss': 0.21210280060768127, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1120: {'train_loss': 0.22891205549240112, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1130: {'train_loss': 0.24201063811779022, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1134: {'eval_loss': 2.0416502617299557, 'memory_mb': 1904.546875}\n",
      "Step 1140: {'train_loss': 0.15817944705486298, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1150: {'train_loss': 0.16710375249385834, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1160: {'train_loss': 0.18486972153186798, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1170: {'train_loss': 0.1959245651960373, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1180: {'train_loss': 0.1592203974723816, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1190: {'train_loss': 0.1668691486120224, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1197: {'eval_loss': 2.1000677049160004, 'memory_mb': 1904.546875}\n",
      "Step 1200: {'train_loss': 0.1228625550866127, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1210: {'train_loss': 0.165536031126976, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1220: {'train_loss': 0.16286566853523254, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1230: {'train_loss': 0.14493879675865173, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1240: {'train_loss': 0.18514929711818695, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1250: {'train_loss': 0.1805087774991989, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1260: {'eval_loss': 2.1533354446291924, 'memory_mb': 1904.546875}\n",
      "Step 1260: {'train_loss': 0.1467771828174591, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1270: {'train_loss': 0.13785098493099213, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1280: {'train_loss': 0.12881164252758026, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1290: {'train_loss': 0.1371181458234787, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1300: {'train_loss': 0.1376843899488449, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1310: {'train_loss': 0.14358319342136383, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1320: {'train_loss': 0.18984945118427277, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1323: {'eval_loss': 2.149941761046648, 'memory_mb': 1904.546875}\n",
      "Step 1330: {'train_loss': 0.14353972673416138, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1340: {'train_loss': 0.13305871188640594, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1350: {'train_loss': 0.13277491927146912, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1360: {'train_loss': 0.14095138013362885, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1370: {'train_loss': 0.15195536613464355, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1380: {'train_loss': 0.15340633690357208, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1386: {'eval_loss': 2.2020392827689648, 'memory_mb': 1904.546875}\n",
      "Step 1390: {'train_loss': 0.11658824235200882, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1400: {'train_loss': 0.12627637386322021, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1410: {'train_loss': 0.1475130021572113, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1420: {'train_loss': 0.13201290369033813, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1430: {'train_loss': 0.13274972140789032, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1440: {'train_loss': 0.13949890434741974, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1449: {'eval_loss': 2.1371452137827873, 'memory_mb': 1904.546875}\n",
      "Step 1450: {'train_loss': 0.11754004657268524, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1460: {'train_loss': 0.12362895905971527, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1470: {'train_loss': 0.1260998547077179, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1480: {'train_loss': 0.1265867054462433, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1490: {'train_loss': 0.15287356078624725, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1500: {'train_loss': 0.15601088106632233, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1510: {'train_loss': 0.12586629390716553, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1512: {'eval_loss': 2.145453557372093, 'memory_mb': 1904.546875}\n",
      "Step 1520: {'train_loss': 0.11651479452848434, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1530: {'train_loss': 0.11269822716712952, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1540: {'train_loss': 0.12032455950975418, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1550: {'train_loss': 0.15272989869117737, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1560: {'train_loss': 0.14361175894737244, 'lr': 0.0003, 'memory_mb': 1904.546875}\n",
      "Step 1570: {'train_loss': 0.14887692034244537, 'lr': 0.0003, 'memory_mb': 1904.546875}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1575: {'eval_loss': 2.1792117543518543, 'memory_mb': 1904.546875}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.17921\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1904.54688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.14888\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_plateau_20250425_232159\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/kxsmkw0m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_232159-kxsmkw0m/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule plateau \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06f50d",
   "metadata": {},
   "source": [
    "### Lower LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be6ee87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_232408-qv50nbrl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_warmup_20250425_232407\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/qv50nbrl\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 6.369426751592357e-07, 'memory_mb': 1898.31640625}\n",
      "Step 10: {'train_loss': 6.465214252471924, 'lr': 7.006369426751593e-06, 'memory_mb': 1901.31640625}\n",
      "Step 20: {'train_loss': 4.976337909698486, 'lr': 1.337579617834395e-05, 'memory_mb': 1901.81640625}\n",
      "Step 30: {'train_loss': 2.6029560565948486, 'lr': 1.974522292993631e-05, 'memory_mb': 1902.31640625}\n",
      "Step 40: {'train_loss': 1.7829177379608154, 'lr': 2.6114649681528662e-05, 'memory_mb': 1902.31640625}\n",
      "Step 50: {'train_loss': 1.5611118078231812, 'lr': 3.248407643312102e-05, 'memory_mb': 1902.31640625}\n",
      "Step 60: {'train_loss': 1.6385478973388672, 'lr': 3.885350318471338e-05, 'memory_mb': 1902.31640625}\n",
      "Step 63: {'eval_loss': 1.6569702997803688, 'memory_mb': 1904.31640625}\n",
      "Step 70: {'train_loss': 1.7184717655181885, 'lr': 4.522292993630574e-05, 'memory_mb': 1904.31640625}\n",
      "Step 80: {'train_loss': 1.853180170059204, 'lr': 5.159235668789809e-05, 'memory_mb': 1904.31640625}\n",
      "Step 90: {'train_loss': 1.5460628271102905, 'lr': 5.796178343949045e-05, 'memory_mb': 1904.31640625}\n",
      "Step 100: {'train_loss': 1.3521548509597778, 'lr': 6.43312101910828e-05, 'memory_mb': 1904.31640625}\n",
      "Step 110: {'train_loss': 1.354373812675476, 'lr': 7.070063694267515e-05, 'memory_mb': 1904.31640625}\n",
      "Step 120: {'train_loss': 1.5520051717758179, 'lr': 7.707006369426753e-05, 'memory_mb': 1904.31640625}\n",
      "Step 126: {'eval_loss': 1.4360261969268322, 'memory_mb': 1904.31640625}\n",
      "Step 130: {'train_loss': 1.410448431968689, 'lr': 8.343949044585988e-05, 'memory_mb': 1904.31640625}\n",
      "Step 140: {'train_loss': 1.1927003860473633, 'lr': 8.980891719745223e-05, 'memory_mb': 1904.31640625}\n",
      "Step 150: {'train_loss': 1.3315045833587646, 'lr': 9.617834394904459e-05, 'memory_mb': 1904.31640625}\n",
      "Step 160: {'train_loss': 1.0773448944091797, 'lr': 9.97179125528914e-05, 'memory_mb': 1904.31640625}\n",
      "Step 170: {'train_loss': 1.3479065895080566, 'lr': 9.901269393511989e-05, 'memory_mb': 1904.31640625}\n",
      "Step 180: {'train_loss': 1.2637132406234741, 'lr': 9.830747531734838e-05, 'memory_mb': 1904.31640625}\n",
      "Step 189: {'eval_loss': 1.2836838737130165, 'memory_mb': 1904.31640625}\n",
      "Step 190: {'train_loss': 1.4544041156768799, 'lr': 9.760225669957688e-05, 'memory_mb': 1904.31640625}\n",
      "Step 200: {'train_loss': 1.303650975227356, 'lr': 9.689703808180537e-05, 'memory_mb': 1904.31640625}\n",
      "Step 210: {'train_loss': 1.0628904104232788, 'lr': 9.619181946403385e-05, 'memory_mb': 1904.31640625}\n",
      "Step 220: {'train_loss': 1.1611652374267578, 'lr': 9.548660084626234e-05, 'memory_mb': 1904.31640625}\n",
      "Step 230: {'train_loss': 1.126105546951294, 'lr': 9.478138222849083e-05, 'memory_mb': 1904.31640625}\n",
      "Step 240: {'train_loss': 1.3256721496582031, 'lr': 9.407616361071933e-05, 'memory_mb': 1904.31640625}\n",
      "Step 250: {'train_loss': 1.10507333278656, 'lr': 9.337094499294782e-05, 'memory_mb': 1904.31640625}\n",
      "Step 252: {'eval_loss': 1.267851609736681, 'memory_mb': 1904.31640625}\n",
      "Step 260: {'train_loss': 1.0584609508514404, 'lr': 9.266572637517631e-05, 'memory_mb': 1904.31640625}\n",
      "Step 270: {'train_loss': 1.5821422338485718, 'lr': 9.19605077574048e-05, 'memory_mb': 1904.31640625}\n",
      "Step 280: {'train_loss': 1.4448271989822388, 'lr': 9.125528913963329e-05, 'memory_mb': 1904.31640625}\n",
      "Step 290: {'train_loss': 1.149415373802185, 'lr': 9.055007052186178e-05, 'memory_mb': 1904.31640625}\n",
      "Step 300: {'train_loss': 1.013479471206665, 'lr': 8.984485190409028e-05, 'memory_mb': 1904.31640625}\n",
      "Step 310: {'train_loss': 1.3077670335769653, 'lr': 8.913963328631876e-05, 'memory_mb': 1904.31640625}\n",
      "Step 315: {'eval_loss': 1.2552896775305271, 'memory_mb': 1904.31640625}\n",
      "Step 320: {'train_loss': 1.0521786212921143, 'lr': 8.843441466854725e-05, 'memory_mb': 1904.31640625}\n",
      "Step 330: {'train_loss': 1.189632773399353, 'lr': 8.772919605077574e-05, 'memory_mb': 1904.31640625}\n",
      "Step 340: {'train_loss': 1.1733683347702026, 'lr': 8.702397743300423e-05, 'memory_mb': 1904.31640625}\n",
      "Step 350: {'train_loss': 1.216010332107544, 'lr': 8.631875881523273e-05, 'memory_mb': 1904.31640625}\n",
      "Step 360: {'train_loss': 1.0825865268707275, 'lr': 8.561354019746122e-05, 'memory_mb': 1904.31640625}\n",
      "Step 370: {'train_loss': 1.1505537033081055, 'lr': 8.490832157968971e-05, 'memory_mb': 1904.31640625}\n",
      "Step 378: {'eval_loss': 1.253973163664341, 'memory_mb': 1904.31640625}\n",
      "Step 380: {'train_loss': 1.1773245334625244, 'lr': 8.42031029619182e-05, 'memory_mb': 1904.31640625}\n",
      "Step 390: {'train_loss': 1.2109065055847168, 'lr': 8.349788434414669e-05, 'memory_mb': 1904.31640625}\n",
      "Step 400: {'train_loss': 1.1320494413375854, 'lr': 8.279266572637518e-05, 'memory_mb': 1904.31640625}\n",
      "Step 410: {'train_loss': 1.0208792686462402, 'lr': 8.208744710860367e-05, 'memory_mb': 1904.31640625}\n",
      "Step 420: {'train_loss': 1.114749550819397, 'lr': 8.138222849083216e-05, 'memory_mb': 1904.31640625}\n",
      "Step 430: {'train_loss': 1.2347602844238281, 'lr': 8.067700987306065e-05, 'memory_mb': 1904.31640625}\n",
      "Step 440: {'train_loss': 1.175937533378601, 'lr': 7.997179125528914e-05, 'memory_mb': 1904.31640625}\n",
      "Step 441: {'eval_loss': 1.254761964082718, 'memory_mb': 1904.31640625}\n",
      "Model converged at step 441\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 1.25476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 8e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1904.31641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 1.17594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_warmup_20250425_232407\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/qv50nbrl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_232408-qv50nbrl/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8906f8",
   "metadata": {},
   "source": [
    "### Higher LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "312ae686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_232453-andv0pbe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_warmup_20250425_232453\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/andv0pbe\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 6.369426751592357e-06, 'memory_mb': 1898.30078125}\n",
      "Step 10: {'train_loss': 3.0036230087280273, 'lr': 7.006369426751592e-05, 'memory_mb': 1901.80078125}\n",
      "Step 20: {'train_loss': 1.712828516960144, 'lr': 0.00013375796178343948, 'memory_mb': 1902.30078125}\n",
      "Step 30: {'train_loss': 1.4652690887451172, 'lr': 0.00019745222929936309, 'memory_mb': 1902.30078125}\n",
      "Step 40: {'train_loss': 1.4793974161148071, 'lr': 0.00026114649681528663, 'memory_mb': 1902.30078125}\n",
      "Step 50: {'train_loss': 1.2479735612869263, 'lr': 0.0003248407643312102, 'memory_mb': 1902.30078125}\n",
      "Step 60: {'train_loss': 1.2841695547103882, 'lr': 0.0003885350318471338, 'memory_mb': 1902.30078125}\n",
      "Step 63: {'eval_loss': 1.2988103590905666, 'memory_mb': 1904.30078125}\n",
      "Step 70: {'train_loss': 1.3414320945739746, 'lr': 0.00045222929936305736, 'memory_mb': 1904.30078125}\n",
      "Step 80: {'train_loss': 1.4275175333023071, 'lr': 0.0005159235668789809, 'memory_mb': 1904.30078125}\n",
      "Step 90: {'train_loss': 1.218489170074463, 'lr': 0.0005796178343949045, 'memory_mb': 1904.30078125}\n",
      "Step 100: {'train_loss': 1.0553492307662964, 'lr': 0.000643312101910828, 'memory_mb': 1904.30078125}\n",
      "Step 110: {'train_loss': 1.0810962915420532, 'lr': 0.0007070063694267516, 'memory_mb': 1904.30078125}\n",
      "Step 120: {'train_loss': 1.3078452348709106, 'lr': 0.0007707006369426752, 'memory_mb': 1904.30078125}\n",
      "Step 126: {'eval_loss': 1.254588846117258, 'memory_mb': 1904.30078125}\n",
      "Step 130: {'train_loss': 1.144448161125183, 'lr': 0.0008343949044585987, 'memory_mb': 1904.30078125}\n",
      "Step 140: {'train_loss': 0.9894551634788513, 'lr': 0.0008980891719745223, 'memory_mb': 1904.30078125}\n",
      "Step 150: {'train_loss': 1.1883949041366577, 'lr': 0.0009617834394904459, 'memory_mb': 1904.30078125}\n",
      "Step 160: {'train_loss': 0.9419080018997192, 'lr': 0.000997179125528914, 'memory_mb': 1904.30078125}\n",
      "Step 170: {'train_loss': 1.2525098323822021, 'lr': 0.0009901269393511988, 'memory_mb': 1904.30078125}\n",
      "Step 180: {'train_loss': 1.1449570655822754, 'lr': 0.0009830747531734839, 'memory_mb': 1904.30078125}\n",
      "Step 189: {'eval_loss': 1.2680480889976025, 'memory_mb': 1904.30078125}\n",
      "Step 190: {'train_loss': 1.1829121112823486, 'lr': 0.0009760225669957687, 'memory_mb': 1904.30078125}\n",
      "Step 200: {'train_loss': 0.9976204633712769, 'lr': 0.0009689703808180536, 'memory_mb': 1904.30078125}\n",
      "Step 210: {'train_loss': 0.842375636100769, 'lr': 0.0009619181946403384, 'memory_mb': 1904.30078125}\n",
      "Step 220: {'train_loss': 0.9004253149032593, 'lr': 0.0009548660084626234, 'memory_mb': 1904.30078125}\n",
      "Step 230: {'train_loss': 0.8969465494155884, 'lr': 0.0009478138222849083, 'memory_mb': 1904.30078125}\n",
      "Step 240: {'train_loss': 1.1124407052993774, 'lr': 0.0009407616361071932, 'memory_mb': 1904.30078125}\n",
      "Step 250: {'train_loss': 0.9467955827713013, 'lr': 0.0009337094499294781, 'memory_mb': 1904.30078125}\n",
      "Step 252: {'eval_loss': 1.3035636842250824, 'memory_mb': 1904.30078125}\n",
      "Step 260: {'train_loss': 0.7112366557121277, 'lr': 0.000926657263751763, 'memory_mb': 1904.30078125}\n",
      "Step 270: {'train_loss': 1.1173887252807617, 'lr': 0.000919605077574048, 'memory_mb': 1904.30078125}\n",
      "Step 280: {'train_loss': 1.0208649635314941, 'lr': 0.0009125528913963329, 'memory_mb': 1904.30078125}\n",
      "Step 290: {'train_loss': 0.7752402424812317, 'lr': 0.0009055007052186178, 'memory_mb': 1904.30078125}\n",
      "Step 300: {'train_loss': 0.7236387729644775, 'lr': 0.0008984485190409027, 'memory_mb': 1904.30078125}\n",
      "Step 310: {'train_loss': 0.9020049571990967, 'lr': 0.0008913963328631876, 'memory_mb': 1904.30078125}\n",
      "Step 315: {'eval_loss': 1.305184768512845, 'memory_mb': 1904.30078125}\n",
      "Step 320: {'train_loss': 0.5352604985237122, 'lr': 0.0008843441466854725, 'memory_mb': 1904.30078125}\n",
      "Step 330: {'train_loss': 0.7185596823692322, 'lr': 0.0008772919605077574, 'memory_mb': 1904.30078125}\n",
      "Step 340: {'train_loss': 0.6756449341773987, 'lr': 0.0008702397743300423, 'memory_mb': 1904.30078125}\n",
      "Step 350: {'train_loss': 0.7294884324073792, 'lr': 0.0008631875881523273, 'memory_mb': 1904.30078125}\n",
      "Step 360: {'train_loss': 0.6898707747459412, 'lr': 0.0008561354019746122, 'memory_mb': 1904.30078125}\n",
      "Step 370: {'train_loss': 0.7086465358734131, 'lr': 0.0008490832157968971, 'memory_mb': 1904.30078125}\n",
      "Step 378: {'eval_loss': 1.3802275136113167, 'memory_mb': 1904.30078125}\n",
      "Step 380: {'train_loss': 0.5397762656211853, 'lr': 0.000842031029619182, 'memory_mb': 1904.30078125}\n",
      "Step 390: {'train_loss': 0.5854023098945618, 'lr': 0.000834978843441467, 'memory_mb': 1904.30078125}\n",
      "Step 400: {'train_loss': 0.5747308731079102, 'lr': 0.0008279266572637519, 'memory_mb': 1904.30078125}\n",
      "Step 410: {'train_loss': 0.5377011895179749, 'lr': 0.0008208744710860366, 'memory_mb': 1904.30078125}\n",
      "Step 420: {'train_loss': 0.5840098857879639, 'lr': 0.0008138222849083215, 'memory_mb': 1904.30078125}\n",
      "Step 430: {'train_loss': 0.6214242577552795, 'lr': 0.0008067700987306064, 'memory_mb': 1904.30078125}\n",
      "Step 440: {'train_loss': 0.6148480772972107, 'lr': 0.0007997179125528914, 'memory_mb': 1904.30078125}\n",
      "Step 441: {'eval_loss': 1.4655917584896088, 'memory_mb': 1904.30078125}\n",
      "Step 450: {'train_loss': 0.44790950417518616, 'lr': 0.0007926657263751763, 'memory_mb': 1904.30078125}\n",
      "Step 460: {'train_loss': 0.5446437001228333, 'lr': 0.0007856135401974612, 'memory_mb': 1904.30078125}\n",
      "Step 470: {'train_loss': 0.5140273571014404, 'lr': 0.0007785613540197461, 'memory_mb': 1904.30078125}\n",
      "Step 480: {'train_loss': 0.6137562990188599, 'lr': 0.000771509167842031, 'memory_mb': 1904.30078125}\n",
      "Step 490: {'train_loss': 0.5561715960502625, 'lr': 0.000764456981664316, 'memory_mb': 1904.30078125}\n",
      "Step 500: {'train_loss': 0.6877123117446899, 'lr': 0.0007574047954866009, 'memory_mb': 1904.30078125}\n",
      "Step 504: {'eval_loss': 1.5281785540282726, 'memory_mb': 1904.30078125}\n",
      "Step 510: {'train_loss': 0.410482257604599, 'lr': 0.0007503526093088858, 'memory_mb': 1904.30078125}\n",
      "Step 520: {'train_loss': 0.43613308668136597, 'lr': 0.0007433004231311706, 'memory_mb': 1904.30078125}\n",
      "Step 530: {'train_loss': 0.4114627242088318, 'lr': 0.0007362482369534556, 'memory_mb': 1904.80078125}\n",
      "Step 540: {'train_loss': 0.42327025532722473, 'lr': 0.0007291960507757405, 'memory_mb': 1904.80078125}\n",
      "Step 550: {'train_loss': 0.44751685857772827, 'lr': 0.0007221438645980254, 'memory_mb': 1904.80078125}\n",
      "Step 560: {'train_loss': 0.4826757609844208, 'lr': 0.0007150916784203103, 'memory_mb': 1904.80078125}\n",
      "Step 567: {'eval_loss': 1.5716436356306076, 'memory_mb': 1904.80078125}\n",
      "Step 570: {'train_loss': 0.3414716422557831, 'lr': 0.0007080394922425953, 'memory_mb': 1904.80078125}\n",
      "Step 580: {'train_loss': 0.3626076281070709, 'lr': 0.0007009873060648802, 'memory_mb': 1904.80078125}\n",
      "Step 590: {'train_loss': 0.3389052450656891, 'lr': 0.0006939351198871651, 'memory_mb': 1904.80078125}\n",
      "Step 600: {'train_loss': 0.3678736388683319, 'lr': 0.00068688293370945, 'memory_mb': 1904.80078125}\n",
      "Step 610: {'train_loss': 0.40835899114608765, 'lr': 0.000679830747531735, 'memory_mb': 1904.80078125}\n",
      "Step 620: {'train_loss': 0.4238930344581604, 'lr': 0.0006727785613540198, 'memory_mb': 1904.80078125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 630: {'eval_loss': 1.6624111756682396, 'memory_mb': 1904.80078125}\n",
      "Step 630: {'train_loss': 0.3799760937690735, 'lr': 0.0006657263751763047, 'memory_mb': 1904.80078125}\n",
      "Step 640: {'train_loss': 0.3471374213695526, 'lr': 0.0006586741889985895, 'memory_mb': 1904.80078125}\n",
      "Step 650: {'train_loss': 0.3375658094882965, 'lr': 0.0006516220028208744, 'memory_mb': 1904.80078125}\n",
      "Step 660: {'train_loss': 0.3278222382068634, 'lr': 0.0006445698166431593, 'memory_mb': 1904.80078125}\n",
      "Step 670: {'train_loss': 0.29686757922172546, 'lr': 0.0006375176304654443, 'memory_mb': 1904.80078125}\n",
      "Step 680: {'train_loss': 0.40701600909233093, 'lr': 0.0006304654442877292, 'memory_mb': 1904.80078125}\n",
      "Step 690: {'train_loss': 0.32295694947242737, 'lr': 0.0006234132581100141, 'memory_mb': 1904.80078125}\n",
      "Step 693: {'eval_loss': 1.7043009586632252, 'memory_mb': 1904.80078125}\n",
      "Step 700: {'train_loss': 0.2612331211566925, 'lr': 0.000616361071932299, 'memory_mb': 1904.80078125}\n",
      "Step 710: {'train_loss': 0.2712968587875366, 'lr': 0.000609308885754584, 'memory_mb': 1904.80078125}\n",
      "Step 720: {'train_loss': 0.30657970905303955, 'lr': 0.0006022566995768688, 'memory_mb': 1904.80078125}\n",
      "Step 730: {'train_loss': 0.3126470446586609, 'lr': 0.0005952045133991537, 'memory_mb': 1904.80078125}\n",
      "Step 740: {'train_loss': 0.26649948954582214, 'lr': 0.0005881523272214386, 'memory_mb': 1904.80078125}\n",
      "Step 750: {'train_loss': 0.28316137194633484, 'lr': 0.0005811001410437236, 'memory_mb': 1904.80078125}\n",
      "Step 756: {'eval_loss': 1.8304358571767807, 'memory_mb': 1904.80078125}\n",
      "Step 760: {'train_loss': 0.21809393167495728, 'lr': 0.0005740479548660085, 'memory_mb': 1904.80078125}\n",
      "Step 770: {'train_loss': 0.20741955935955048, 'lr': 0.0005669957686882934, 'memory_mb': 1904.80078125}\n",
      "Step 780: {'train_loss': 0.2158755213022232, 'lr': 0.0005599435825105783, 'memory_mb': 1904.80078125}\n",
      "Step 790: {'train_loss': 0.21579930186271667, 'lr': 0.0005528913963328632, 'memory_mb': 1904.80078125}\n",
      "Step 800: {'train_loss': 0.2499166876077652, 'lr': 0.0005458392101551482, 'memory_mb': 1904.80078125}\n",
      "Step 810: {'train_loss': 0.2591334581375122, 'lr': 0.0005387870239774331, 'memory_mb': 1904.80078125}\n",
      "Step 819: {'eval_loss': 1.8699319325387478, 'memory_mb': 1904.80078125}\n",
      "Step 820: {'train_loss': 0.20621027052402496, 'lr': 0.0005317348377997179, 'memory_mb': 1904.80078125}\n",
      "Step 830: {'train_loss': 0.17664486169815063, 'lr': 0.0005246826516220028, 'memory_mb': 1904.80078125}\n",
      "Step 840: {'train_loss': 0.21396231651306152, 'lr': 0.0005176304654442878, 'memory_mb': 1904.80078125}\n",
      "Step 850: {'train_loss': 0.22948764264583588, 'lr': 0.0005105782792665727, 'memory_mb': 1904.80078125}\n",
      "Step 860: {'train_loss': 0.179764524102211, 'lr': 0.0005035260930888576, 'memory_mb': 1904.80078125}\n",
      "Step 870: {'train_loss': 0.23017533123493195, 'lr': 0.0004964739069111424, 'memory_mb': 1904.80078125}\n",
      "Step 880: {'train_loss': 0.21477356553077698, 'lr': 0.0004894217207334273, 'memory_mb': 1904.80078125}\n",
      "Step 882: {'eval_loss': 1.9445668049156666, 'memory_mb': 1904.80078125}\n",
      "Step 890: {'train_loss': 0.1793607622385025, 'lr': 0.0004823695345557123, 'memory_mb': 1904.80078125}\n",
      "Step 900: {'train_loss': 0.17607057094573975, 'lr': 0.0004753173483779972, 'memory_mb': 1904.80078125}\n",
      "Step 910: {'train_loss': 0.20638543367385864, 'lr': 0.0004682651622002821, 'memory_mb': 1904.80078125}\n",
      "Step 920: {'train_loss': 0.17630407214164734, 'lr': 0.00046121297602256704, 'memory_mb': 1904.80078125}\n",
      "Step 930: {'train_loss': 0.17209696769714355, 'lr': 0.0004541607898448519, 'memory_mb': 1904.80078125}\n",
      "Step 940: {'train_loss': 0.18477605283260345, 'lr': 0.00044710860366713683, 'memory_mb': 1904.80078125}\n",
      "Step 945: {'eval_loss': 1.9978145249187946, 'memory_mb': 1904.80078125}\n",
      "Step 950: {'train_loss': 0.14904405176639557, 'lr': 0.0004400564174894217, 'memory_mb': 1904.80078125}\n",
      "Step 960: {'train_loss': 0.16917484998703003, 'lr': 0.0004330042313117066, 'memory_mb': 1904.80078125}\n",
      "Step 970: {'train_loss': 0.1665041744709015, 'lr': 0.00042595204513399155, 'memory_mb': 1904.80078125}\n",
      "Step 980: {'train_loss': 0.1473461538553238, 'lr': 0.00041889985895627647, 'memory_mb': 1904.80078125}\n",
      "Step 990: {'train_loss': 0.1542530357837677, 'lr': 0.0004118476727785614, 'memory_mb': 1904.80078125}\n",
      "Step 1000: {'train_loss': 0.16721059381961823, 'lr': 0.00040479548660084626, 'memory_mb': 1904.80078125}\n",
      "Step 1008: {'eval_loss': 2.0297940596938133, 'memory_mb': 1904.80078125}\n",
      "Step 1010: {'train_loss': 0.12227234244346619, 'lr': 0.0003977433004231312, 'memory_mb': 1904.80078125}\n",
      "Step 1020: {'train_loss': 0.12547028064727783, 'lr': 0.0003906911142454161, 'memory_mb': 1904.80078125}\n",
      "Step 1030: {'train_loss': 0.13397109508514404, 'lr': 0.00038363892806770103, 'memory_mb': 1904.80078125}\n",
      "Step 1040: {'train_loss': 0.15110664069652557, 'lr': 0.0003765867418899859, 'memory_mb': 1904.80078125}\n",
      "Step 1050: {'train_loss': 0.13580405712127686, 'lr': 0.00036953455571227077, 'memory_mb': 1904.80078125}\n",
      "Step 1060: {'train_loss': 0.1504349708557129, 'lr': 0.0003624823695345557, 'memory_mb': 1904.80078125}\n",
      "Step 1070: {'train_loss': 0.14395633339881897, 'lr': 0.0003554301833568406, 'memory_mb': 1904.80078125}\n",
      "Step 1071: {'eval_loss': 2.1101596914231777, 'memory_mb': 1904.80078125}\n",
      "Step 1080: {'train_loss': 0.10743246227502823, 'lr': 0.00034837799717912554, 'memory_mb': 1904.80078125}\n",
      "Step 1090: {'train_loss': 0.11500678956508636, 'lr': 0.00034132581100141047, 'memory_mb': 1904.80078125}\n",
      "Step 1100: {'train_loss': 0.11832279711961746, 'lr': 0.00033427362482369534, 'memory_mb': 1904.80078125}\n",
      "Step 1110: {'train_loss': 0.11987980455160141, 'lr': 0.00032722143864598026, 'memory_mb': 1904.80078125}\n",
      "Step 1120: {'train_loss': 0.12697908282279968, 'lr': 0.0003201692524682652, 'memory_mb': 1904.80078125}\n",
      "Step 1130: {'train_loss': 0.11786895245313644, 'lr': 0.0003131170662905501, 'memory_mb': 1904.80078125}\n",
      "Step 1134: {'eval_loss': 2.1325072944164276, 'memory_mb': 1904.80078125}\n",
      "Step 1140: {'train_loss': 0.10231627523899078, 'lr': 0.00030606488011283503, 'memory_mb': 1904.80078125}\n",
      "Step 1150: {'train_loss': 0.09905995428562164, 'lr': 0.00029901269393511985, 'memory_mb': 1904.80078125}\n",
      "Step 1160: {'train_loss': 0.11996213346719742, 'lr': 0.00029196050775740477, 'memory_mb': 1904.80078125}\n",
      "Step 1170: {'train_loss': 0.1170978918671608, 'lr': 0.0002849083215796897, 'memory_mb': 1904.80078125}\n",
      "Step 1180: {'train_loss': 0.1082490012049675, 'lr': 0.0002778561354019746, 'memory_mb': 1904.80078125}\n",
      "Step 1190: {'train_loss': 0.1146506816148758, 'lr': 0.00027080394922425954, 'memory_mb': 1904.80078125}\n",
      "Step 1197: {'eval_loss': 2.1433958895504475, 'memory_mb': 1904.80078125}\n",
      "Step 1200: {'train_loss': 0.09620460122823715, 'lr': 0.0002637517630465444, 'memory_mb': 1904.80078125}\n",
      "Step 1210: {'train_loss': 0.10834081470966339, 'lr': 0.00025669957686882933, 'memory_mb': 1904.80078125}\n",
      "Step 1220: {'train_loss': 0.10144927352666855, 'lr': 0.00024964739069111426, 'memory_mb': 1904.80078125}\n",
      "Step 1230: {'train_loss': 0.0950969010591507, 'lr': 0.00024259520451339918, 'memory_mb': 1904.80078125}\n",
      "Step 1240: {'train_loss': 0.1064402312040329, 'lr': 0.00023554301833568405, 'memory_mb': 1904.80078125}\n",
      "Step 1250: {'train_loss': 0.11501077562570572, 'lr': 0.00022849083215796897, 'memory_mb': 1904.80078125}\n",
      "Step 1260: {'eval_loss': 2.201548870652914, 'memory_mb': 1904.80078125}\n",
      "Step 1260: {'train_loss': 0.09792201966047287, 'lr': 0.00022143864598025387, 'memory_mb': 1904.80078125}\n",
      "Step 1270: {'train_loss': 0.08666753023862839, 'lr': 0.0002143864598025388, 'memory_mb': 1904.80078125}\n",
      "Step 1280: {'train_loss': 0.08865313977003098, 'lr': 0.00020733427362482372, 'memory_mb': 1904.80078125}\n",
      "Step 1290: {'train_loss': 0.08768410235643387, 'lr': 0.00020028208744710861, 'memory_mb': 1904.80078125}\n",
      "Step 1300: {'train_loss': 0.10250671952962875, 'lr': 0.0001932299012693935, 'memory_mb': 1904.80078125}\n",
      "Step 1310: {'train_loss': 0.09491636604070663, 'lr': 0.0001861777150916784, 'memory_mb': 1904.80078125}\n",
      "Step 1320: {'train_loss': 0.10173309594392776, 'lr': 0.00017912552891396333, 'memory_mb': 1904.80078125}\n",
      "Step 1323: {'eval_loss': 2.2474203556776047, 'memory_mb': 1904.80078125}\n",
      "Step 1330: {'train_loss': 0.09313801676034927, 'lr': 0.00017207334273624825, 'memory_mb': 1904.80078125}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1340: {'train_loss': 0.09447171539068222, 'lr': 0.00016502115655853315, 'memory_mb': 1904.80078125}\n",
      "Step 1350: {'train_loss': 0.08813205361366272, 'lr': 0.00015796897038081807, 'memory_mb': 1904.80078125}\n",
      "Step 1360: {'train_loss': 0.09395182132720947, 'lr': 0.00015091678420310294, 'memory_mb': 1904.80078125}\n",
      "Step 1370: {'train_loss': 0.08865999430418015, 'lr': 0.00014386459802538787, 'memory_mb': 1904.80078125}\n",
      "Step 1380: {'train_loss': 0.09107517451047897, 'lr': 0.0001368124118476728, 'memory_mb': 1904.80078125}\n",
      "Step 1386: {'eval_loss': 2.2806325890123844, 'memory_mb': 1904.80078125}\n",
      "Step 1390: {'train_loss': 0.07579607516527176, 'lr': 0.0001297602256699577, 'memory_mb': 1904.80078125}\n",
      "Step 1400: {'train_loss': 0.07770518958568573, 'lr': 0.0001227080394922426, 'memory_mb': 1904.80078125}\n",
      "Step 1410: {'train_loss': 0.08015798777341843, 'lr': 0.00011565585331452751, 'memory_mb': 1904.80078125}\n",
      "Step 1420: {'train_loss': 0.08342104405164719, 'lr': 0.00010860366713681242, 'memory_mb': 1904.80078125}\n",
      "Step 1430: {'train_loss': 0.08533288538455963, 'lr': 0.00010155148095909731, 'memory_mb': 1904.80078125}\n",
      "Step 1440: {'train_loss': 0.08434528112411499, 'lr': 9.449929478138224e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1449: {'eval_loss': 2.28780235722661, 'memory_mb': 1904.80078125}\n",
      "Step 1450: {'train_loss': 0.0839148759841919, 'lr': 8.744710860366715e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1460: {'train_loss': 0.08289983123540878, 'lr': 8.039492242595204e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1470: {'train_loss': 0.08360899239778519, 'lr': 7.334273624823695e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1480: {'train_loss': 0.07866125553846359, 'lr': 6.629055007052185e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1490: {'train_loss': 0.08301632851362228, 'lr': 5.923836389280677e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1500: {'train_loss': 0.08740679919719696, 'lr': 5.2186177715091684e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1510: {'train_loss': 0.08483852446079254, 'lr': 4.513399153737659e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1512: {'eval_loss': 2.3047262504696846, 'memory_mb': 1904.80078125}\n",
      "Step 1520: {'train_loss': 0.074764184653759, 'lr': 3.80818053596615e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1530: {'train_loss': 0.07731565833091736, 'lr': 3.10296191819464e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1540: {'train_loss': 0.08219356089830399, 'lr': 2.3977433004231315e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1550: {'train_loss': 0.08760486543178558, 'lr': 1.692524682651622e-05, 'memory_mb': 1904.80078125}\n",
      "Step 1560: {'train_loss': 0.0846184715628624, 'lr': 9.87306064880113e-06, 'memory_mb': 1904.80078125}\n",
      "Step 1570: {'train_loss': 0.08156020939350128, 'lr': 2.8208744710860367e-06, 'memory_mb': 1904.80078125}\n",
      "Step 1575: {'eval_loss': 2.329533077776432, 'memory_mb': 1904.80078125}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.32953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1904.80078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.08156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_warmup_20250425_232453\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/andv0pbe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_232453-andv0pbe/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a90b2a",
   "metadata": {},
   "source": [
    "### Higher Weight Decay (0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786923e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_232701-3optkkka\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_warmup_20250425_232701\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/3optkkka\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 1.910828025477707e-06, 'memory_mb': 1898.09765625}\n",
      "Step 10: {'train_loss': 5.76346492767334, 'lr': 2.1019108280254774e-05, 'memory_mb': 1901.09765625}\n",
      "Step 20: {'train_loss': 2.1713926792144775, 'lr': 4.012738853503184e-05, 'memory_mb': 1902.09765625}\n",
      "Step 30: {'train_loss': 1.571203351020813, 'lr': 5.923566878980892e-05, 'memory_mb': 1902.09765625}\n",
      "Step 40: {'train_loss': 1.6515851020812988, 'lr': 7.834394904458598e-05, 'memory_mb': 1902.09765625}\n",
      "Step 50: {'train_loss': 1.4757450819015503, 'lr': 9.745222929936304e-05, 'memory_mb': 1902.09765625}\n",
      "Step 60: {'train_loss': 1.5212090015411377, 'lr': 0.00011656050955414013, 'memory_mb': 1902.09765625}\n",
      "Step 63: {'eval_loss': 1.5299494080245495, 'memory_mb': 1904.09765625}\n",
      "Step 70: {'train_loss': 1.5515369176864624, 'lr': 0.0001356687898089172, 'memory_mb': 1904.09765625}\n",
      "Step 80: {'train_loss': 1.6303465366363525, 'lr': 0.00015477707006369425, 'memory_mb': 1904.09765625}\n",
      "Step 90: {'train_loss': 1.2718795537948608, 'lr': 0.00017388535031847131, 'memory_mb': 1904.09765625}\n",
      "Step 100: {'train_loss': 1.0973107814788818, 'lr': 0.00019299363057324838, 'memory_mb': 1904.09765625}\n",
      "Step 110: {'train_loss': 1.1209899187088013, 'lr': 0.00021210191082802544, 'memory_mb': 1904.09765625}\n",
      "Step 120: {'train_loss': 1.354167103767395, 'lr': 0.00023121019108280255, 'memory_mb': 1904.09765625}\n",
      "Step 126: {'eval_loss': 1.272247564047575, 'memory_mb': 1904.09765625}\n",
      "Step 130: {'train_loss': 1.2381287813186646, 'lr': 0.0002503184713375796, 'memory_mb': 1904.09765625}\n",
      "Step 140: {'train_loss': 1.0633939504623413, 'lr': 0.0002694267515923567, 'memory_mb': 1904.09765625}\n",
      "Step 150: {'train_loss': 1.2631443738937378, 'lr': 0.00028853503184713374, 'memory_mb': 1904.09765625}\n",
      "Step 160: {'train_loss': 1.0018428564071655, 'lr': 0.0002991537376586742, 'memory_mb': 1904.09765625}\n",
      "Step 170: {'train_loss': 1.3020960092544556, 'lr': 0.00029703808180535964, 'memory_mb': 1904.09765625}\n",
      "Step 180: {'train_loss': 1.2096349000930786, 'lr': 0.0002949224259520451, 'memory_mb': 1904.09765625}\n",
      "Step 189: {'eval_loss': 1.250329878181219, 'memory_mb': 1904.09765625}\n",
      "Step 190: {'train_loss': 1.3578284978866577, 'lr': 0.0002928067700987306, 'memory_mb': 1904.09765625}\n",
      "Step 200: {'train_loss': 1.1941194534301758, 'lr': 0.00029069111424541606, 'memory_mb': 1904.09765625}\n",
      "Step 210: {'train_loss': 0.9994441866874695, 'lr': 0.0002885754583921015, 'memory_mb': 1904.09765625}\n",
      "Step 220: {'train_loss': 1.0615127086639404, 'lr': 0.000286459802538787, 'memory_mb': 1904.09765625}\n",
      "Step 230: {'train_loss': 1.0311137437820435, 'lr': 0.0002843441466854725, 'memory_mb': 1904.09765625}\n",
      "Step 240: {'train_loss': 1.2655212879180908, 'lr': 0.00028222849083215794, 'memory_mb': 1904.09765625}\n",
      "Step 250: {'train_loss': 1.0590159893035889, 'lr': 0.0002801128349788434, 'memory_mb': 1904.09765625}\n",
      "Step 252: {'eval_loss': 1.2557619363069534, 'memory_mb': 1904.09765625}\n",
      "Step 260: {'train_loss': 0.9392563104629517, 'lr': 0.0002779971791255289, 'memory_mb': 1904.09765625}\n",
      "Step 270: {'train_loss': 1.4298053979873657, 'lr': 0.00027588152327221436, 'memory_mb': 1904.09765625}\n",
      "Step 280: {'train_loss': 1.2797423601150513, 'lr': 0.0002737658674188998, 'memory_mb': 1904.09765625}\n",
      "Step 290: {'train_loss': 1.0263420343399048, 'lr': 0.0002716502115655853, 'memory_mb': 1904.09765625}\n",
      "Step 300: {'train_loss': 0.934032678604126, 'lr': 0.0002695345557122708, 'memory_mb': 1904.09765625}\n",
      "Step 310: {'train_loss': 1.169266939163208, 'lr': 0.00026741889985895624, 'memory_mb': 1904.09765625}\n",
      "Step 315: {'eval_loss': 1.2704027071595192, 'memory_mb': 1904.09765625}\n",
      "Step 320: {'train_loss': 0.8679962754249573, 'lr': 0.0002653032440056417, 'memory_mb': 1904.09765625}\n",
      "Step 330: {'train_loss': 1.0300426483154297, 'lr': 0.0002631875881523272, 'memory_mb': 1904.09765625}\n",
      "Step 340: {'train_loss': 0.9837068319320679, 'lr': 0.00026107193229901266, 'memory_mb': 1904.09765625}\n",
      "Step 350: {'train_loss': 1.0478668212890625, 'lr': 0.00025895627644569817, 'memory_mb': 1904.09765625}\n",
      "Step 360: {'train_loss': 0.9433963894844055, 'lr': 0.0002568406205923836, 'memory_mb': 1904.09765625}\n",
      "Step 370: {'train_loss': 0.9916133284568787, 'lr': 0.0002547249647390691, 'memory_mb': 1904.09765625}\n",
      "Step 378: {'eval_loss': 1.3075388073921204, 'memory_mb': 1904.09765625}\n",
      "Step 380: {'train_loss': 0.8903243541717529, 'lr': 0.0002526093088857546, 'memory_mb': 1904.09765625}\n",
      "Step 390: {'train_loss': 0.9416882395744324, 'lr': 0.00025049365303244004, 'memory_mb': 1904.09765625}\n",
      "Step 400: {'train_loss': 0.8642483949661255, 'lr': 0.0002483779971791255, 'memory_mb': 1904.09765625}\n",
      "Step 410: {'train_loss': 0.8215994238853455, 'lr': 0.00024626234132581095, 'memory_mb': 1904.09765625}\n",
      "Step 420: {'train_loss': 0.8893325924873352, 'lr': 0.00024414668547249646, 'memory_mb': 1904.09765625}\n",
      "Step 430: {'train_loss': 0.9580628275871277, 'lr': 0.00024203102961918192, 'memory_mb': 1904.09765625}\n",
      "Step 440: {'train_loss': 0.9650703072547913, 'lr': 0.0002399153737658674, 'memory_mb': 1904.09765625}\n",
      "Step 441: {'eval_loss': 1.344992257654667, 'memory_mb': 1904.09765625}\n",
      "Step 450: {'train_loss': 0.7361518740653992, 'lr': 0.00023779971791255286, 'memory_mb': 1904.09765625}\n",
      "Step 460: {'train_loss': 0.9009206891059875, 'lr': 0.00023568406205923834, 'memory_mb': 1904.09765625}\n",
      "Step 470: {'train_loss': 0.8309310674667358, 'lr': 0.00023356840620592382, 'memory_mb': 1904.09765625}\n",
      "Step 480: {'train_loss': 0.9631812572479248, 'lr': 0.0002314527503526093, 'memory_mb': 1904.09765625}\n",
      "Step 490: {'train_loss': 0.861521303653717, 'lr': 0.00022933709449929476, 'memory_mb': 1904.09765625}\n",
      "Step 500: {'train_loss': 1.0694822072982788, 'lr': 0.00022722143864598024, 'memory_mb': 1904.09765625}\n",
      "Step 504: {'eval_loss': 1.3922715447843075, 'memory_mb': 1904.09765625}\n",
      "Step 510: {'train_loss': 0.7613101601600647, 'lr': 0.00022510578279266572, 'memory_mb': 1904.09765625}\n",
      "Step 520: {'train_loss': 0.7479026317596436, 'lr': 0.00022299012693935118, 'memory_mb': 1904.09765625}\n",
      "Step 530: {'train_loss': 0.6939547061920166, 'lr': 0.00022087447108603663, 'memory_mb': 1904.09765625}\n",
      "Step 540: {'train_loss': 0.6800351738929749, 'lr': 0.00021875881523272212, 'memory_mb': 1904.09765625}\n",
      "Step 550: {'train_loss': 0.742412805557251, 'lr': 0.0002166431593794076, 'memory_mb': 1904.09765625}\n",
      "Step 560: {'train_loss': 0.8180548548698425, 'lr': 0.00021452750352609308, 'memory_mb': 1904.09765625}\n",
      "Step 567: {'eval_loss': 1.4646370932459831, 'memory_mb': 1904.09765625}\n",
      "Step 570: {'train_loss': 0.6039210557937622, 'lr': 0.00021241184767277854, 'memory_mb': 1904.09765625}\n",
      "Step 580: {'train_loss': 0.6426727175712585, 'lr': 0.00021029619181946402, 'memory_mb': 1904.09765625}\n",
      "Step 590: {'train_loss': 0.6035541296005249, 'lr': 0.0002081805359661495, 'memory_mb': 1904.09765625}\n",
      "Step 600: {'train_loss': 0.6712140440940857, 'lr': 0.00020606488011283499, 'memory_mb': 1904.09765625}\n",
      "Step 610: {'train_loss': 0.6710449457168579, 'lr': 0.00020394922425952044, 'memory_mb': 1904.59765625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 620: {'train_loss': 0.7454532980918884, 'lr': 0.0002018335684062059, 'memory_mb': 1904.59765625}\n",
      "Step 630: {'eval_loss': 1.4909956641495228, 'memory_mb': 1904.59765625}\n",
      "Step 630: {'train_loss': 0.7710346579551697, 'lr': 0.00019971791255289138, 'memory_mb': 1904.59765625}\n",
      "Step 640: {'train_loss': 0.6100303530693054, 'lr': 0.00019760225669957683, 'memory_mb': 1904.59765625}\n",
      "Step 650: {'train_loss': 0.7486898303031921, 'lr': 0.00019548660084626232, 'memory_mb': 1904.59765625}\n",
      "Step 660: {'train_loss': 0.6698547601699829, 'lr': 0.0001933709449929478, 'memory_mb': 1904.59765625}\n",
      "Step 670: {'train_loss': 0.5320841670036316, 'lr': 0.00019125528913963328, 'memory_mb': 1904.59765625}\n",
      "Step 680: {'train_loss': 0.7093816995620728, 'lr': 0.00018913963328631874, 'memory_mb': 1904.59765625}\n",
      "Step 690: {'train_loss': 0.574926495552063, 'lr': 0.00018702397743300422, 'memory_mb': 1904.59765625}\n",
      "Step 693: {'eval_loss': 1.5459104739129543, 'memory_mb': 1904.59765625}\n",
      "Step 700: {'train_loss': 0.5262984037399292, 'lr': 0.0001849083215796897, 'memory_mb': 1904.59765625}\n",
      "Step 710: {'train_loss': 0.539124071598053, 'lr': 0.00018279266572637518, 'memory_mb': 1904.59765625}\n",
      "Step 720: {'train_loss': 0.61111980676651, 'lr': 0.0001806770098730606, 'memory_mb': 1904.59765625}\n",
      "Step 730: {'train_loss': 0.6319924592971802, 'lr': 0.0001785613540197461, 'memory_mb': 1904.59765625}\n",
      "Step 740: {'train_loss': 0.5141059160232544, 'lr': 0.00017644569816643158, 'memory_mb': 1904.59765625}\n",
      "Step 750: {'train_loss': 0.46983373165130615, 'lr': 0.00017433004231311706, 'memory_mb': 1904.59765625}\n",
      "Step 756: {'eval_loss': 1.5875448286533356, 'memory_mb': 1904.59765625}\n",
      "Step 760: {'train_loss': 0.4649569094181061, 'lr': 0.00017221438645980252, 'memory_mb': 1904.59765625}\n",
      "Step 770: {'train_loss': 0.3940180242061615, 'lr': 0.000170098730606488, 'memory_mb': 1904.59765625}\n",
      "Step 780: {'train_loss': 0.4065459370613098, 'lr': 0.00016798307475317348, 'memory_mb': 1904.59765625}\n",
      "Step 790: {'train_loss': 0.4578281342983246, 'lr': 0.00016586741889985896, 'memory_mb': 1904.59765625}\n",
      "Step 800: {'train_loss': 0.5200895667076111, 'lr': 0.00016375176304654442, 'memory_mb': 1904.59765625}\n",
      "Step 810: {'train_loss': 0.5274001359939575, 'lr': 0.0001616361071932299, 'memory_mb': 1904.59765625}\n",
      "Step 819: {'eval_loss': 1.6265059150755405, 'memory_mb': 1904.59765625}\n",
      "Step 820: {'train_loss': 0.48594290018081665, 'lr': 0.00015952045133991536, 'memory_mb': 1904.59765625}\n",
      "Step 830: {'train_loss': 0.3604522943496704, 'lr': 0.0001574047954866008, 'memory_mb': 1904.59765625}\n",
      "Step 840: {'train_loss': 0.4547361135482788, 'lr': 0.0001552891396332863, 'memory_mb': 1904.59765625}\n",
      "Step 850: {'train_loss': 0.41212141513824463, 'lr': 0.00015317348377997178, 'memory_mb': 1904.59765625}\n",
      "Step 860: {'train_loss': 0.3537493050098419, 'lr': 0.00015105782792665726, 'memory_mb': 1904.59765625}\n",
      "Step 870: {'train_loss': 0.4971933364868164, 'lr': 0.00014894217207334271, 'memory_mb': 1904.59765625}\n",
      "Step 880: {'train_loss': 0.42572227120399475, 'lr': 0.0001468265162200282, 'memory_mb': 1904.59765625}\n",
      "Step 882: {'eval_loss': 1.7155404277145863, 'memory_mb': 1904.59765625}\n",
      "Step 890: {'train_loss': 0.444441556930542, 'lr': 0.00014471086036671368, 'memory_mb': 1904.59765625}\n",
      "Step 900: {'train_loss': 0.35031941533088684, 'lr': 0.00014259520451339913, 'memory_mb': 1904.59765625}\n",
      "Step 910: {'train_loss': 0.4967562258243561, 'lr': 0.00014047954866008462, 'memory_mb': 1904.59765625}\n",
      "Step 920: {'train_loss': 0.34715062379837036, 'lr': 0.0001383638928067701, 'memory_mb': 1904.59765625}\n",
      "Step 930: {'train_loss': 0.42357686161994934, 'lr': 0.00013624823695345556, 'memory_mb': 1904.59765625}\n",
      "Step 940: {'train_loss': 0.37846803665161133, 'lr': 0.00013413258110014104, 'memory_mb': 1904.59765625}\n",
      "Step 945: {'eval_loss': 1.7615814730525017, 'memory_mb': 1904.59765625}\n",
      "Step 950: {'train_loss': 0.3244827389717102, 'lr': 0.0001320169252468265, 'memory_mb': 1904.59765625}\n",
      "Step 960: {'train_loss': 0.39568009972572327, 'lr': 0.00012990126939351198, 'memory_mb': 1904.59765625}\n",
      "Step 970: {'train_loss': 0.389522522687912, 'lr': 0.00012778561354019746, 'memory_mb': 1904.59765625}\n",
      "Step 980: {'train_loss': 0.3321835398674011, 'lr': 0.00012566995768688291, 'memory_mb': 1904.59765625}\n",
      "Step 990: {'train_loss': 0.33663973212242126, 'lr': 0.0001235543018335684, 'memory_mb': 1904.59765625}\n",
      "Step 1000: {'train_loss': 0.4172949492931366, 'lr': 0.00012143864598025387, 'memory_mb': 1904.59765625}\n",
      "Step 1008: {'eval_loss': 1.8620108030736446, 'memory_mb': 1904.59765625}\n",
      "Step 1010: {'train_loss': 0.28161635994911194, 'lr': 0.00011932299012693933, 'memory_mb': 1904.59765625}\n",
      "Step 1020: {'train_loss': 0.2720924913883209, 'lr': 0.00011720733427362482, 'memory_mb': 1904.59765625}\n",
      "Step 1030: {'train_loss': 0.274032860994339, 'lr': 0.00011509167842031029, 'memory_mb': 1904.59765625}\n",
      "Step 1040: {'train_loss': 0.3680418133735657, 'lr': 0.00011297602256699577, 'memory_mb': 1904.59765625}\n",
      "Step 1050: {'train_loss': 0.29031750559806824, 'lr': 0.00011086036671368122, 'memory_mb': 1904.59765625}\n",
      "Step 1060: {'train_loss': 0.41948238015174866, 'lr': 0.0001087447108603667, 'memory_mb': 1904.59765625}\n",
      "Step 1070: {'train_loss': 0.2812931537628174, 'lr': 0.00010662905500705217, 'memory_mb': 1904.59765625}\n",
      "Step 1071: {'eval_loss': 1.8824685476720333, 'memory_mb': 1904.59765625}\n",
      "Step 1080: {'train_loss': 0.2512136697769165, 'lr': 0.00010451339915373766, 'memory_mb': 1904.59765625}\n",
      "Step 1090: {'train_loss': 0.2781979441642761, 'lr': 0.00010239774330042313, 'memory_mb': 1904.59765625}\n",
      "Step 1100: {'train_loss': 0.24711249768733978, 'lr': 0.0001002820874471086, 'memory_mb': 1904.59765625}\n",
      "Step 1110: {'train_loss': 0.3094403147697449, 'lr': 9.816643159379406e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1120: {'train_loss': 0.34465456008911133, 'lr': 9.605077574047955e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1130: {'train_loss': 0.3550054132938385, 'lr': 9.393511988716502e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1134: {'eval_loss': 1.9784165136516094, 'memory_mb': 1904.59765625}\n",
      "Step 1140: {'train_loss': 0.2484864741563797, 'lr': 9.18194640338505e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1150: {'train_loss': 0.2752312123775482, 'lr': 8.970380818053595e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1160: {'train_loss': 0.2864477336406708, 'lr': 8.758815232722142e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1170: {'train_loss': 0.3300096392631531, 'lr': 8.54724964739069e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1180: {'train_loss': 0.22454963624477386, 'lr': 8.335684062059237e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1190: {'train_loss': 0.26540517807006836, 'lr': 8.124118476727786e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1197: {'eval_loss': 2.017070960253477, 'memory_mb': 1904.59765625}\n",
      "Step 1200: {'train_loss': 0.20832960307598114, 'lr': 7.912552891396331e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1210: {'train_loss': 0.24435213208198547, 'lr': 7.70098730606488e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1220: {'train_loss': 0.27201423048973083, 'lr': 7.489421720733426e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1230: {'train_loss': 0.21684610843658447, 'lr': 7.277856135401975e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1240: {'train_loss': 0.23957908153533936, 'lr': 7.066290550070521e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1250: {'train_loss': 0.250293493270874, 'lr': 6.854724964739068e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1260: {'eval_loss': 2.075293902307749, 'memory_mb': 1904.59765625}\n",
      "Step 1260: {'train_loss': 0.25122442841529846, 'lr': 6.643159379407615e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1270: {'train_loss': 0.2032664269208908, 'lr': 6.431593794076164e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1280: {'train_loss': 0.21378940343856812, 'lr': 6.22002820874471e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1290: {'train_loss': 0.18123610317707062, 'lr': 6.008462623413257e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1300: {'train_loss': 0.21598847210407257, 'lr': 5.796897038081805e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1310: {'train_loss': 0.2270004153251648, 'lr': 5.585331452750352e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1320: {'train_loss': 0.28085431456565857, 'lr': 5.3737658674188994e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1323: {'eval_loss': 2.109397180378437, 'memory_mb': 1904.59765625}\n",
      "Step 1330: {'train_loss': 0.24444004893302917, 'lr': 5.162200282087447e-05, 'memory_mb': 1904.59765625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1340: {'train_loss': 0.20831605792045593, 'lr': 4.950634696755994e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1350: {'train_loss': 0.17868943512439728, 'lr': 4.7390691114245414e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1360: {'train_loss': 0.20014694333076477, 'lr': 4.527503526093088e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1370: {'train_loss': 0.2445942461490631, 'lr': 4.315937940761636e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1380: {'train_loss': 0.2106448858976364, 'lr': 4.1043723554301835e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1386: {'eval_loss': 2.1494819447398186, 'memory_mb': 1904.59765625}\n",
      "Step 1390: {'train_loss': 0.17952527105808258, 'lr': 3.8928067700987303e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1400: {'train_loss': 0.1721239537000656, 'lr': 3.681241184767278e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1410: {'train_loss': 0.2317851185798645, 'lr': 3.469675599435825e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1420: {'train_loss': 0.16210952401161194, 'lr': 3.258110014104372e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1430: {'train_loss': 0.19272948801517487, 'lr': 3.0465444287729193e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1440: {'train_loss': 0.18213503062725067, 'lr': 2.834978843441467e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1449: {'eval_loss': 2.1708256490528584, 'memory_mb': 1904.59765625}\n",
      "Step 1450: {'train_loss': 0.1708376109600067, 'lr': 2.623413258110014e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1460: {'train_loss': 0.2145332247018814, 'lr': 2.411847672778561e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1470: {'train_loss': 0.2062283754348755, 'lr': 2.2002820874471082e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1480: {'train_loss': 0.1999543458223343, 'lr': 1.9887165021156555e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1490: {'train_loss': 0.209603950381279, 'lr': 1.777150916784203e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1500: {'train_loss': 0.20217327773571014, 'lr': 1.5655853314527503e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1510: {'train_loss': 0.18361131846904755, 'lr': 1.3540197461212975e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1512: {'eval_loss': 2.204005278646946, 'memory_mb': 1904.59765625}\n",
      "Step 1520: {'train_loss': 0.18324746191501617, 'lr': 1.1424541607898449e-05, 'memory_mb': 1904.59765625}\n",
      "Step 1530: {'train_loss': 0.1640004813671112, 'lr': 9.30888575458392e-06, 'memory_mb': 1904.59765625}\n",
      "Step 1540: {'train_loss': 0.19555318355560303, 'lr': 7.193229901269393e-06, 'memory_mb': 1904.59765625}\n",
      "Step 1550: {'train_loss': 0.26350918412208557, 'lr': 5.077574047954866e-06, 'memory_mb': 1904.59765625}\n",
      "Step 1560: {'train_loss': 0.22030706703662872, 'lr': 2.9619181946403384e-06, 'memory_mb': 1904.59765625}\n",
      "Step 1570: {'train_loss': 0.20817184448242188, 'lr': 8.462623413258109e-07, 'memory_mb': 1904.59765625}\n",
      "Step 1575: {'eval_loss': 2.214519504457712, 'memory_mb': 1904.59765625}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.21452\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1904.59766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.20817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_warmup_20250425_232701\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/3optkkka\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_232701-3optkkka/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bf468",
   "metadata": {},
   "source": [
    "# Lower Weight Decay (0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4192c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_232909-7umzyrrb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_warmup_20250425_232909\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/7umzyrrb\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 1.910828025477707e-06, 'memory_mb': 1897.97265625}\n",
      "Step 10: {'train_loss': 5.763557434082031, 'lr': 2.1019108280254774e-05, 'memory_mb': 1901.47265625}\n",
      "Step 20: {'train_loss': 2.171449899673462, 'lr': 4.012738853503184e-05, 'memory_mb': 1901.97265625}\n",
      "Step 30: {'train_loss': 1.5711643695831299, 'lr': 5.923566878980892e-05, 'memory_mb': 1901.97265625}\n",
      "Step 40: {'train_loss': 1.6516224145889282, 'lr': 7.834394904458598e-05, 'memory_mb': 1901.97265625}\n",
      "Step 50: {'train_loss': 1.475612998008728, 'lr': 9.745222929936304e-05, 'memory_mb': 1901.97265625}\n",
      "Step 60: {'train_loss': 1.5212557315826416, 'lr': 0.00011656050955414013, 'memory_mb': 1901.97265625}\n",
      "Step 63: {'eval_loss': 1.5299045778810978, 'memory_mb': 1903.97265625}\n",
      "Step 70: {'train_loss': 1.5514835119247437, 'lr': 0.0001356687898089172, 'memory_mb': 1903.97265625}\n",
      "Step 80: {'train_loss': 1.6302460432052612, 'lr': 0.00015477707006369425, 'memory_mb': 1903.97265625}\n",
      "Step 90: {'train_loss': 1.2717525959014893, 'lr': 0.00017388535031847131, 'memory_mb': 1903.97265625}\n",
      "Step 100: {'train_loss': 1.0973756313323975, 'lr': 0.00019299363057324838, 'memory_mb': 1903.97265625}\n",
      "Step 110: {'train_loss': 1.12106192111969, 'lr': 0.00021210191082802544, 'memory_mb': 1903.97265625}\n",
      "Step 120: {'train_loss': 1.3542851209640503, 'lr': 0.00023121019108280255, 'memory_mb': 1903.97265625}\n",
      "Step 126: {'eval_loss': 1.2722260057926178, 'memory_mb': 1903.97265625}\n",
      "Step 130: {'train_loss': 1.2379095554351807, 'lr': 0.0002503184713375796, 'memory_mb': 1903.97265625}\n",
      "Step 140: {'train_loss': 1.0633238554000854, 'lr': 0.0002694267515923567, 'memory_mb': 1903.97265625}\n",
      "Step 150: {'train_loss': 1.2631844282150269, 'lr': 0.00028853503184713374, 'memory_mb': 1903.97265625}\n",
      "Step 160: {'train_loss': 1.0017744302749634, 'lr': 0.0002991537376586742, 'memory_mb': 1903.97265625}\n",
      "Step 170: {'train_loss': 1.3018975257873535, 'lr': 0.00029703808180535964, 'memory_mb': 1903.97265625}\n",
      "Step 180: {'train_loss': 1.2094030380249023, 'lr': 0.0002949224259520451, 'memory_mb': 1903.97265625}\n",
      "Step 189: {'eval_loss': 1.2503585368394852, 'memory_mb': 1903.97265625}\n",
      "Step 190: {'train_loss': 1.357747197151184, 'lr': 0.0002928067700987306, 'memory_mb': 1903.97265625}\n",
      "Step 200: {'train_loss': 1.1938709020614624, 'lr': 0.00029069111424541606, 'memory_mb': 1903.97265625}\n",
      "Step 210: {'train_loss': 0.9990756511688232, 'lr': 0.0002885754583921015, 'memory_mb': 1903.97265625}\n",
      "Step 220: {'train_loss': 1.061363697052002, 'lr': 0.000286459802538787, 'memory_mb': 1903.97265625}\n",
      "Step 230: {'train_loss': 1.030841588973999, 'lr': 0.0002843441466854725, 'memory_mb': 1903.97265625}\n",
      "Step 240: {'train_loss': 1.2650740146636963, 'lr': 0.00028222849083215794, 'memory_mb': 1903.97265625}\n",
      "Step 250: {'train_loss': 1.0587633848190308, 'lr': 0.0002801128349788434, 'memory_mb': 1903.97265625}\n",
      "Step 252: {'eval_loss': 1.2558991983532906, 'memory_mb': 1903.97265625}\n",
      "Step 260: {'train_loss': 0.938758373260498, 'lr': 0.0002779971791255289, 'memory_mb': 1903.97265625}\n",
      "Step 270: {'train_loss': 1.42938232421875, 'lr': 0.00027588152327221436, 'memory_mb': 1903.97265625}\n",
      "Step 280: {'train_loss': 1.2786978483200073, 'lr': 0.0002737658674188998, 'memory_mb': 1903.97265625}\n",
      "Step 290: {'train_loss': 1.0273137092590332, 'lr': 0.0002716502115655853, 'memory_mb': 1903.97265625}\n",
      "Step 300: {'train_loss': 0.9330441951751709, 'lr': 0.0002695345557122708, 'memory_mb': 1903.97265625}\n",
      "Step 310: {'train_loss': 1.1691820621490479, 'lr': 0.00026741889985895624, 'memory_mb': 1903.97265625}\n",
      "Step 315: {'eval_loss': 1.270846540108323, 'memory_mb': 1903.97265625}\n",
      "Step 320: {'train_loss': 0.8686394095420837, 'lr': 0.0002653032440056417, 'memory_mb': 1903.97265625}\n",
      "Step 330: {'train_loss': 1.0289161205291748, 'lr': 0.0002631875881523272, 'memory_mb': 1903.97265625}\n",
      "Step 340: {'train_loss': 0.9826895594596863, 'lr': 0.00026107193229901266, 'memory_mb': 1903.97265625}\n",
      "Step 350: {'train_loss': 1.0470860004425049, 'lr': 0.00025895627644569817, 'memory_mb': 1903.97265625}\n",
      "Step 360: {'train_loss': 0.941843569278717, 'lr': 0.0002568406205923836, 'memory_mb': 1903.97265625}\n",
      "Step 370: {'train_loss': 0.9905133247375488, 'lr': 0.0002547249647390691, 'memory_mb': 1903.97265625}\n",
      "Step 378: {'eval_loss': 1.3084201663732529, 'memory_mb': 1903.97265625}\n",
      "Step 380: {'train_loss': 0.88840651512146, 'lr': 0.0002526093088857546, 'memory_mb': 1903.97265625}\n",
      "Step 390: {'train_loss': 0.9404104948043823, 'lr': 0.00025049365303244004, 'memory_mb': 1903.97265625}\n",
      "Step 400: {'train_loss': 0.8641415238380432, 'lr': 0.0002483779971791255, 'memory_mb': 1903.97265625}\n",
      "Step 410: {'train_loss': 0.8193516731262207, 'lr': 0.00024626234132581095, 'memory_mb': 1903.97265625}\n",
      "Step 420: {'train_loss': 0.8885864019393921, 'lr': 0.00024414668547249646, 'memory_mb': 1903.97265625}\n",
      "Step 430: {'train_loss': 0.9560072422027588, 'lr': 0.00024203102961918192, 'memory_mb': 1903.97265625}\n",
      "Step 440: {'train_loss': 0.9634654521942139, 'lr': 0.0002399153737658674, 'memory_mb': 1903.97265625}\n",
      "Step 441: {'eval_loss': 1.344672940671444, 'memory_mb': 1903.97265625}\n",
      "Step 450: {'train_loss': 0.7356191873550415, 'lr': 0.00023779971791255286, 'memory_mb': 1903.97265625}\n",
      "Step 460: {'train_loss': 0.8992409110069275, 'lr': 0.00023568406205923834, 'memory_mb': 1903.97265625}\n",
      "Step 470: {'train_loss': 0.8278436660766602, 'lr': 0.00023356840620592382, 'memory_mb': 1903.97265625}\n",
      "Step 480: {'train_loss': 0.9599394202232361, 'lr': 0.0002314527503526093, 'memory_mb': 1903.97265625}\n",
      "Step 490: {'train_loss': 0.8608236908912659, 'lr': 0.00022933709449929476, 'memory_mb': 1903.97265625}\n",
      "Step 500: {'train_loss': 1.067283034324646, 'lr': 0.00022722143864598024, 'memory_mb': 1903.97265625}\n",
      "Step 504: {'eval_loss': 1.3950621113181114, 'memory_mb': 1903.97265625}\n",
      "Step 510: {'train_loss': 0.75929194688797, 'lr': 0.00022510578279266572, 'memory_mb': 1903.97265625}\n",
      "Step 520: {'train_loss': 0.7455812692642212, 'lr': 0.00022299012693935118, 'memory_mb': 1903.97265625}\n",
      "Step 530: {'train_loss': 0.6917933225631714, 'lr': 0.00022087447108603663, 'memory_mb': 1903.97265625}\n",
      "Step 540: {'train_loss': 0.6798146367073059, 'lr': 0.00021875881523272212, 'memory_mb': 1903.97265625}\n",
      "Step 550: {'train_loss': 0.7406797409057617, 'lr': 0.0002166431593794076, 'memory_mb': 1903.97265625}\n",
      "Step 560: {'train_loss': 0.8213260173797607, 'lr': 0.00021452750352609308, 'memory_mb': 1903.97265625}\n",
      "Step 567: {'eval_loss': 1.4649959430098534, 'memory_mb': 1904.47265625}\n",
      "Step 570: {'train_loss': 0.5999033451080322, 'lr': 0.00021241184767277854, 'memory_mb': 1904.47265625}\n",
      "Step 580: {'train_loss': 0.6395297050476074, 'lr': 0.00021029619181946402, 'memory_mb': 1904.47265625}\n",
      "Step 590: {'train_loss': 0.6002858877182007, 'lr': 0.0002081805359661495, 'memory_mb': 1904.47265625}\n",
      "Step 600: {'train_loss': 0.6637247204780579, 'lr': 0.00020606488011283499, 'memory_mb': 1904.47265625}\n",
      "Step 610: {'train_loss': 0.665888786315918, 'lr': 0.00020394922425952044, 'memory_mb': 1904.47265625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 620: {'train_loss': 0.7433295249938965, 'lr': 0.0002018335684062059, 'memory_mb': 1904.47265625}\n",
      "Step 630: {'eval_loss': 1.490316767245531, 'memory_mb': 1904.47265625}\n",
      "Step 630: {'train_loss': 0.7680268883705139, 'lr': 0.00019971791255289138, 'memory_mb': 1904.47265625}\n",
      "Step 640: {'train_loss': 0.6067194938659668, 'lr': 0.00019760225669957683, 'memory_mb': 1904.47265625}\n",
      "Step 650: {'train_loss': 0.7430320382118225, 'lr': 0.00019548660084626232, 'memory_mb': 1904.47265625}\n",
      "Step 660: {'train_loss': 0.668258547782898, 'lr': 0.0001933709449929478, 'memory_mb': 1904.47265625}\n",
      "Step 670: {'train_loss': 0.5281335115432739, 'lr': 0.00019125528913963328, 'memory_mb': 1904.47265625}\n",
      "Step 680: {'train_loss': 0.7060790657997131, 'lr': 0.00018913963328631874, 'memory_mb': 1904.47265625}\n",
      "Step 690: {'train_loss': 0.570083737373352, 'lr': 0.00018702397743300422, 'memory_mb': 1904.47265625}\n",
      "Step 693: {'eval_loss': 1.5525021329522133, 'memory_mb': 1904.47265625}\n",
      "Step 700: {'train_loss': 0.5185546875, 'lr': 0.0001849083215796897, 'memory_mb': 1904.47265625}\n",
      "Step 710: {'train_loss': 0.539699137210846, 'lr': 0.00018279266572637518, 'memory_mb': 1904.47265625}\n",
      "Step 720: {'train_loss': 0.6062130928039551, 'lr': 0.0001806770098730606, 'memory_mb': 1904.47265625}\n",
      "Step 730: {'train_loss': 0.6294696927070618, 'lr': 0.0001785613540197461, 'memory_mb': 1904.47265625}\n",
      "Step 740: {'train_loss': 0.5179564952850342, 'lr': 0.00017644569816643158, 'memory_mb': 1904.47265625}\n",
      "Step 750: {'train_loss': 0.4669910669326782, 'lr': 0.00017433004231311706, 'memory_mb': 1904.47265625}\n",
      "Step 756: {'eval_loss': 1.58638009801507, 'memory_mb': 1904.47265625}\n",
      "Step 760: {'train_loss': 0.4602728486061096, 'lr': 0.00017221438645980252, 'memory_mb': 1904.47265625}\n",
      "Step 770: {'train_loss': 0.4021912217140198, 'lr': 0.000170098730606488, 'memory_mb': 1904.47265625}\n",
      "Step 780: {'train_loss': 0.4032096266746521, 'lr': 0.00016798307475317348, 'memory_mb': 1904.47265625}\n",
      "Step 790: {'train_loss': 0.454618364572525, 'lr': 0.00016586741889985896, 'memory_mb': 1904.47265625}\n",
      "Step 800: {'train_loss': 0.5294713377952576, 'lr': 0.00016375176304654442, 'memory_mb': 1904.47265625}\n",
      "Step 810: {'train_loss': 0.5181099772453308, 'lr': 0.0001616361071932299, 'memory_mb': 1904.47265625}\n",
      "Step 819: {'eval_loss': 1.6362035237252712, 'memory_mb': 1904.47265625}\n",
      "Step 820: {'train_loss': 0.48748472332954407, 'lr': 0.00015952045133991536, 'memory_mb': 1904.47265625}\n",
      "Step 830: {'train_loss': 0.3543379008769989, 'lr': 0.0001574047954866008, 'memory_mb': 1904.47265625}\n",
      "Step 840: {'train_loss': 0.4586966633796692, 'lr': 0.0001552891396332863, 'memory_mb': 1904.47265625}\n",
      "Step 850: {'train_loss': 0.41367214918136597, 'lr': 0.00015317348377997178, 'memory_mb': 1904.47265625}\n",
      "Step 860: {'train_loss': 0.3468436300754547, 'lr': 0.00015105782792665726, 'memory_mb': 1904.47265625}\n",
      "Step 870: {'train_loss': 0.4982622563838959, 'lr': 0.00014894217207334271, 'memory_mb': 1904.47265625}\n",
      "Step 880: {'train_loss': 0.42441827058792114, 'lr': 0.0001468265162200282, 'memory_mb': 1904.47265625}\n",
      "Step 882: {'eval_loss': 1.7282916978001595, 'memory_mb': 1904.47265625}\n",
      "Step 890: {'train_loss': 0.4430353045463562, 'lr': 0.00014471086036671368, 'memory_mb': 1904.47265625}\n",
      "Step 900: {'train_loss': 0.354238897562027, 'lr': 0.00014259520451339913, 'memory_mb': 1904.47265625}\n",
      "Step 910: {'train_loss': 0.49785956740379333, 'lr': 0.00014047954866008462, 'memory_mb': 1904.47265625}\n",
      "Step 920: {'train_loss': 0.34643203020095825, 'lr': 0.0001383638928067701, 'memory_mb': 1904.47265625}\n",
      "Step 930: {'train_loss': 0.4187847375869751, 'lr': 0.00013624823695345556, 'memory_mb': 1904.47265625}\n",
      "Step 940: {'train_loss': 0.3715031147003174, 'lr': 0.00013413258110014104, 'memory_mb': 1904.47265625}\n",
      "Step 945: {'eval_loss': 1.771104246377945, 'memory_mb': 1904.47265625}\n",
      "Step 950: {'train_loss': 0.3228118419647217, 'lr': 0.0001320169252468265, 'memory_mb': 1904.47265625}\n",
      "Step 960: {'train_loss': 0.3900614082813263, 'lr': 0.00012990126939351198, 'memory_mb': 1904.47265625}\n",
      "Step 970: {'train_loss': 0.3903888463973999, 'lr': 0.00012778561354019746, 'memory_mb': 1904.47265625}\n",
      "Step 980: {'train_loss': 0.33302173018455505, 'lr': 0.00012566995768688291, 'memory_mb': 1904.47265625}\n",
      "Step 990: {'train_loss': 0.3452896475791931, 'lr': 0.0001235543018335684, 'memory_mb': 1904.47265625}\n",
      "Step 1000: {'train_loss': 0.4140610992908478, 'lr': 0.00012143864598025387, 'memory_mb': 1904.47265625}\n",
      "Step 1008: {'eval_loss': 1.8730683997273445, 'memory_mb': 1904.47265625}\n",
      "Step 1010: {'train_loss': 0.27922821044921875, 'lr': 0.00011932299012693933, 'memory_mb': 1904.47265625}\n",
      "Step 1020: {'train_loss': 0.27899476885795593, 'lr': 0.00011720733427362482, 'memory_mb': 1904.47265625}\n",
      "Step 1030: {'train_loss': 0.26367971301078796, 'lr': 0.00011509167842031029, 'memory_mb': 1904.47265625}\n",
      "Step 1040: {'train_loss': 0.36672353744506836, 'lr': 0.00011297602256699577, 'memory_mb': 1904.47265625}\n",
      "Step 1050: {'train_loss': 0.2828465402126312, 'lr': 0.00011086036671368122, 'memory_mb': 1904.47265625}\n",
      "Step 1060: {'train_loss': 0.41197338700294495, 'lr': 0.0001087447108603667, 'memory_mb': 1904.47265625}\n",
      "Step 1070: {'train_loss': 0.28953155875205994, 'lr': 0.00010662905500705217, 'memory_mb': 1904.47265625}\n",
      "Step 1071: {'eval_loss': 1.8961698412895203, 'memory_mb': 1904.47265625}\n",
      "Step 1080: {'train_loss': 0.24810372292995453, 'lr': 0.00010451339915373766, 'memory_mb': 1904.47265625}\n",
      "Step 1090: {'train_loss': 0.2824232876300812, 'lr': 0.00010239774330042313, 'memory_mb': 1904.47265625}\n",
      "Step 1100: {'train_loss': 0.2459503561258316, 'lr': 0.0001002820874471086, 'memory_mb': 1904.47265625}\n",
      "Step 1110: {'train_loss': 0.3093913495540619, 'lr': 9.816643159379406e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1120: {'train_loss': 0.337114155292511, 'lr': 9.605077574047955e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1130: {'train_loss': 0.35764971375465393, 'lr': 9.393511988716502e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1134: {'eval_loss': 1.9581226594746113, 'memory_mb': 1904.47265625}\n",
      "Step 1140: {'train_loss': 0.23968584835529327, 'lr': 9.18194640338505e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1150: {'train_loss': 0.275482177734375, 'lr': 8.970380818053595e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1160: {'train_loss': 0.2838412821292877, 'lr': 8.758815232722142e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1170: {'train_loss': 0.3219503164291382, 'lr': 8.54724964739069e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1180: {'train_loss': 0.22622627019882202, 'lr': 8.335684062059237e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1190: {'train_loss': 0.2640911042690277, 'lr': 8.124118476727786e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1197: {'eval_loss': 2.0207662619650364, 'memory_mb': 1904.47265625}\n",
      "Step 1200: {'train_loss': 0.2068042755126953, 'lr': 7.912552891396331e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1210: {'train_loss': 0.2445233166217804, 'lr': 7.70098730606488e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1220: {'train_loss': 0.2700907588005066, 'lr': 7.489421720733426e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1230: {'train_loss': 0.2130391001701355, 'lr': 7.277856135401975e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1240: {'train_loss': 0.23385152220726013, 'lr': 7.066290550070521e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1250: {'train_loss': 0.24710598587989807, 'lr': 6.854724964739068e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1260: {'eval_loss': 2.0789181776344776, 'memory_mb': 1904.47265625}\n",
      "Step 1260: {'train_loss': 0.2416897863149643, 'lr': 6.643159379407615e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1270: {'train_loss': 0.2078295648097992, 'lr': 6.431593794076164e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1280: {'train_loss': 0.21150949597358704, 'lr': 6.22002820874471e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1290: {'train_loss': 0.1822367161512375, 'lr': 6.008462623413257e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1300: {'train_loss': 0.22018569707870483, 'lr': 5.796897038081805e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1310: {'train_loss': 0.22582192718982697, 'lr': 5.585331452750352e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1320: {'train_loss': 0.2816227376461029, 'lr': 5.3737658674188994e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1323: {'eval_loss': 2.116435021162033, 'memory_mb': 1904.47265625}\n",
      "Step 1330: {'train_loss': 0.24277842044830322, 'lr': 5.162200282087447e-05, 'memory_mb': 1904.47265625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1340: {'train_loss': 0.20412513613700867, 'lr': 4.950634696755994e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1350: {'train_loss': 0.17489473521709442, 'lr': 4.7390691114245414e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1360: {'train_loss': 0.1968735009431839, 'lr': 4.527503526093088e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1370: {'train_loss': 0.24387578666210175, 'lr': 4.315937940761636e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1380: {'train_loss': 0.20639033615589142, 'lr': 4.1043723554301835e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1386: {'eval_loss': 2.1517949253320694, 'memory_mb': 1904.47265625}\n",
      "Step 1390: {'train_loss': 0.18191082775592804, 'lr': 3.8928067700987303e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1400: {'train_loss': 0.16672268509864807, 'lr': 3.681241184767278e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1410: {'train_loss': 0.23016390204429626, 'lr': 3.469675599435825e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1420: {'train_loss': 0.15999571979045868, 'lr': 3.258110014104372e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1430: {'train_loss': 0.19491766393184662, 'lr': 3.0465444287729193e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1440: {'train_loss': 0.1793304681777954, 'lr': 2.834978843441467e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1449: {'eval_loss': 2.1849978528916836, 'memory_mb': 1904.47265625}\n",
      "Step 1450: {'train_loss': 0.16952048242092133, 'lr': 2.623413258110014e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1460: {'train_loss': 0.21402019262313843, 'lr': 2.411847672778561e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1470: {'train_loss': 0.19983229041099548, 'lr': 2.2002820874471082e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1480: {'train_loss': 0.19812023639678955, 'lr': 1.9887165021156555e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1490: {'train_loss': 0.20950041711330414, 'lr': 1.777150916784203e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1500: {'train_loss': 0.1985590010881424, 'lr': 1.5655853314527503e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1510: {'train_loss': 0.18218623101711273, 'lr': 1.3540197461212975e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1512: {'eval_loss': 2.208099465817213, 'memory_mb': 1904.47265625}\n",
      "Step 1520: {'train_loss': 0.1858731508255005, 'lr': 1.1424541607898449e-05, 'memory_mb': 1904.47265625}\n",
      "Step 1530: {'train_loss': 0.16492635011672974, 'lr': 9.30888575458392e-06, 'memory_mb': 1904.47265625}\n",
      "Step 1540: {'train_loss': 0.19731849431991577, 'lr': 7.193229901269393e-06, 'memory_mb': 1904.47265625}\n",
      "Step 1550: {'train_loss': 0.2613426744937897, 'lr': 5.077574047954866e-06, 'memory_mb': 1904.47265625}\n",
      "Step 1560: {'train_loss': 0.22149378061294556, 'lr': 2.9619181946403384e-06, 'memory_mb': 1904.47265625}\n",
      "Step 1570: {'train_loss': 0.20819704234600067, 'lr': 8.462623413258109e-07, 'memory_mb': 1904.47265625}\n",
      "Step 1575: {'eval_loss': 2.2206690907478333, 'memory_mb': 1904.47265625}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.22067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1904.47266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.2082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_warmup_20250425_232909\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/7umzyrrb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_232909-7umzyrrb/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule warmup \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.001 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253193f6",
   "metadata": {},
   "source": [
    "### Plateau with F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7885d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanvi-kaple14\u001b[0m (\u001b[33mtkaple\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/ice1/6/9/tkaple3/DL_Project/wandb/run-20250425_233118-hrr9cn0y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama1b_lora_adamw_plateau_20250425_233118\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/hrr9cn0y\u001b[0m\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hice1/tkaple3/.conda/envs/myenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Step 0: {'train_loss': 7.116552352905273, 'lr': 0.0003, 'memory_mb': 1897.80859375}\n",
      "Step 10: {'train_loss': 1.6952680349349976, 'lr': 0.0003, 'memory_mb': 1900.80859375}\n",
      "Step 20: {'train_loss': 1.441868543624878, 'lr': 0.0003, 'memory_mb': 1901.80859375}\n",
      "Step 30: {'train_loss': 1.1627061367034912, 'lr': 0.0003, 'memory_mb': 1901.80859375}\n",
      "Step 40: {'train_loss': 1.2523077726364136, 'lr': 0.0003, 'memory_mb': 1901.80859375}\n",
      "Step 50: {'train_loss': 1.1674116849899292, 'lr': 0.0003, 'memory_mb': 1901.80859375}\n",
      "Step 60: {'train_loss': 1.239466667175293, 'lr': 0.0003, 'memory_mb': 1901.80859375}\n",
      "Step 63: {'eval_loss': 1.2657389268279076, 'memory_mb': 1903.80859375}\n",
      "Step 70: {'train_loss': 1.2837108373641968, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 80: {'train_loss': 1.3620747327804565, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 90: {'train_loss': 1.188518762588501, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 100: {'train_loss': 1.0395869016647339, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 110: {'train_loss': 1.0578974485397339, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 120: {'train_loss': 1.2720706462860107, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 126: {'eval_loss': 1.2475565671920776, 'memory_mb': 1903.80859375}\n",
      "Step 130: {'train_loss': 1.1231704950332642, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 140: {'train_loss': 0.9602336287498474, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 150: {'train_loss': 1.174655795097351, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 160: {'train_loss': 0.9106057286262512, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 170: {'train_loss': 1.2193965911865234, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 180: {'train_loss': 1.1314449310302734, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 189: {'eval_loss': 1.2613228764384985, 'memory_mb': 1903.80859375}\n",
      "Step 190: {'train_loss': 1.2221193313598633, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 200: {'train_loss': 1.033835530281067, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 210: {'train_loss': 0.8785492181777954, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 220: {'train_loss': 0.9134131669998169, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 230: {'train_loss': 0.9008175134658813, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 240: {'train_loss': 1.1410971879959106, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 250: {'train_loss': 0.9526962637901306, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 252: {'eval_loss': 1.3013879042118788, 'memory_mb': 1903.80859375}\n",
      "Step 260: {'train_loss': 0.7860466241836548, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 270: {'train_loss': 1.2088228464126587, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 280: {'train_loss': 1.095112681388855, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 290: {'train_loss': 0.8514939546585083, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 300: {'train_loss': 0.8040570616722107, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 310: {'train_loss': 0.9927032589912415, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 315: {'eval_loss': 1.3213716447353363, 'memory_mb': 1903.80859375}\n",
      "Step 320: {'train_loss': 0.6430411338806152, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 330: {'train_loss': 0.823472797870636, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 340: {'train_loss': 0.759400486946106, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 350: {'train_loss': 0.8100113272666931, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 360: {'train_loss': 0.7306981086730957, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 370: {'train_loss': 0.7854183912277222, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 378: {'eval_loss': 1.3463969584554434, 'memory_mb': 1903.80859375}\n",
      "Step 380: {'train_loss': 0.6111602783203125, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 390: {'train_loss': 0.6728606224060059, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 400: {'train_loss': 0.6289180517196655, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 410: {'train_loss': 0.5843498110771179, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 420: {'train_loss': 0.6444849371910095, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 430: {'train_loss': 0.677000880241394, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 440: {'train_loss': 0.6895051002502441, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 441: {'eval_loss': 1.415365755558014, 'memory_mb': 1903.80859375}\n",
      "Step 450: {'train_loss': 0.523108184337616, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 460: {'train_loss': 0.6224876046180725, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 470: {'train_loss': 0.5795547962188721, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 480: {'train_loss': 0.6740065217018127, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 490: {'train_loss': 0.5899368524551392, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 500: {'train_loss': 0.7636972665786743, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 504: {'eval_loss': 1.4997936561703682, 'memory_mb': 1903.80859375}\n",
      "Step 510: {'train_loss': 0.4694980978965759, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 520: {'train_loss': 0.4801350235939026, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 530: {'train_loss': 0.4582260251045227, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 540: {'train_loss': 0.46077004075050354, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 550: {'train_loss': 0.516086995601654, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 560: {'train_loss': 0.5293470621109009, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 567: {'eval_loss': 1.5596660412847996, 'memory_mb': 1903.80859375}\n",
      "Step 570: {'train_loss': 0.3765726387500763, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 580: {'train_loss': 0.3909081816673279, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 590: {'train_loss': 0.371082603931427, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 600: {'train_loss': 0.43560758233070374, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 610: {'train_loss': 0.4481430649757385, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 620: {'train_loss': 0.5154308080673218, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 630: {'eval_loss': 1.6358608603477478, 'memory_mb': 1903.80859375}\n",
      "Step 630: {'train_loss': 0.48701903223991394, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 640: {'train_loss': 0.38335996866226196, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 650: {'train_loss': 0.44482019543647766, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 660: {'train_loss': 0.4083450734615326, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 670: {'train_loss': 0.33766621351242065, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 680: {'train_loss': 0.46994656324386597, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 690: {'train_loss': 0.3874885141849518, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 693: {'eval_loss': 1.6884109191596508, 'memory_mb': 1903.80859375}\n",
      "Step 700: {'train_loss': 0.32607901096343994, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 710: {'train_loss': 0.34362712502479553, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 720: {'train_loss': 0.39331990480422974, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 730: {'train_loss': 0.40583643317222595, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 740: {'train_loss': 0.36308032274246216, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 750: {'train_loss': 0.3187917172908783, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 756: {'eval_loss': 1.7425782680511475, 'memory_mb': 1903.80859375}\n",
      "Step 760: {'train_loss': 0.26501789689064026, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 770: {'train_loss': 0.2713455855846405, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 780: {'train_loss': 0.2609120309352875, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 790: {'train_loss': 0.27872899174690247, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 800: {'train_loss': 0.35063081979751587, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 810: {'train_loss': 0.3591235876083374, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 819: {'eval_loss': 1.8210347406566143, 'memory_mb': 1903.80859375}\n",
      "Step 820: {'train_loss': 0.2863727807998657, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 830: {'train_loss': 0.24195127189159393, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 840: {'train_loss': 0.29500773549079895, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 850: {'train_loss': 0.2874475121498108, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 860: {'train_loss': 0.25099486112594604, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 870: {'train_loss': 0.32951751351356506, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 880: {'train_loss': 0.29608356952667236, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 882: {'eval_loss': 1.8757389672100544, 'memory_mb': 1903.80859375}\n",
      "Step 890: {'train_loss': 0.2996633052825928, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 900: {'train_loss': 0.23010919988155365, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 910: {'train_loss': 0.3102838695049286, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 920: {'train_loss': 0.24140949547290802, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 930: {'train_loss': 0.26526957750320435, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 940: {'train_loss': 0.2424735277891159, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 945: {'eval_loss': 1.9359658546745777, 'memory_mb': 1903.80859375}\n",
      "Step 950: {'train_loss': 0.21520739793777466, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 960: {'train_loss': 0.2586970925331116, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 970: {'train_loss': 0.2380307912826538, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 980: {'train_loss': 0.19412298500537872, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 990: {'train_loss': 0.23376528918743134, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1000: {'train_loss': 0.2612270712852478, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1008: {'eval_loss': 1.9670737348496914, 'memory_mb': 1903.80859375}\n",
      "Step 1010: {'train_loss': 0.18680520355701447, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1020: {'train_loss': 0.1704561412334442, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1030: {'train_loss': 0.19093820452690125, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1040: {'train_loss': 0.23938995599746704, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1050: {'train_loss': 0.19295775890350342, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1060: {'train_loss': 0.2631259560585022, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1070: {'train_loss': 0.21523168683052063, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1071: {'eval_loss': 1.9915537275373936, 'memory_mb': 1903.80859375}\n",
      "Step 1080: {'train_loss': 0.1656138151884079, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1090: {'train_loss': 0.17045825719833374, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1100: {'train_loss': 0.1712685525417328, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1110: {'train_loss': 0.21210280060768127, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1120: {'train_loss': 0.22891205549240112, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1130: {'train_loss': 0.24201063811779022, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1134: {'eval_loss': 2.0416502617299557, 'memory_mb': 1903.80859375}\n",
      "Step 1140: {'train_loss': 0.15817944705486298, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1150: {'train_loss': 0.16710375249385834, 'lr': 0.0003, 'memory_mb': 1903.80859375}\n",
      "Step 1160: {'train_loss': 0.18486972153186798, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1170: {'train_loss': 0.1959245651960373, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1180: {'train_loss': 0.1592203974723816, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1190: {'train_loss': 0.1668691486120224, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1197: {'eval_loss': 2.1000677049160004, 'memory_mb': 1904.30859375}\n",
      "Step 1200: {'train_loss': 0.1228625550866127, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1210: {'train_loss': 0.165536031126976, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1220: {'train_loss': 0.16286566853523254, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1230: {'train_loss': 0.14493879675865173, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1240: {'train_loss': 0.18514929711818695, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1250: {'train_loss': 0.1805087774991989, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1260: {'eval_loss': 2.1533354446291924, 'memory_mb': 1904.30859375}\n",
      "Step 1260: {'train_loss': 0.1467771828174591, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1270: {'train_loss': 0.13785098493099213, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1280: {'train_loss': 0.12881164252758026, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1290: {'train_loss': 0.1371181458234787, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1300: {'train_loss': 0.1376843899488449, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1310: {'train_loss': 0.14358319342136383, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1320: {'train_loss': 0.18984945118427277, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1323: {'eval_loss': 2.149941761046648, 'memory_mb': 1904.30859375}\n",
      "Step 1330: {'train_loss': 0.14353972673416138, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1340: {'train_loss': 0.13305871188640594, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1350: {'train_loss': 0.13277491927146912, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1360: {'train_loss': 0.14095138013362885, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1370: {'train_loss': 0.15195536613464355, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1380: {'train_loss': 0.15340633690357208, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1386: {'eval_loss': 2.2020392827689648, 'memory_mb': 1904.30859375}\n",
      "Step 1390: {'train_loss': 0.11658824235200882, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1400: {'train_loss': 0.12627637386322021, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1410: {'train_loss': 0.1475130021572113, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1420: {'train_loss': 0.13201290369033813, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1430: {'train_loss': 0.13274972140789032, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1440: {'train_loss': 0.13949890434741974, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1449: {'eval_loss': 2.1371452137827873, 'memory_mb': 1904.30859375}\n",
      "Step 1450: {'train_loss': 0.11754004657268524, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1460: {'train_loss': 0.12362895905971527, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1470: {'train_loss': 0.1260998547077179, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1480: {'train_loss': 0.1265867054462433, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1490: {'train_loss': 0.15287356078624725, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1500: {'train_loss': 0.15601088106632233, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1510: {'train_loss': 0.12586629390716553, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1512: {'eval_loss': 2.145453557372093, 'memory_mb': 1904.30859375}\n",
      "Step 1520: {'train_loss': 0.11651479452848434, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1530: {'train_loss': 0.11269822716712952, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1540: {'train_loss': 0.12032455950975418, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1550: {'train_loss': 0.15272989869117737, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1560: {'train_loss': 0.14361175894737244, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1570: {'train_loss': 0.14887692034244537, 'lr': 0.0003, 'memory_mb': 1904.30859375}\n",
      "Step 1575: {'eval_loss': 2.1792117543518543, 'memory_mb': 1904.30859375}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  eval_loss 2.17921\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  memory_mb 1904.30859\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.14888\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mllama1b_lora_adamw_plateau_20250425_233118\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news/runs/hrr9cn0y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/tkaple/peft-convergence-llama-ag-news\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250425_233118-hrr9cn0y/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python peft_llama_v2.py \\\n",
    "    --model_name \"meta-llama/Llama-3.2-1B\" \\\n",
    "    --dataset_name ag_news \\\n",
    "    --peft_method lora \\\n",
    "    --use_peft \\\n",
    "    --optimizer adamw \\\n",
    "    --lr_schedule plateau \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_epochs 25 \\\n",
    "    --max_samples 1000 \\\n",
    "    --log_wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5955ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv]",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
