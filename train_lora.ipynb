{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3V5uqSwH3Ic",
        "outputId": "1c749f75-2850-4407-9eb2-49f86e397e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to your project folder\n",
        "%cd /content/drive/MyDrive/peft-convergence\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Make sure you're using a GPU\n",
        "!nvidia-smi\n",
        "\n",
        "# Check the GPU availability in PyTorch\n",
        "import torch\n",
        "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "# # print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "# print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCZd2nlnH5P8",
        "outputId": "1d7ae07d-fea5-43a3-957d-31564f5ddb0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/peft-convergence\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.50.0)\n",
            "Collecting datasets>=2.14.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.14.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
            "Requirement already satisfied: wandb>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.19.8)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Collecting lion-pytorch>=0.1.2 (from -r requirements.txt (line 11))\n",
            "  Downloading lion_pytorch-0.2.3-py3-none-any.whl.metadata (616 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.0->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets>=2.14.0->-r requirements.txt (line 3))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.14.0->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch>=2.0.0->-r requirements.txt (line 1))\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.11.14)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->-r requirements.txt (line 7)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 8)) (2025.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.15.0->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 6)) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.15.0->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.15.0->-r requirements.txt (line 6)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 6)) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lion_pytorch-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, lion-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 lion-pytorch-0.2.3 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n",
            "Fri Mar 28 18:53:23 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a basic experiment\n",
        "!python peft_training.py \\\n",
        "    --model_name google/flan-t5-small \\\n",
        "    --dataset_name ag_news \\\n",
        "    --peft_method lora \\\n",
        "    --optimizer adamw \\\n",
        "    --lr_schedule warmup \\\n",
        "    --learning_rate 1e-3 \\\n",
        "    --batch_size 16 \\\n",
        "    --num_epochs 3 \\\n",
        "    --max_samples 1000 \\\n",
        "    --log_wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZegMY77H8xW",
        "outputId": "e6e2f87b-08f9-4ff8-93f2-a9ea28f7e86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-25 21:09:43.437049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742936983.819215    1871 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742936983.926635    1871 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-25 21:09:44.737145: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model: google/flan-t5-small\n",
            "tokenizer_config.json: 100% 2.54k/2.54k [00:00<00:00, 14.6MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 12.3MB/s]\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 25.1MB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 10.8MB/s]\n",
            "config.json: 100% 1.40k/1.40k [00:00<00:00, 7.19MB/s]\n",
            "model.safetensors: 100% 308M/308M [00:01<00:00, 193MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 890kB/s]\n",
            "Preparing dataset: ag_news\n",
            "README.md: 100% 8.07k/8.07k [00:00<00:00, 28.5MB/s]\n",
            "train-00000-of-00001.parquet: 100% 18.6M/18.6M [00:00<00:00, 99.9MB/s]\n",
            "test-00000-of-00001.parquet: 100% 1.23M/1.23M [00:00<00:00, 80.2MB/s]\n",
            "Generating train split: 100% 120000/120000 [00:00<00:00, 380658.72 examples/s]\n",
            "Generating test split: 100% 7600/7600 [00:00<00:00, 376312.87 examples/s]\n",
            "Map:   0% 0/1000 [00:00<?, ? examples/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 1000/1000 [00:00<00:00, 3247.74 examples/s]\n",
            "Map: 100% 500/500 [00:00<00:00, 3171.26 examples/s]\n",
            "Applying PEFT method: lora\n",
            "Total parameters: 77305216\n",
            "Trainable parameters: 344064 (0.45%)\n",
            "Starting training...\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.45970153808594, 'lr': 0.0, 'memory_usage_mb': 3249.59375}\n",
            "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 30.021957397460938, 'lr': 0.0005555555555555556, 'memory_usage_mb': 4097.80859375}\n",
            "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 22.02187728881836, 'lr': 0.0009883040935672516, 'memory_usage_mb': 4074.87890625}\n",
            "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 13.143773078918457, 'lr': 0.0009298245614035087, 'memory_usage_mb': 4050.0859375}\n",
            "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 7.995516300201416, 'lr': 0.0008713450292397661, 'memory_usage_mb': 3984.76171875}\n",
            "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 5.894002914428711, 'lr': 0.0008128654970760234, 'memory_usage_mb': 3956.7890625}\n",
            "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 4.5543670654296875, 'lr': 0.0007543859649122807, 'memory_usage_mb': 4040.421875}\n",
            "Step 63: {'epoch': 0, 'eval_loss': 0.0, 'accuracy': 0.664, 'f1_score': 0.6518490946079318, 'epoch_time_seconds': 397.426087141037, 'memory_usage_mb': 4046.265625}\n",
            "New best accuracy: 0.6640\n",
            "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 3.9924614429473877, 'lr': 0.000695906432748538, 'memory_usage_mb': 3979.84765625}\n",
            "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 3.8451523780822754, 'lr': 0.0006374269005847954, 'memory_usage_mb': 3958.12890625}\n",
            "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 3.694118022918701, 'lr': 0.0005789473684210527, 'memory_usage_mb': 3984.5078125}\n",
            "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 3.6168267726898193, 'lr': 0.00052046783625731, 'memory_usage_mb': 3944.87109375}\n",
            "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 3.6420233249664307, 'lr': 0.00046198830409356726, 'memory_usage_mb': 3917.12890625}\n",
            "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 3.630676746368408, 'lr': 0.00040350877192982455, 'memory_usage_mb': 3917.12890625}\n",
            "Step 126: {'epoch': 1, 'eval_loss': 0.0, 'accuracy': 0.806, 'f1_score': 0.806907203344698, 'epoch_time_seconds': 399.73624539375305, 'memory_usage_mb': 3917.12890625}\n",
            "New best accuracy: 0.8060\n",
            "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 3.462282180786133, 'lr': 0.0003450292397660819, 'memory_usage_mb': 3851.1484375}\n",
            "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 3.49172306060791, 'lr': 0.0002865497076023392, 'memory_usage_mb': 3895.25}\n",
            "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 3.4666359424591064, 'lr': 0.0002280701754385965, 'memory_usage_mb': 3893.6953125}\n",
            "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 3.3516287803649902, 'lr': 0.0001695906432748538, 'memory_usage_mb': 3894.109375}\n",
            "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 3.3755996227264404, 'lr': 0.0001111111111111111, 'memory_usage_mb': 3957.5078125}\n",
            "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 3.356745958328247, 'lr': 5.263157894736842e-05, 'memory_usage_mb': 3898.6796875}\n",
            "Step 189: {'epoch': 2, 'eval_loss': 0.0, 'accuracy': 0.806, 'f1_score': 0.8047083914701314, 'epoch_time_seconds': 395.38382363319397, 'memory_usage_mb': 3932.9609375}\n",
            "Step final: {'total_training_time_seconds': 1192.5472311973572, 'avg_epoch_time_seconds': np.float64(397.515385389328), 'best_accuracy': 0.806, 'convergence_step': 'Not converged', 'memory_usage_mb': 3932.9609375}\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a basic experiment\n",
        "!python peft_training.py \\\n",
        "    --model_name google/flan-t5-small \\\n",
        "    --dataset_name ag_news \\\n",
        "    --peft_method lora \\\n",
        "    --optimizer adamw \\\n",
        "    --lr_schedule warmup \\\n",
        "    --learning_rate 1e-3 \\\n",
        "    --batch_size 16 \\\n",
        "    --num_epochs 3 \\\n",
        "    --max_samples 1000 \\\n",
        "    --log_wandb"
      ],
      "metadata": {
        "id": "waNjoIk3I3Gu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc6c192-7d1f-4cff-d72e-7774dba1bcf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-28 18:56:09.846905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743188170.123688    1867 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743188170.199874    1867 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-28 18:56:10.819174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m10zin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/peft-convergence/wandb/run-20250328_185746-g4dpx4wr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora_adamw_warmup_20250328_185616\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/10zin/peft-convergence\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/10zin/peft-convergence/runs/g4dpx4wr\u001b[0m\n",
            "Loading model: google/flan-t5-small\n",
            "tokenizer_config.json: 100% 2.54k/2.54k [00:00<00:00, 8.48MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 17.6MB/s]\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 39.9MB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 8.90MB/s]\n",
            "config.json: 100% 1.40k/1.40k [00:00<00:00, 5.51MB/s]\n",
            "model.safetensors: 100% 308M/308M [00:01<00:00, 239MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 242kB/s]\n",
            "Preparing dataset: ag_news\n",
            "README.md: 100% 8.07k/8.07k [00:00<00:00, 15.1MB/s]\n",
            "train-00000-of-00001.parquet: 100% 18.6M/18.6M [00:00<00:00, 153MB/s]\n",
            "test-00000-of-00001.parquet: 100% 1.23M/1.23M [00:00<00:00, 185MB/s]\n",
            "Generating train split: 100% 120000/120000 [00:00<00:00, 596591.11 examples/s]\n",
            "Generating test split: 100% 7600/7600 [00:00<00:00, 486474.23 examples/s]\n",
            "Map:   0% 0/1000 [00:00<?, ? examples/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 1000/1000 [00:00<00:00, 3979.29 examples/s]\n",
            "Map: 100% 500/500 [00:00<00:00, 4412.86 examples/s]\n",
            "Applying PEFT method: lora\n",
            "Total parameters: 77305216\n",
            "Trainable parameters: 344064 (0.45%)\n",
            "Starting training...\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 34.99641799926758, 'lr': 0.0, 'memory_usage_mb': 1892.57421875}\n",
            "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 31.80825424194336, 'lr': 0.0005555555555555556, 'memory_usage_mb': 1956.671875}\n",
            "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 21.68268394470215, 'lr': 0.0009883040935672516, 'memory_usage_mb': 1956.671875}\n",
            "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 13.095848083496094, 'lr': 0.0009298245614035087, 'memory_usage_mb': 1956.671875}\n",
            "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 8.359895706176758, 'lr': 0.0008713450292397661, 'memory_usage_mb': 1956.671875}\n",
            "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 6.086468696594238, 'lr': 0.0008128654970760234, 'memory_usage_mb': 1956.671875}\n",
            "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 4.713057994842529, 'lr': 0.0007543859649122807, 'memory_usage_mb': 1956.671875}\n",
            "Step 63: {'epoch': 0, 'eval_loss': 0.0, 'accuracy': 0.636, 'f1_score': 0.6134251955893286, 'epoch_time_seconds': 11.855761528015137, 'memory_usage_mb': 2057.80859375}\n",
            "New best accuracy: 0.6360\n",
            "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 4.181475639343262, 'lr': 0.000695906432748538, 'memory_usage_mb': 2060.4453125}\n",
            "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 3.8143465518951416, 'lr': 0.0006374269005847954, 'memory_usage_mb': 2060.4453125}\n",
            "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 3.757599115371704, 'lr': 0.0005789473684210527, 'memory_usage_mb': 2060.4453125}\n",
            "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 3.6775574684143066, 'lr': 0.00052046783625731, 'memory_usage_mb': 2060.4453125}\n",
            "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 3.7387466430664062, 'lr': 0.00046198830409356726, 'memory_usage_mb': 2060.4453125}\n",
            "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 3.5792596340179443, 'lr': 0.00040350877192982455, 'memory_usage_mb': 2060.4453125}\n",
            "Step 126: {'epoch': 1, 'eval_loss': 0.0, 'accuracy': 0.78, 'f1_score': 0.7722020952389793, 'epoch_time_seconds': 9.60099458694458, 'memory_usage_mb': 2060.4453125}\n",
            "New best accuracy: 0.7800\n",
            "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 3.5301764011383057, 'lr': 0.0003450292397660819, 'memory_usage_mb': 2060.4453125}\n",
            "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 3.500035524368286, 'lr': 0.0002865497076023392, 'memory_usage_mb': 2060.4453125}\n",
            "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 3.4465365409851074, 'lr': 0.0002280701754385965, 'memory_usage_mb': 2060.4453125}\n",
            "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 3.4505743980407715, 'lr': 0.0001695906432748538, 'memory_usage_mb': 2060.4453125}\n",
            "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 3.3658058643341064, 'lr': 0.0001111111111111111, 'memory_usage_mb': 2060.4453125}\n",
            "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 3.348600149154663, 'lr': 5.263157894736842e-05, 'memory_usage_mb': 2060.4453125}\n",
            "Step 189: {'epoch': 2, 'eval_loss': 0.0, 'accuracy': 0.812, 'f1_score': 0.810042524765434, 'epoch_time_seconds': 10.209728002548218, 'memory_usage_mb': 2060.4453125}\n",
            "New best accuracy: 0.8120\n",
            "Step final: {'total_training_time_seconds': 31.66899275779724, 'avg_epoch_time_seconds': np.float64(10.555494705835978), 'best_accuracy': 0.812, 'convergence_step': 'Not converged', 'memory_usage_mb': 2060.4453125}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/peft-convergence/peft_training.py\", line 559, in <module>\n",
            "    train_and_evaluate(args)\n",
            "  File \"/content/drive/MyDrive/peft-convergence/peft_training.py\", line 548, in train_and_evaluate\n",
            "    log_metrics(\"final\", final_metrics, args.log_wandb)\n",
            "  File \"/content/drive/MyDrive/peft-convergence/peft_training.py\", line 231, in log_metrics\n",
            "    wandb.log(metrics, step=step)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 401, in wrapper_fn\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 391, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1873, in log\n",
            "    self._log(data=data, step=step, commit=commit)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1587, in _log\n",
            "    self._partial_history_callback(data, step, commit)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1417, in _partial_history_callback\n",
            "    self._backend.interface.publish_partial_history(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface.py\", line 687, in publish_partial_history\n",
            "    partial_history.step.num = step\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: 'str' object cannot be interpreted as an integer\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mlora_adamw_warmup_20250328_185616\u001b[0m at: \u001b[34mhttps://wandb.ai/10zin/peft-convergence/runs/g4dpx4wr\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250328_185746-g4dpx4wr/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a basic experiment\n",
        "!python peft_training.py \\\n",
        "    --model_name google/flan-t5-small \\\n",
        "    --dataset_name ag_news \\\n",
        "    --full_ft \\\n",
        "    --optimizer adamw \\\n",
        "    --lr_schedule warmup \\\n",
        "    --learning_rate 1e-3 \\\n",
        "    --batch_size 16 \\\n",
        "    --num_epochs 3 \\\n",
        "    --max_samples 1000 \\\n",
        "    --log_wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLYMzXe2HFUq",
        "outputId": "a3fa5982-924b-4def-d86c-be876beb6acb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-28 19:16:10.171472: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743189370.192366    6981 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743189370.198668    6981 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-28 19:16:10.219975: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m10zin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/peft-convergence/wandb/run-20250328_191613-yy7696lc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfull_ft_adamw_warmup_20250328_191613\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/10zin/peft-convergence\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/10zin/peft-convergence/runs/yy7696lc\u001b[0m\n",
            "Loading model: google/flan-t5-small\n",
            "Preparing dataset: ag_news\n",
            "Using full fine-tuning (no PEFT)\n",
            "Total parameters: 76961152\n",
            "Trainable parameters: 76961152 (100.00%)\n",
            "Starting training...\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Step 0: {'epoch': 0, 'step': 0, 'train_loss': 33.21565246582031, 'lr': 0.0, 'memory_usage_mb': 1888.12109375}\n",
            "Step 10: {'epoch': 0, 'step': 10, 'train_loss': 16.816526412963867, 'lr': 0.0005555555555555556, 'memory_usage_mb': 1952.71484375}\n",
            "Step 20: {'epoch': 0, 'step': 20, 'train_loss': 4.593477249145508, 'lr': 0.0009883040935672516, 'memory_usage_mb': 1952.71484375}\n",
            "Step 30: {'epoch': 0, 'step': 30, 'train_loss': 1.4570691585540771, 'lr': 0.0009298245614035087, 'memory_usage_mb': 1952.71484375}\n",
            "Step 40: {'epoch': 0, 'step': 40, 'train_loss': 0.21354836225509644, 'lr': 0.0008713450292397661, 'memory_usage_mb': 1952.71484375}\n",
            "Step 50: {'epoch': 0, 'step': 50, 'train_loss': 0.13997310400009155, 'lr': 0.0008128654970760234, 'memory_usage_mb': 1952.71484375}\n",
            "Step 60: {'epoch': 0, 'step': 60, 'train_loss': 0.11897087097167969, 'lr': 0.0007543859649122807, 'memory_usage_mb': 1952.71484375}\n",
            "Step 63: {'epoch': 0, 'eval_loss': 0.0, 'accuracy': 0.82, 'f1_score': 0.8153559639060534, 'epoch_time_seconds': 12.491191864013672, 'memory_usage_mb': 2051.50390625}\n",
            "New best accuracy: 0.8200\n",
            "Step 70: {'epoch': 1, 'step': 70, 'train_loss': 0.12327004969120026, 'lr': 0.000695906432748538, 'memory_usage_mb': 2053.69140625}\n",
            "Step 80: {'epoch': 1, 'step': 80, 'train_loss': 0.12664122879505157, 'lr': 0.0006374269005847954, 'memory_usage_mb': 2053.69140625}\n",
            "Step 90: {'epoch': 1, 'step': 90, 'train_loss': 0.028417430818080902, 'lr': 0.0005789473684210527, 'memory_usage_mb': 2053.69140625}\n",
            "Step 100: {'epoch': 1, 'step': 100, 'train_loss': 0.12113813310861588, 'lr': 0.00052046783625731, 'memory_usage_mb': 2053.69140625}\n",
            "Step 110: {'epoch': 1, 'step': 110, 'train_loss': 0.061139706522226334, 'lr': 0.00046198830409356726, 'memory_usage_mb': 2053.69140625}\n",
            "Step 120: {'epoch': 1, 'step': 120, 'train_loss': 0.06655899435281754, 'lr': 0.00040350877192982455, 'memory_usage_mb': 2053.69140625}\n",
            "Step 126: {'epoch': 1, 'eval_loss': 0.0, 'accuracy': 0.844, 'f1_score': 0.8444884653798994, 'epoch_time_seconds': 11.736745119094849, 'memory_usage_mb': 2053.69140625}\n",
            "New best accuracy: 0.8440\n",
            "Step 130: {'epoch': 2, 'step': 130, 'train_loss': 0.12981072068214417, 'lr': 0.0003450292397660819, 'memory_usage_mb': 2053.69140625}\n",
            "Step 140: {'epoch': 2, 'step': 140, 'train_loss': 0.03751597926020622, 'lr': 0.0002865497076023392, 'memory_usage_mb': 2053.69140625}\n",
            "Step 150: {'epoch': 2, 'step': 150, 'train_loss': 0.11165820807218552, 'lr': 0.0002280701754385965, 'memory_usage_mb': 2053.69140625}\n",
            "Step 160: {'epoch': 2, 'step': 160, 'train_loss': 0.03871145844459534, 'lr': 0.0001695906432748538, 'memory_usage_mb': 2053.69140625}\n",
            "Step 170: {'epoch': 2, 'step': 170, 'train_loss': 0.02519950456917286, 'lr': 0.0001111111111111111, 'memory_usage_mb': 2053.69140625}\n",
            "Step 180: {'epoch': 2, 'step': 180, 'train_loss': 0.07119759917259216, 'lr': 5.263157894736842e-05, 'memory_usage_mb': 2053.69140625}\n",
            "Step 189: {'epoch': 2, 'eval_loss': 0.0, 'accuracy': 0.864, 'f1_score': 0.8633500564404866, 'epoch_time_seconds': 11.845430374145508, 'memory_usage_mb': 2053.69140625}\n",
            "New best accuracy: 0.8640\n",
            "Step final: {'total_training_time_seconds': 36.07725977897644, 'avg_epoch_time_seconds': np.float64(12.024455785751343), 'best_accuracy': 0.864, 'convergence_step': 'Not converged', 'memory_usage_mb': 2053.69140625}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/peft-convergence/peft_training.py\", line 581, in <module>\n",
            "    train_and_evaluate(args)\n",
            "  File \"/content/drive/MyDrive/peft-convergence/peft_training.py\", line 570, in train_and_evaluate\n",
            "    log_metrics(\"final\", final_metrics, args.log_wandb)\n",
            "  File \"/content/drive/MyDrive/peft-convergence/peft_training.py\", line 248, in log_metrics\n",
            "    wandb.log(metrics, step=step)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 401, in wrapper_fn\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 391, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1873, in log\n",
            "    self._log(data=data, step=step, commit=commit)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1587, in _log\n",
            "    self._partial_history_callback(data, step, commit)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 1417, in _partial_history_callback\n",
            "    self._backend.interface.publish_partial_history(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface.py\", line 687, in publish_partial_history\n",
            "    partial_history.step.num = step\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: 'str' object cannot be interpreted as an integer\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mfull_ft_adamw_warmup_20250328_191613\u001b[0m at: \u001b[34mhttps://wandb.ai/10zin/peft-convergence/runs/yy7696lc\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250328_191613-yy7696lc/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEVwfZDMLQf5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}